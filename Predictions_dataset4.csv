,title,keywords,abstract,label,Title_Abstract,PREDS_UWTFIDF,PREDS_UWbert,PREDS_Wbert,PREDS_Wsbert,PREDS_UWsbert,PREDS_pargraphsbert
0,JEETuningExpert: A software assistant for improving Java Enterprise Edition application performance,"Java enterprise applications, Performance tuning, Performance anti-patterns, Expert systems, Intelligent systems","Designing a JEE (Java Enterprise Edition)-based enterprise application capable of achieving its performance objectives is rather hard. Predicting the performance of this type of systems at the design level is difficult and sometimes not viable, because this requires having precise knowledge of the expected load conditions and the underlying software infrastructure. Besides, the requirement for rapid time-to-market leads to postpone performance tuning until systems are developed, packaged and running. In this paper we present a novel approach for automatically detecting performance problems in JEE-based applications and, in turn, suggesting courses of actions to correct them. The idea is to allow developers to smoothly identify and eradicate performance anti-patterns by automatically analyzing execution traces. The approach has been implemented as a tool called JEETuningExpert, and validated using three well-known JEE reference applications. Specifically, we evaluated the effectiveness of JEETuningExpert for detecting performance problems, measured the overhead imposed by online monitoring each application and the improvements were achieved after following the suggested corrective actions. These results empirically showed that the refactored applications are 40.08%, 76.94% and 61.13% faster, on average.",0,"JEETuningExpert: A software assistant for improving Java Enterprise Edition application performance. Designing a JEE (Java Enterprise Edition)-based enterprise application capable of achieving its performance objectives is rather hard. Predicting the performance of this type of systems at the design level is difficult and sometimes not viable, because this requires having precise knowledge of the expected load conditions and the underlying software infrastructure. Besides, the requirement for rapid time-to-market leads to postpone performance tuning until systems are developed, packaged and running. In this paper we present a novel approach for automatically detecting performance problems in JEE-based applications and, in turn, suggesting courses of actions to correct them. The idea is to allow developers to smoothly identify and eradicate performance anti-patterns by automatically analyzing execution traces. The approach has been implemented as a tool called JEETuningExpert, and validated using three well-known JEE reference applications. Specifically, we evaluated the effectiveness of JEETuningExpert for detecting performance problems, measured the overhead imposed by online monitoring each application and the improvements were achieved after following the suggested corrective actions. These results empirically showed that the refactored applications are 40.08%, 76.94% and 61.13% faster, on average.",0,3,0,0,2,2
1,"Retinal connectomics: Towards complete, accurate networks","Retina, Neurons, Connectome, Networks, Synapses, Gap junctions","Connectomics is a strategy for mapping complex neural networks based on high-speed automated electron optical imaging, computational assembly of neural data volumes, web-based navigational tools to explore 1012--1015 byte (terabyte to petabyte) image volumes, and annotation and markup tools to convert images into rich networks with cellular metadata. These collections of network data and associated metadata, analyzed using tools from graph theory and classification theory, can be merged with classical systems theory, giving a more completely parameterized view of how biologic information processing systems are implemented in retina and brain. Networks have two separable features: topology and connection attributes. The first findings from connectomics strongly validate the idea that the topologies of complete retinal networks are far more complex than the simple schematics that emerged from classical anatomy. In particular, connectomics has permitted an aggressive refactoring of the retinal inner plexiform layer, demonstrating that network function cannot be simply inferred from stratification; exposing the complex geometric rules for inserting different cells into a shared network; revealing unexpected bidirectional signaling pathways between mammalian rod and cone systems; documenting selective feedforward systems, novel candidate signaling architectures, new coupling motifs, and the highly complex architecture of the mammalian AII amacrine cell. This is but the beginning, as the underlying principles of connectomics are readily transferrable to non-neural cell complexes and provide new contexts for assessing intercellular communication.",0,"Retinal connectomics: Towards complete, accurate networks. Connectomics is a strategy for mapping complex neural networks based on high-speed automated electron optical imaging, computational assembly of neural data volumes, web-based navigational tools to explore 1012--1015 byte (terabyte to petabyte) image volumes, and annotation and markup tools to convert images into rich networks with cellular metadata. These collections of network data and associated metadata, analyzed using tools from graph theory and classification theory, can be merged with classical systems theory, giving a more completely parameterized view of how biologic information processing systems are implemented in retina and brain. Networks have two separable features: topology and connection attributes. The first findings from connectomics strongly validate the idea that the topologies of complete retinal networks are far more complex than the simple schematics that emerged from classical anatomy. In particular, connectomics has permitted an aggressive refactoring of the retinal inner plexiform layer, demonstrating that network function cannot be simply inferred from stratification; exposing the complex geometric rules for inserting different cells into a shared network; revealing unexpected bidirectional signaling pathways between mammalian rod and cone systems; documenting selective feedforward systems, novel candidate signaling architectures, new coupling motifs, and the highly complex architecture of the mammalian AII amacrine cell. This is but the beginning, as the underlying principles of connectomics are readily transferrable to non-neural cell complexes and provide new contexts for assessing intercellular communication.",1,2,0,0,1,1
2,"Chapter 7 - Modernization Case Study: Italian Ministry of Instruction, University, and Research",,"Publisher Summary
This chapter presents a case study focusing on The Italian Ministry of Instruction, University, and Research (MIUR). MIUR desired to outsource its IT infrastructure, as well as modernize the IT systems supporting its most important administrative, financial, and HR processes. This study describes the modernization component of the project, which was executed by Hewlett-Packard with support from several partners. The overall process included a variety of transformation strategies applied across characteristic sets of applications based on a business case and application analysis. The strategies included replacement with commercial off-the-shelf software (COTS), rehosting to less expensive platforms, refactoring user interface (UI) components as part of Web enablement, and rearchitecting and rewriting applications. The target platform was Oracle/UNIX running both Oracle applications and the migrated J2EE, and batch COBOL applications. The applications are now accessed through more efficient Web-based UIs (replacing green screens) and provide improved navigation with better overall system performance. End-user productivity has doubled in terms of daily operations for some business processes. The application portfolio has been greatly simplified---function point counts dropped by 33%. From a financial perspective, the results are significant.",0,"Chapter 7 - Modernization Case Study: Italian Ministry of Instruction, University, and Research. Publisher Summary
This chapter presents a case study focusing on The Italian Ministry of Instruction, University, and Research (MIUR). MIUR desired to outsource its IT infrastructure, as well as modernize the IT systems supporting its most important administrative, financial, and HR processes. This study describes the modernization component of the project, which was executed by Hewlett-Packard with support from several partners. The overall process included a variety of transformation strategies applied across characteristic sets of applications based on a business case and application analysis. The strategies included replacement with commercial off-the-shelf software (COTS), rehosting to less expensive platforms, refactoring user interface (UI) components as part of Web enablement, and rearchitecting and rewriting applications. The target platform was Oracle/UNIX running both Oracle applications and the migrated J2EE, and batch COBOL applications. The applications are now accessed through more efficient Web-based UIs (replacing green screens) and provide improved navigation with better overall system performance. End-user productivity has doubled in terms of daily operations for some business processes. The application portfolio has been greatly simplified---function point counts dropped by 33%. From a financial perspective, the results are significant.",0,3,0,0,0,1
3,High-level replacement units and their termination properties,"Visual transformations, Transformation units, High level replacement, Termination, Refactoring","Visual rewriting techniques, in particular graph transformations, are increasingly used to model transformations of systems specified through diagrammatic sentences. Several rewriting models have been proposed, differing in the expressivity of the types of rules and in the complexity of the rewriting mechanism; yet, for many of them, basic results concerning the formal properties of these models are still missing. In this paper, we give a contribution towards solving the termination problem for rewriting systems with external control mechanisms. In particular, we obtain results of more general validity by extending the concept of transformation unit to high-level replacement systems, a generalization of graph transformation systems. For high-level replacement units, we state and prove several abstract properties based on termination criteria. Then, we instantiate the high-level replacement systems by attributed graph transformation systems and present concrete termination criteria. We explore some types of rules and replacement units for which the criterion can be established. These are used to show the termination of some replacement units needed to express model transformations formalizing refactoring.",0,"High-level replacement units and their termination properties. Visual rewriting techniques, in particular graph transformations, are increasingly used to model transformations of systems specified through diagrammatic sentences. Several rewriting models have been proposed, differing in the expressivity of the types of rules and in the complexity of the rewriting mechanism; yet, for many of them, basic results concerning the formal properties of these models are still missing. In this paper, we give a contribution towards solving the termination problem for rewriting systems with external control mechanisms. In particular, we obtain results of more general validity by extending the concept of transformation unit to high-level replacement systems, a generalization of graph transformation systems. For high-level replacement units, we state and prove several abstract properties based on termination criteria. Then, we instantiate the high-level replacement systems by attributed graph transformation systems and present concrete termination criteria. We explore some types of rules and replacement units for which the criterion can be established. These are used to show the termination of some replacement units needed to express model transformations formalizing refactoring.",1,1,0,0,1,4
4,A graph mining approach for detecting identical design structures in object-oriented design models,"Software design models, Identical design structures, Software motifs, Pattern extraction, Graph mining","The object-oriented approach has been the most popular software design methodology for the past twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs that are designed for the specific application domains. Another commonly used technique is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining-based approach for detecting identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and reused design clones, which could help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source and industrial projects show that we can successfully find many identical design structures within a project (intra-project) and between different projects (inter-project). We observe that usually most of the detected identical structures are an implementation of common design patterns; however, we also detect various anti-patterns, domain-specific patterns, reused design parts and design-level clones.",0,"A graph mining approach for detecting identical design structures in object-oriented design models. The object-oriented approach has been the most popular software design methodology for the past twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs that are designed for the specific application domains. Another commonly used technique is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining-based approach for detecting identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and reused design clones, which could help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source and industrial projects show that we can successfully find many identical design structures within a project (intra-project) and between different projects (inter-project). We observe that usually most of the detected identical structures are an implementation of common design patterns; however, we also detect various anti-patterns, domain-specific patterns, reused design parts and design-level clones.",0,2,1,1,1,1
5,Digital rights management architectures,"Digital rights management, Interoperability, Multimedia architecture","Digital rights management (DRM) is increasingly becoming a necessity for content management and distribution in highly networked environments such as the Internet. However, very few DRM models have been able to achieve commercial success and acceptance among users. This paper analyzes the problems with current DRM environments and proposes an open layered framework for development of DRM systems, where different technologies can interoperate within the framework. Furthermore, interoperability is studied in terms of the proposed layered framework, and problems posed by the current rights expression languages (RELs) are identified. We conclude that a refactoring of current RELs based on a set of design principles is necessary to achieve a reasonable level of DRM interoperability. We emphasize the need for middleware services for DRM, along with their responsibilities and places of operation within the proposed framework. Finally, a specific prototype architecture is introduced that makes use of existing infrastructures in order to implement a DRM environment consistent with the design principles described in this paper.",0,"Digital rights management architectures. Digital rights management (DRM) is increasingly becoming a necessity for content management and distribution in highly networked environments such as the Internet. However, very few DRM models have been able to achieve commercial success and acceptance among users. This paper analyzes the problems with current DRM environments and proposes an open layered framework for development of DRM systems, where different technologies can interoperate within the framework. Furthermore, interoperability is studied in terms of the proposed layered framework, and problems posed by the current rights expression languages (RELs) are identified. We conclude that a refactoring of current RELs based on a set of design principles is necessary to achieve a reasonable level of DRM interoperability. We emphasize the need for middleware services for DRM, along with their responsibilities and places of operation within the proposed framework. Finally, a specific prototype architecture is introduced that makes use of existing infrastructures in order to implement a DRM environment consistent with the design principles described in this paper.",0,3,0,0,0,1
6,"The Haskell Refactorer, HaRe, and its API","Haskell, refactoring, HaRe, program transformation API, source code, layout preservation, strategic programming, Strafunski, Programatica","We demonstrate the Haskell Refactorer, HaRe, both as an example of a fully-functional tool for a complete (functional) programming language, and to show the API which HaRe provides for building source-level program transformations for Haskell. We comment on the challenges presented by the construction of this and similar tools for language frameworks and processors.",0,"The Haskell Refactorer, HaRe, and its API. We demonstrate the Haskell Refactorer, HaRe, both as an example of a fully-functional tool for a complete (functional) programming language, and to show the API which HaRe provides for building source-level program transformations for Haskell. We comment on the challenges presented by the construction of this and similar tools for language frameworks and processors.",1,1,0,0,1,4
7,Chapter 9 - Implementing the model-driven architecture,,"Publisher Summary
There has been a long history of research on the use of transformations to carry out design and program refinement. Gries used transformations to map specifications into programs that established the specifications. Extensive strategies and tool support for design refinement by transformation were developed in the 1980s and 1990s, leading to systems such as KIDS. This chapter describes the concepts and terminology of the model-driven architecture (MDA), and the role of transformations and refactoring. It is shown how transformations can be formally represented and justified and a survey of current MDA tools is presented. A Computation Independent Model (CIM) abstracts completely from implementation details, and uses terminology familiar to practitioners in the domain of the application---it is similar to the concept of a domain model, and can be used to assist in establishing requirements and expressing these in details. It describes the system in terms of the domain in which it must operate. Transformations on models are fundamental to the MDA approach: either as a means to improve a model to make it more generic and flexible for use as a Platform-Independent Model (PIM), or as a means to transform PIMs into Platform-Specific Model (PSMs). For reverse-engineering or round-trip engineering, PSM to PIM transformations, essentially ways of abstracting away from model-specific details are also important. This chapter also describes a number of useful transformations and gives a framework for formally representing and reasoning transformations.",0,"Chapter 9 - Implementing the model-driven architecture. Publisher Summary
There has been a long history of research on the use of transformations to carry out design and program refinement. Gries used transformations to map specifications into programs that established the specifications. Extensive strategies and tool support for design refinement by transformation were developed in the 1980s and 1990s, leading to systems such as KIDS. This chapter describes the concepts and terminology of the model-driven architecture (MDA), and the role of transformations and refactoring. It is shown how transformations can be formally represented and justified and a survey of current MDA tools is presented. A Computation Independent Model (CIM) abstracts completely from implementation details, and uses terminology familiar to practitioners in the domain of the application---it is similar to the concept of a domain model, and can be used to assist in establishing requirements and expressing these in details. It describes the system in terms of the domain in which it must operate. Transformations on models are fundamental to the MDA approach: either as a means to improve a model to make it more generic and flexible for use as a Platform-Independent Model (PIM), or as a means to transform PIMs into Platform-Specific Model (PSMs). For reverse-engineering or round-trip engineering, PSM to PIM transformations, essentially ways of abstracting away from model-specific details are also important. This chapter also describes a number of useful transformations and gives a framework for formally representing and reasoning transformations.",1,3,2,0,0,1
8,EMAN2: An extensible image processing suite for electron microscopy,"EMAN, Single particle analysis, cryoEM, TEM, Software, Image processing, Electron microscopy","EMAN is a scientific image processing package with a particular focus on single particle reconstruction from transmission electron microscopy (TEM) images. It was first released in 1999, and new versions have been released typically 2--3 times each year since that time. EMAN2 has been under development for the last two years, with a completely refactored image processing library, and a wide range of features to make it much more flexible and extensible than EMAN1. The user-level programs are better documented, more straightforward to use, and written in the Python scripting language, so advanced users can modify the programs' behavior without any recompilation. A completely rewritten 3D transformation class simplifies translation between Euler angle standards and symmetry conventions. The core C++ library has over 500 functions for image processing and associated tasks, and it is modular with introspection capabilities, so programmers can add new algorithms with minimal effort and programs can incorporate new capabilities automatically. Finally, a flexible new parallelism system has been designed to address the shortcomings in the rigid system in EMAN1.",0,"EMAN2: An extensible image processing suite for electron microscopy. EMAN is a scientific image processing package with a particular focus on single particle reconstruction from transmission electron microscopy (TEM) images. It was first released in 1999, and new versions have been released typically 2--3 times each year since that time. EMAN2 has been under development for the last two years, with a completely refactored image processing library, and a wide range of features to make it much more flexible and extensible than EMAN1. The user-level programs are better documented, more straightforward to use, and written in the Python scripting language, so advanced users can modify the programs' behavior without any recompilation. A completely rewritten 3D transformation class simplifies translation between Euler angle standards and symmetry conventions. The core C++ library has over 500 functions for image processing and associated tasks, and it is modular with introspection capabilities, so programmers can add new algorithms with minimal effort and programs can incorporate new capabilities automatically. Finally, a flexible new parallelism system has been designed to address the shortcomings in the rigid system in EMAN1.",1,3,0,0,1,1
9,Refactoring: Current Research and Future Trends,,"In this paper we provide an detailed overview of existing research in the field of software restructuring and refactoring, from a formal as well as a practical point of view. Next, we propose an extensive list of open questions that indicate future research directions, and we provide some partial answers to these questions.",0,"Refactoring: Current Research and Future Trends. In this paper we provide an detailed overview of existing research in the field of software restructuring and refactoring, from a formal as well as a practical point of view. Next, we propose an extensive list of open questions that indicate future research directions, and we provide some partial answers to these questions.",0,3,0,0,0,2
10,Database design for ecologists: Composing core entities with observations,"Data visualization, Domain-specific conceptual structures, Ecoinformatics, End-user programming, Forest canopy, Semantic data integration, Database design","The ecoinformatics community recognizes that ecological synthesis across studies, space, and time will require new informatics tools and infrastructure. Recent advances have been encouraging, but many problems still face ecologists who manage their own datasets, prepare data for archiving, and search data stores for synthetic research. In this paper, we describe how work by the Canopy Database Project (CDP) might enable use of database technology by field ecologists: increasing the quality of database design, improving data validation, and providing structural and semantic metadata --- all of which might improve the quality of data archives and thereby help drive ecological synthesis. The CDP has experimented with conceptual components for database design, templates, to address information technology issues facing ecologists. Templates represent forest structures and observational measurements on these structures. Using our software, researchers select templates to represent their study's data and can generate normalized relational databases. Information hidden in those databases is used by ancillary tools, including data intake forms and simple data validation, data visualization, and metadata export. The primary question we address in this paper is, which templates are the right templates. We argue for defining simple templates (with relatively few attributes) that describe the domain's major entities, and for coupling those with focused and flexible observation templates. We present a conceptual model for the observation data type, and show how we have implemented the model as an observation entity in the DataBank database designer and generator. We show how our visualization tool CanopyView exploits metadata made explicit by DataBank to help scientists with analysis and synthesis. We conclude by presenting future plans for tools to conduct statistical calculations common to forest ecology and to enhance data mining with DataBank databases. DataBank could be extended to another domain by replacing our forest--ecology-specific templates with those for the new domain. This work extends the basic computer science idea of abstract data types and user-defined types to ecology-specific database design tools for individual users, and applies to ecoinformatics the software engineering innovations of domain-specific languages, software patterns, components, refactoring, and end-user programming.",0,"Database design for ecologists: Composing core entities with observations. The ecoinformatics community recognizes that ecological synthesis across studies, space, and time will require new informatics tools and infrastructure. Recent advances have been encouraging, but many problems still face ecologists who manage their own datasets, prepare data for archiving, and search data stores for synthetic research. In this paper, we describe how work by the Canopy Database Project (CDP) might enable use of database technology by field ecologists: increasing the quality of database design, improving data validation, and providing structural and semantic metadata --- all of which might improve the quality of data archives and thereby help drive ecological synthesis. The CDP has experimented with conceptual components for database design, templates, to address information technology issues facing ecologists. Templates represent forest structures and observational measurements on these structures. Using our software, researchers select templates to represent their study's data and can generate normalized relational databases. Information hidden in those databases is used by ancillary tools, including data intake forms and simple data validation, data visualization, and metadata export. The primary question we address in this paper is, which templates are the right templates. We argue for defining simple templates (with relatively few attributes) that describe the domain's major entities, and for coupling those with focused and flexible observation templates. We present a conceptual model for the observation data type, and show how we have implemented the model as an observation entity in the DataBank database designer and generator. We show how our visualization tool CanopyView exploits metadata made explicit by DataBank to help scientists with analysis and synthesis. We conclude by presenting future plans for tools to conduct statistical calculations common to forest ecology and to enhance data mining with DataBank databases. DataBank could be extended to another domain by replacing our forest--ecology-specific templates with those for the new domain. This work extends the basic computer science idea of abstract data types and user-defined types to ecology-specific database design tools for individual users, and applies to ecoinformatics the software engineering innovations of domain-specific languages, software patterns, components, refactoring, and end-user programming.",0,0,0,0,2,2
11,MudPie: layers in the ball of mud,"Code analysis, Program dependence graph, Program maintenance, Program understanding, Refactoring, Reverse engineering, Smalltalk, Software architecture, Strongly connected components","The uses-hierarchy of a Smalltalk program's packages is not easily visible to its maintainer. This sets the stage for a common error---extending a low-level package in a way that makes it depend on a higher level package. Such a mistake introduces a cyclic dependency, which prevents the low-level package, and all others in the cycle, from being reused independently. This paper describes a tool called MudPie that uses well-known techniques to visualize the dependency structure as it is reflected in the code. We apply these techniques to Smalltalk and show how SUnit tests can detect the cycles as they occur. This can help programmers learn a system's package hierarchy and avoid breaking it.",0,"MudPie: layers in the ball of mud. The uses-hierarchy of a Smalltalk program's packages is not easily visible to its maintainer. This sets the stage for a common error---extending a low-level package in a way that makes it depend on a higher level package. Such a mistake introduces a cyclic dependency, which prevents the low-level package, and all others in the cycle, from being reused independently. This paper describes a tool called MudPie that uses well-known techniques to visualize the dependency structure as it is reflected in the code. We apply these techniques to Smalltalk and show how SUnit tests can detect the cycles as they occur. This can help programmers learn a system's package hierarchy and avoid breaking it.",0,1,0,0,1,1
12,Chapter 5 - Detailed Model Capture,,"Publisher Summary
This chapter focuses on detailed model capture. Most of the patterns concerned with ``Detailed Model Capture'' entail extensive technical knowledge, use of tools, and investment of effort. This is only natural because only after building the ``Initial Understanding'', the chances of return from intensive investment of effort can be determined. The patterns of ``Detailed Model Capture'' propose a series of activities that help to expose design artifacts, which are hidden in the code. Although some of these patterns, in particular tie code and questions, are lightweight, most of them entail considerable effort, so the expectations regarding the outcome after their application should be carefully evaluated. The chapter suggests some possible relationships between the patterns.. Tie code and questions is perhaps the most fundamental of these patterns, and the easiest to apply. While working through the source code, the track of comments, questions, hypotheses, and possible actions should be kept by directly annotating the source code at the point where the comment applies.. This pattern works well with the other patterns in this chapter and can be productively applied throughout a re-engineering project. Refactor to Understand helps to expose the design of cryptic code. It is important to understand that the intent of this pattern is not to improve the code base itself, but only to improve understanding.",0,"Chapter 5 - Detailed Model Capture. Publisher Summary
This chapter focuses on detailed model capture. Most of the patterns concerned with ``Detailed Model Capture'' entail extensive technical knowledge, use of tools, and investment of effort. This is only natural because only after building the ``Initial Understanding'', the chances of return from intensive investment of effort can be determined. The patterns of ``Detailed Model Capture'' propose a series of activities that help to expose design artifacts, which are hidden in the code. Although some of these patterns, in particular tie code and questions, are lightweight, most of them entail considerable effort, so the expectations regarding the outcome after their application should be carefully evaluated. The chapter suggests some possible relationships between the patterns.. Tie code and questions is perhaps the most fundamental of these patterns, and the easiest to apply. While working through the source code, the track of comments, questions, hypotheses, and possible actions should be kept by directly annotating the source code at the point where the comment applies.. This pattern works well with the other patterns in this chapter and can be productively applied throughout a re-engineering project. Refactor to Understand helps to expose the design of cryptic code. It is important to understand that the intent of this pattern is not to improve the code base itself, but only to improve understanding.",0,3,0,0,2,1
13,Chapter 12 - Software Architectures for Real-Time Embedded Systems,"Software architecture, Real-time tasks, Worst-case task execution time, Round-robin architecture, Hardware concurrency, Round-robin with interrupts, Queue-based architecture","Software architecture can affect the performance of a real-time embedded system just as well as hardware architecture. In this chapter, we start with notation for specifying real-time tasks, and introduce three software architectures widely used in real-time embedded systems. The first software architecture is based on the well-known round-robin principle, where a system is composed of a series of ``detect-acknowledge-service'' patterns one for each device of interest. The round-robin architecture suffers from low hardware concurrency because the outstanding period of a service request can be as long as one round of execution in the worst case. The second software architecture is called round robin with interrupts, which brings the interrupt concept into play. Although it can significantly improve hardware concurrency, because of the round-robin nature, it still takes one round of execution time to complete a service request in the worst case. The round-robin architecture with interrupts is then refactored into a queue-based architecture, which allows us to further consider queues with various ordering policies. In particular, we study one architecture based on a FIFO queue and one based on a priority queue. The architecture based on a priority queue brings us one step closer to real-time operating systems---the most powerful yet expensive architecture for real-time embedded systems.",0,"Chapter 12 - Software Architectures for Real-Time Embedded Systems. Software architecture can affect the performance of a real-time embedded system just as well as hardware architecture. In this chapter, we start with notation for specifying real-time tasks, and introduce three software architectures widely used in real-time embedded systems. The first software architecture is based on the well-known round-robin principle, where a system is composed of a series of ``detect-acknowledge-service'' patterns one for each device of interest. The round-robin architecture suffers from low hardware concurrency because the outstanding period of a service request can be as long as one round of execution in the worst case. The second software architecture is called round robin with interrupts, which brings the interrupt concept into play. Although it can significantly improve hardware concurrency, because of the round-robin nature, it still takes one round of execution time to complete a service request in the worst case. The round-robin architecture with interrupts is then refactored into a queue-based architecture, which allows us to further consider queues with various ordering policies. In particular, we study one architecture based on a FIFO queue and one based on a priority queue. The architecture based on a priority queue brings us one step closer to real-time operating systems---the most powerful yet expensive architecture for real-time embedded systems.",0,2,0,0,1,1
14,Traits at work: The design of a new trait-based stream library,"Object-oriented programming, Inheritance, Refactoring, Traits, Code reuse, Smalltalk","Recent years saw the development of a composition mechanism called traits. Traits are pure units of behavior that can be composed to form classes or other traits. The trait composition mechanism is an alternative to multiple or mixin inheritance in which the composer has full control over the trait composition. To evaluate the expressiveness of traits, some hierarchies were refactored, showing code reuse. However, such large refactorings, while valuable, may not exhibit all possible composition problems, since the hierarchies were previously expressed using single inheritance and following certain patterns. This paper presents our work on designing and implementing a new trait-based stream library named Nile. It evaluates how far traits enable reuse, what problems can be encountered when building a library using traits from scratch and compares the traits solution to alternative composition mechanisms. Nile's core allows the definition of compact collection and file streaming libraries as well as the implementation of a backward-compatible new stream library. Nile method size shows a reduction of 40% compared to the Squeak equivalent. The possibility to reuse the same set of traits to implement two distinct libraries is a concrete illustration of trait reuse capability.",0,"Traits at work: The design of a new trait-based stream library. Recent years saw the development of a composition mechanism called traits. Traits are pure units of behavior that can be composed to form classes or other traits. The trait composition mechanism is an alternative to multiple or mixin inheritance in which the composer has full control over the trait composition. To evaluate the expressiveness of traits, some hierarchies were refactored, showing code reuse. However, such large refactorings, while valuable, may not exhibit all possible composition problems, since the hierarchies were previously expressed using single inheritance and following certain patterns. This paper presents our work on designing and implementing a new trait-based stream library named Nile. It evaluates how far traits enable reuse, what problems can be encountered when building a library using traits from scratch and compares the traits solution to alternative composition mechanisms. Nile's core allows the definition of compact collection and file streaming libraries as well as the implementation of a backward-compatible new stream library. Nile method size shows a reduction of 40% compared to the Squeak equivalent. The possibility to reuse the same set of traits to implement two distinct libraries is a concrete illustration of trait reuse capability.",0,1,0,0,1,4
15,Reengineering from Tradition to Cloud: A Case Study,"Reengingeering, Cloud Computing, Software Engingeering, Onling Course ;","In China more and more universities are updating their online course system to Cloud-based Education Service, which actually is a re-engineering of online course system. While ``Cloud-based Education Service'' is not fullfledged, universities have lots of difficulties to deal with in the process of this transition. But what is for sure is that ``Cloud-based Education Service'' is based on ``Could Computing Service.'' To port the frame and refactor the data access layer of the existing system, we use Google App Engine, Google Web Toolkit and Ext-GWT with serviceoriented design. As a result, we successfully port the traditional online course system to Google Cloud Computing Platform. And it is proved that service-oriented developing style Google Cloud Computing Service provided is a scientific way to develop ``Cloud-based Education Service''. It has the key functions that future ``Cloud-based Education Service'' requires. Our conclusion is that as Cloud Computing platform supports more and more traditional frameworks, less and less effort will be made to reengineer the online course system.",0,"Reengineering from Tradition to Cloud: A Case Study. In China more and more universities are updating their online course system to Cloud-based Education Service, which actually is a re-engineering of online course system. While ``Cloud-based Education Service'' is not fullfledged, universities have lots of difficulties to deal with in the process of this transition. But what is for sure is that ``Cloud-based Education Service'' is based on ``Could Computing Service.'' To port the frame and refactor the data access layer of the existing system, we use Google App Engine, Google Web Toolkit and Ext-GWT with serviceoriented design. As a result, we successfully port the traditional online course system to Google Cloud Computing Platform. And it is proved that service-oriented developing style Google Cloud Computing Service provided is a scientific way to develop ``Cloud-based Education Service''. It has the key functions that future ``Cloud-based Education Service'' requires. Our conclusion is that as Cloud Computing platform supports more and more traditional frameworks, less and less effort will be made to reengineer the online course system.",0,3,0,0,0,1
16,An instrumentation framework for the critical task of measurement collection in the future Internet,"Network measurements, Instrumentation system, Software-Defined Measurement, Performance evaluation, OML","Experimental research on future Internet technologies involves observing multiple metrics at various distributed points of the networks under study. Collecting these measurements is often a tedious, repetitive and error prone task, be it in a testbed or in an uncontrolled field experiment. The relevant experimental data is usually scattered across multiple hosts in potentially different formats, and sometimes buried amongst a trove of other measurements, irrelevant to the current study. Collecting, selecting and formatting the useful measurements is a time-consuming and error-prone manual operation. In this paper, we present a conceptual Software-Defined Measurement (SDM) framework to facilitate this task. It includes a common representation for any type of experimental data, as well as the elements to process and collect the measurement samples and their associated metadata. We then present an implementation of this concept, which we built as a major extension and refactoring of the existing Orbit Measurement Library (OML). We outline its API, and how it can be used to instrument an experiment in only a few lines of code. We also evaluate the current implementation, and demonstrate that it efficiently allows measurement collection without interfering with the systems under observation.",0,"An instrumentation framework for the critical task of measurement collection in the future Internet. Experimental research on future Internet technologies involves observing multiple metrics at various distributed points of the networks under study. Collecting these measurements is often a tedious, repetitive and error prone task, be it in a testbed or in an uncontrolled field experiment. The relevant experimental data is usually scattered across multiple hosts in potentially different formats, and sometimes buried amongst a trove of other measurements, irrelevant to the current study. Collecting, selecting and formatting the useful measurements is a time-consuming and error-prone manual operation. In this paper, we present a conceptual Software-Defined Measurement (SDM) framework to facilitate this task. It includes a common representation for any type of experimental data, as well as the elements to process and collect the measurement samples and their associated metadata. We then present an implementation of this concept, which we built as a major extension and refactoring of the existing Orbit Measurement Library (OML). We outline its API, and how it can be used to instrument an experiment in only a few lines of code. We also evaluate the current implementation, and demonstrate that it efficiently allows measurement collection without interfering with the systems under observation.",1,3,0,0,2,3
17,rbFeatures: Feature-oriented programming with Ruby,"Feature-oriented programming, Domain-specific languages, Dynamic programming languages","Features are pieces of core functionality of a program that is relevant to particular stakeholders. Features pose dependencies and constraints among each other. These dependencies and constraints describe the possible number of variants of the program: A valid feature configuration generates a specific variant with unique behavior. Feature-Oriented Programming is used to implement features as program units. This paper introduces rbFeatures, a feature-oriented programming language implemented on top of the dynamic programming language Ruby. With rbFeatures, programmers use software product lines, variants, and features as first-class entities. This allows several runtime reflection and modification capabilities, including the extension of the product line with new features and the provision of multiple variants. The paper gives a broad overview to the implementation and application of rbFeatures. We explain how features as first-class entities are designed and implemented, and discuss how the semantics of features are carefully added to Ruby programs. We show two case studies: The expression product line, a common example in feature-oriented programming, and a web application.",0,"rbFeatures: Feature-oriented programming with Ruby. Features are pieces of core functionality of a program that is relevant to particular stakeholders. Features pose dependencies and constraints among each other. These dependencies and constraints describe the possible number of variants of the program: A valid feature configuration generates a specific variant with unique behavior. Feature-Oriented Programming is used to implement features as program units. This paper introduces rbFeatures, a feature-oriented programming language implemented on top of the dynamic programming language Ruby. With rbFeatures, programmers use software product lines, variants, and features as first-class entities. This allows several runtime reflection and modification capabilities, including the extension of the product line with new features and the provision of multiple variants. The paper gives a broad overview to the implementation and application of rbFeatures. We explain how features as first-class entities are designed and implemented, and discuss how the semantics of features are carefully added to Ruby programs. We show two case studies: The expression product line, a common example in feature-oriented programming, and a web application.",1,1,2,0,1,4
18,"To lock, or not to lock: That is the question","Concurrency control, Software Configuration Management, Version control","Mechanisms to control concurrent access over project artefacts are needed to execute the software development process in an organized way. These mechanisms are implemented by concurrency control policies in version control systems that may inhibit (i.e. `to lock') or allow (i.e., `not to lock') parallel development. This work presents a novel approach named Orion that analyzes the project's historical changes and suggests the most appropriate concurrency control policy for each software element. This suggestion aims at minimizing conflict situations and thus improving the productivity of the development team. In addition, it identifies critical elements that do not work well with any of these policies and are candidates to refactoring. We evaluated Orion through two experimental studies and the results, which indicated it was effective, led us to a prototype implementation. Apart from the Orion approach this paper also presents the planning, execution, and analysis stages of the evaluation, and details of prototype internals.",0,"To lock, or not to lock: That is the question. Mechanisms to control concurrent access over project artefacts are needed to execute the software development process in an organized way. These mechanisms are implemented by concurrency control policies in version control systems that may inhibit (i.e. `to lock') or allow (i.e., `not to lock') parallel development. This work presents a novel approach named Orion that analyzes the project's historical changes and suggests the most appropriate concurrency control policy for each software element. This suggestion aims at minimizing conflict situations and thus improving the productivity of the development team. In addition, it identifies critical elements that do not work well with any of these policies and are candidates to refactoring. We evaluated Orion through two experimental studies and the results, which indicated it was effective, led us to a prototype implementation. Apart from the Orion approach this paper also presents the planning, execution, and analysis stages of the evaluation, and details of prototype internals.",0,3,0,0,0,1
19,The BORM methodology: a third-generation fully object-oriented methodology,"Business object relationship modelling, Prototype, Class refactoring","Business object relationship modelling (BORM) is a development methodology developed to capture Knowledge of typical business systems. It has been in development since 1993 and has proved an increasingly effective method which is popular with both users and developers. The effectiveness gained is largely as a result of a unified and simple method for presenting all aspects of the relevant model. This paper outlines BORM, its tools, methods and its differences from other similar development methodologies.",0,"The BORM methodology: a third-generation fully object-oriented methodology. Business object relationship modelling (BORM) is a development methodology developed to capture Knowledge of typical business systems. It has been in development since 1993 and has proved an increasingly effective method which is popular with both users and developers. The effectiveness gained is largely as a result of a unified and simple method for presenting all aspects of the relevant model. This paper outlines BORM, its tools, methods and its differences from other similar development methodologies.",0,3,1,1,2,1
20,The effects of design pattern application on metric scores,"Software quality, Object-orientation, Design metrics, Design patterns, Refactoring","One method suggested for improving software quality has been that of collecting metric scores for a given design, and refactoring in response to what are deemed to be unsatisfactory metric values. More recently, the usage of design patterns has been recommended to promote adaptable designs, so reducing maintenance effort. These two approaches are therefore observed to effectively have the same general aim. The question then arises as to whether design metrics and design patterns are always compatible, and where this is not found to be the case whether the metric, the pattern or both are anomalous. Methods of analysis are presented which demonstrate the effects of applying various patterns on certain metric scores, the initial conclusion being that the two approaches are indeed mainly congruent.",0,"The effects of design pattern application on metric scores. One method suggested for improving software quality has been that of collecting metric scores for a given design, and refactoring in response to what are deemed to be unsatisfactory metric values. More recently, the usage of design patterns has been recommended to promote adaptable designs, so reducing maintenance effort. These two approaches are therefore observed to effectively have the same general aim. The question then arises as to whether design metrics and design patterns are always compatible, and where this is not found to be the case whether the metric, the pattern or both are anomalous. Methods of analysis are presented which demonstrate the effects of applying various patterns on certain metric scores, the initial conclusion being that the two approaches are indeed mainly congruent.",0,3,0,0,2,3
21,The impact of accounting for special methods in the measurement of object-oriented class cohesion on refactoring and fault prediction activities,"Object-oriented design, Class quality, Class cohesion, Cohesion metric, Special methods, Refactoring, Fault prediction","Class cohesion is a key attribute that is used to assess the design quality of a class, and it refers to the extent to which the attributes and methods of the class are related. Typically, classes contain special types of methods, such as constructors, destructors, and access methods. Each of these special methods has its own characteristics, which can artificially affect the class cohesion measurement. Several metrics have been proposed in the literature to indicate class cohesion during high- or low-level design phases. The impact of accounting for special methods in cohesion measurement has not been addressed for most of these metrics. This paper empirically explores the impact of including or excluding special methods on cohesion measurements that were performed using 20 existing class cohesion metrics. The empirical study applies the metrics that were considered to five open-source systems under four different scenarios, including (1) considering all special methods, (2) ignoring only constructors, (3) ignoring only access methods, and (4) ignoring all special methods. This study empirically explores the impact of including special methods in cohesion measurement for two applications of interest to software practitioners, including refactoring and predicting faulty classes. The results of the empirical studies show that the cohesion values for most of the metrics considered differ significantly across the four scenarios and that this difference significantly affects the refactoring decisions, but does not significantly affect the abilities of the metrics to predict faulty classes.",0,"The impact of accounting for special methods in the measurement of object-oriented class cohesion on refactoring and fault prediction activities. Class cohesion is a key attribute that is used to assess the design quality of a class, and it refers to the extent to which the attributes and methods of the class are related. Typically, classes contain special types of methods, such as constructors, destructors, and access methods. Each of these special methods has its own characteristics, which can artificially affect the class cohesion measurement. Several metrics have been proposed in the literature to indicate class cohesion during high- or low-level design phases. The impact of accounting for special methods in cohesion measurement has not been addressed for most of these metrics. This paper empirically explores the impact of including or excluding special methods on cohesion measurements that were performed using 20 existing class cohesion metrics. The empirical study applies the metrics that were considered to five open-source systems under four different scenarios, including (1) considering all special methods, (2) ignoring only constructors, (3) ignoring only access methods, and (4) ignoring all special methods. This study empirically explores the impact of including special methods in cohesion measurement for two applications of interest to software practitioners, including refactoring and predicting faulty classes. The results of the empirical studies show that the cohesion values for most of the metrics considered differ significantly across the four scenarios and that this difference significantly affects the refactoring decisions, but does not significantly affect the abilities of the metrics to predict faulty classes.",0,2,0,0,2,3
22,A survey on search-based software design,"Search-based software engineering, Software design, Search algorithms, Software quality","This survey investigates search-based approaches to software design. The basics of the most popular meta-heuristic algorithms are presented as background to the search-based viewpoint. Software design is considered from a wide viewpoint, including topics that can also be categorized as software maintenance or re-engineering. Search-based approaches have been used in research from the high architecture design level to software clustering and finally software refactoring. Enhancing and predicting software quality with search-based methods is also taken into account as a part of the design process. The background for the underlying software engineering problems is discussed, after which search-based approaches are presented. Summarizing remarks and tables collecting the fundamental issues of approaches for each type of problem are given. The choices regarding critical decisions, such as representation and fitness function, when used in meta-heuristic search algorithms, are emphasized and discussed in detail. Ideas for future research directions are also given.",0,"A survey on search-based software design. This survey investigates search-based approaches to software design. The basics of the most popular meta-heuristic algorithms are presented as background to the search-based viewpoint. Software design is considered from a wide viewpoint, including topics that can also be categorized as software maintenance or re-engineering. Search-based approaches have been used in research from the high architecture design level to software clustering and finally software refactoring. Enhancing and predicting software quality with search-based methods is also taken into account as a part of the design process. The background for the underlying software engineering problems is discussed, after which search-based approaches are presented. Summarizing remarks and tables collecting the fundamental issues of approaches for each type of problem are given. The choices regarding critical decisions, such as representation and fitness function, when used in meta-heuristic search algorithms, are emphasized and discussed in detail. Ideas for future research directions are also given.",1,3,1,1,0,3
23,Directive-based GPU programming for computational fluid dynamics,"Graphics processing unit (GPU), Directive-based programming, OpenACC, Fortran, Finite-difference method","Directive-based programming of graphics processing units (GPUs) has recently appeared as a viable alternative to using specialized low-level languages such as CUDA C and OpenCL for general-purpose GPU programming. This technique, which uses ``directive'' or ``pragma'' statements to annotate source codes written in traditional high-level languages, is designed to permit a unified code base to serve multiple computational platforms. In this work we analyze the popular OpenACC programming standard, as implemented by the PGI compiler suite, in order to evaluate its utility and performance potential in computational fluid dynamics (CFD) applications. We examine the process of applying the OpenACC Fortran API to a test CFD code that serves as a proxy for a full-scale research code developed at Virginia Tech; this test code is used to asses the performance improvements attainable for our CFD algorithm on common GPU platforms, as well as to determine the modifications that must be made to the original source code in order to run efficiently on the GPU. Performance is measured on several recent GPU architectures from NVIDIA and AMD (using both double and single precision arithmetic) and the accelerator code is benchmarked against a multithreaded CPU version constructed from the same Fortran source code using OpenMP directives. A single NVIDIA Kepler GPU card is found to perform approximately 20 faster than a single CPU core and more than 2 faster than a 16-core Xeon server. An analysis of optimization techniques for OpenACC reveals cases in which manual intervention by the programmer can improve accelerator performance by up to 30% over the default compiler heuristics, although these optimizations are relevant only for specific platforms. Additionally, the use of multiple accelerators with OpenACC is investigated, including an experimental high-level interface for multi-GPU programming that automates scheduling tasks across multiple devices. While the overall performance of the OpenACC code is found to be satisfactory, we also observe some significant limitations and restrictions imposed by the OpenACC API regarding certain useful features of modern Fortran (2003/8); these are sufficient for us to conclude that it would not be practical to apply OpenACC to our full research code at this time due to the amount of refactoring required.",0,"Directive-based GPU programming for computational fluid dynamics. Directive-based programming of graphics processing units (GPUs) has recently appeared as a viable alternative to using specialized low-level languages such as CUDA C and OpenCL for general-purpose GPU programming. This technique, which uses ``directive'' or ``pragma'' statements to annotate source codes written in traditional high-level languages, is designed to permit a unified code base to serve multiple computational platforms. In this work we analyze the popular OpenACC programming standard, as implemented by the PGI compiler suite, in order to evaluate its utility and performance potential in computational fluid dynamics (CFD) applications. We examine the process of applying the OpenACC Fortran API to a test CFD code that serves as a proxy for a full-scale research code developed at Virginia Tech; this test code is used to asses the performance improvements attainable for our CFD algorithm on common GPU platforms, as well as to determine the modifications that must be made to the original source code in order to run efficiently on the GPU. Performance is measured on several recent GPU architectures from NVIDIA and AMD (using both double and single precision arithmetic) and the accelerator code is benchmarked against a multithreaded CPU version constructed from the same Fortran source code using OpenMP directives. A single NVIDIA Kepler GPU card is found to perform approximately 20 faster than a single CPU core and more than 2 faster than a 16-core Xeon server. An analysis of optimization techniques for OpenACC reveals cases in which manual intervention by the programmer can improve accelerator performance by up to 30% over the default compiler heuristics, although these optimizations are relevant only for specific platforms. Additionally, the use of multiple accelerators with OpenACC is investigated, including an experimental high-level interface for multi-GPU programming that automates scheduling tasks across multiple devices. While the overall performance of the OpenACC code is found to be satisfactory, we also observe some significant limitations and restrictions imposed by the OpenACC API regarding certain useful features of modern Fortran (2003/8); these are sufficient for us to conclude that it would not be practical to apply OpenACC to our full research code at this time due to the amount of refactoring required.",0,0,0,0,1,1
24,Using High Performance Algorithms for the Hybrid Simulation of Disease Dynamics on CPU and GPU,"Mathematical epidemiology, Individual-based modeling, Computer simulation, Monte Carlo methods, Parallel computing, C++, MATLAB, OpenMP, MPI, GPGPU, CUDA","In the current work the authors present several approaches to the high performance simulation of human diseases propagation using hybrid two-component imitational models. The models under study were created by coupling compartmental and discrete-event submodels. The former is responsible for the simulation of the demographic processes in a population while the latter deals with a disease progression for a certain individual. The number and type of components used in a model may vary depending on the research aims and data availability. The introduced high performance approaches are based on batch random number generation, distribution of simulation runs and the calculations on graphical processor units. The emphasis was made on the possibility to use the approaches for various model types without considerable code refactoring for every particular model. The speedup gained was measured on simulation programs written in C++ and MATLAB for the models of HIV and tuberculosis spread and the models of tumor screening for the prevention of colorectal cancer. The benefits and drawbacks of the described approaches along with the future directions of their development are discussed.",0,"Using High Performance Algorithms for the Hybrid Simulation of Disease Dynamics on CPU and GPU. In the current work the authors present several approaches to the high performance simulation of human diseases propagation using hybrid two-component imitational models. The models under study were created by coupling compartmental and discrete-event submodels. The former is responsible for the simulation of the demographic processes in a population while the latter deals with a disease progression for a certain individual. The number and type of components used in a model may vary depending on the research aims and data availability. The introduced high performance approaches are based on batch random number generation, distribution of simulation runs and the calculations on graphical processor units. The emphasis was made on the possibility to use the approaches for various model types without considerable code refactoring for every particular model. The speedup gained was measured on simulation programs written in C++ and MATLAB for the models of HIV and tuberculosis spread and the models of tumor screening for the prevention of colorectal cancer. The benefits and drawbacks of the described approaches along with the future directions of their development are discussed.",1,3,0,0,2,1
25,Chapter 8 - Canonical Message Design,,"Publisher Summary
The enterprise canonical model describes a standard expression for enterprise data. It is for the common information (and processes) that can be reused across the enterprise. Although it is a different type of canonical model, a ``canonical message design'' also serves as a standard. The canonical message design is the standard expression for the information exchanged between consumers and services. Canonical message design relies on, and exploits the structural, semantic, and intuitive characteristics of XML. Prototype message examples are crafted, reviewed, and refined to reflect combinations of information content requirements, operations, message exchange patterns (MEPs), and core service-oriented architecture principles as design guidance. A number of the best practice recommendations can help to enhance the process. The example messages and element content also take advantage of the metadata already defined to the enterprise canonical model. With each step of the canonical message design process, the messages are refined and begin to reflect technical aspects of the service interface. The resulting message examples (request, reply, and fault) are in combination the canonical message design for the service interface and serve as the basis from which the XML schemas are initially generated and then further refactored. They also serve as criteria to guide decomposition into more modular and reusable schemas that can be referenced and assembled for the interface and reused by other services as well. An alternative to canonical message design is model-driven development.",0,"Chapter 8 - Canonical Message Design. Publisher Summary
The enterprise canonical model describes a standard expression for enterprise data. It is for the common information (and processes) that can be reused across the enterprise. Although it is a different type of canonical model, a ``canonical message design'' also serves as a standard. The canonical message design is the standard expression for the information exchanged between consumers and services. Canonical message design relies on, and exploits the structural, semantic, and intuitive characteristics of XML. Prototype message examples are crafted, reviewed, and refined to reflect combinations of information content requirements, operations, message exchange patterns (MEPs), and core service-oriented architecture principles as design guidance. A number of the best practice recommendations can help to enhance the process. The example messages and element content also take advantage of the metadata already defined to the enterprise canonical model. With each step of the canonical message design process, the messages are refined and begin to reflect technical aspects of the service interface. The resulting message examples (request, reply, and fault) are in combination the canonical message design for the service interface and serve as the basis from which the XML schemas are initially generated and then further refactored. They also serve as criteria to guide decomposition into more modular and reusable schemas that can be referenced and assembled for the interface and reused by other services as well. An alternative to canonical message design is model-driven development.",0,3,0,0,1,1
26,Tool Support for Proof Engineering,"IDE, IPE, proof visualization, program visualization, refactoring, program extraction, Coq, proof dependencies, proof transformations, proof strategies, proof framework, proof reuse, proof explanation",Modern integrated development environments (IDEs) provide programmers with a variety of sophisticated tools for program visualization and manipulation. These tools assist the programmer in understanding legacy code and making coordinated changes across large parts of a program. Similar tools incorporated into an integrated proof environment (IPE) would assist proof developers in understanding and manipulating the increasingly larger proofs that are being developed. In this paper we propose some tools and techniques developed for software engineering that we believe would be equally applicable in proof engineering.,0,Tool Support for Proof Engineering. Modern integrated development environments (IDEs) provide programmers with a variety of sophisticated tools for program visualization and manipulation. These tools assist the programmer in understanding legacy code and making coordinated changes across large parts of a program. Similar tools incorporated into an integrated proof environment (IPE) would assist proof developers in understanding and manipulating the increasingly larger proofs that are being developed. In this paper we propose some tools and techniques developed for software engineering that we believe would be equally applicable in proof engineering.,1,3,2,0,1,1
27,System dynamics modelling of software evolution processes for policy investigation: Approach and example,"Anti-regressive activity, Decision making, -type systems, Evolution, FEAST, Feedback, Global software process, Laws of software evolution, Management, Planning, Progressive activity, Simulation, Software process modelling, System dynamics, White-box modelling, Complexity control, Policy evaluation, Refactoring","This paper describes one of the latest in a series of system dynamics models developed during the Feedback, Evolution And Software Technology (FEAST) investigation into software evolution processes. The intention of early models was to simulate real-world processes in order to increase understanding of such processes. The work resulted in a number of lessons learnt, in particular, with regard to the application of system dynamics to the simulation of key attributes of long-term software evolution. The work reported here combines elements of previous work and extends them by describing an approach to investigate the consequences on long-term evolution, of decisions made by the managers of these processes. The approach is illustrated by discussion of the impact of complexity control activity. This model of the impact on product and global process attributes of decisions regarding the fraction of work applied to progressive and to anti-regressive activities such as complexity control, for instance, exemplifies the results of the FEAST investigation.",0,"System dynamics modelling of software evolution processes for policy investigation: Approach and example. This paper describes one of the latest in a series of system dynamics models developed during the Feedback, Evolution And Software Technology (FEAST) investigation into software evolution processes. The intention of early models was to simulate real-world processes in order to increase understanding of such processes. The work resulted in a number of lessons learnt, in particular, with regard to the application of system dynamics to the simulation of key attributes of long-term software evolution. The work reported here combines elements of previous work and extends them by describing an approach to investigate the consequences on long-term evolution, of decisions made by the managers of these processes. The approach is illustrated by discussion of the impact of complexity control activity. This model of the impact on product and global process attributes of decisions regarding the fraction of work applied to progressive and to anti-regressive activities such as complexity control, for instance, exemplifies the results of the FEAST investigation.",0,3,0,0,0,1
28,"Chapter 14 - Opportunities, Threats, and Limitations of Emergent Architecture","Emergent architecture, Alignment, Structuring, Design for understandability, Design for change, Direction of change, Agile architecture","Many discussions in the agile community circle around emergent architecture. The idea is that explicit architectural work is not needed anymore besides an initial architectural vision. Instead, the architecture would emerge from a cycle of implementation and refactoring guided by a few design principles, and this approach would automatically lead to the smallest architecture possible. This chapter shows that this proposition is only partially correct. Starting with the activities and objectives of architectural work, it shows that emergent architecture is providing a valuable alternative to conventional architecture approaches in some areas of architectural work, whereas it does not support other areas at all. On the basis of these findings, a joint approach for architectural work in an agile setting is presented.",0,"Chapter 14 - Opportunities, Threats, and Limitations of Emergent Architecture. Many discussions in the agile community circle around emergent architecture. The idea is that explicit architectural work is not needed anymore besides an initial architectural vision. Instead, the architecture would emerge from a cycle of implementation and refactoring guided by a few design principles, and this approach would automatically lead to the smallest architecture possible. This chapter shows that this proposition is only partially correct. Starting with the activities and objectives of architectural work, it shows that emergent architecture is providing a valuable alternative to conventional architecture approaches in some areas of architectural work, whereas it does not support other areas at all. On the basis of these findings, a joint approach for architectural work in an agile setting is presented.",0,3,0,0,0,1
29,Modeling resources and capabilities in enterprise architecture: A well-founded ontology-based proposal for ArchiMate,"Capability, Resource, Enterprise architecture modeling, Ontology-based semantics, ArchiMate","The importance of capabilities and resources for portfolio management and business strategy has been recognized in the management literature. Despite that, little attention has been given to integrate the notions of capabilities and resources in enterprise architecture descriptions. One notable exception is a recent proposal to extend the ArchiMate framework and language to include capability and resources and thus improve ArchiMates coverage of portfolio management. This paper presents an ontological analysis of the concepts introduced in that proposal, focusing in particular on the resource, capability and competence concepts. We provide an account for these concepts in terms of the Unified Foundational Ontology (UFO). The analysis allows us to identify semantic issues in the proposal and suggests well-founded recommendations for improvements. We revise the proposed metamodel in order to address the identified problems, thereby improving the semantic clarity and usefulness of the proposed language extension. Two real-world cases are modeled with the resulting metamodel to show the applicability of the constructs and relations in an industrial setting.",0,"Modeling resources and capabilities in enterprise architecture: A well-founded ontology-based proposal for ArchiMate. The importance of capabilities and resources for portfolio management and business strategy has been recognized in the management literature. Despite that, little attention has been given to integrate the notions of capabilities and resources in enterprise architecture descriptions. One notable exception is a recent proposal to extend the ArchiMate framework and language to include capability and resources and thus improve ArchiMates coverage of portfolio management. This paper presents an ontological analysis of the concepts introduced in that proposal, focusing in particular on the resource, capability and competence concepts. We provide an account for these concepts in terms of the Unified Foundational Ontology (UFO). The analysis allows us to identify semantic issues in the proposal and suggests well-founded recommendations for improvements. We revise the proposed metamodel in order to address the identified problems, thereby improving the semantic clarity and usefulness of the proposed language extension. Two real-world cases are modeled with the resulting metamodel to show the applicability of the constructs and relations in an industrial setting.",0,3,0,0,0,1
30,Simulating evolution in model-based product line engineering,"Product line engineering, Maintenance and evolution, Model-based development, Simulation, Industrial automation systems","Context
Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments.
Objective
In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines.
Method
We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products).
Results
We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility.
Conclusion
Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines.",0,"Simulating evolution in model-based product line engineering. Context
Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments.
Objective
In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines.
Method
We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products).
Results
We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility.
Conclusion
Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines.",0,2,2,2,2,1
31,Detecting approximate clones in business process model repositories,"Business process model, Clone detection, Model collection, Repository, Standardization","Empirical evidence shows that repositories of business process models used in industrial practice contain significant amounts of duplication. This duplication arises for example when the repository covers multiple variants of the same processes or due to copy-pasting. Previous work has addressed the problem of efficiently retrieving exact clones that can be refactored into shared subprocess models. This paper studies the broader problem of approximate clone detection in process models. The paper proposes techniques for detecting clusters of approximate clones based on two well-known clustering algorithms: DBSCAN and Hierarchical Agglomerative Clustering (HAC). The paper also defines a measure of standardizability of an approximate clone cluster, meaning the potential benefit of replacing the approximate clones with a single standardized subprocess. Experiments show that both techniques, in conjunction with the proposed standardizability measure, accurately retrieve clusters of approximate clones that originate from copy-pasting followed by independent modifications to the copied fragments. Additional experiments show that both techniques produce clusters that match those produced by human subjects and that are perceived to be standardizable.",0,"Detecting approximate clones in business process model repositories. Empirical evidence shows that repositories of business process models used in industrial practice contain significant amounts of duplication. This duplication arises for example when the repository covers multiple variants of the same processes or due to copy-pasting. Previous work has addressed the problem of efficiently retrieving exact clones that can be refactored into shared subprocess models. This paper studies the broader problem of approximate clone detection in process models. The paper proposes techniques for detecting clusters of approximate clones based on two well-known clustering algorithms: DBSCAN and Hierarchical Agglomerative Clustering (HAC). The paper also defines a measure of standardizability of an approximate clone cluster, meaning the potential benefit of replacing the approximate clones with a single standardized subprocess. Experiments show that both techniques, in conjunction with the proposed standardizability measure, accurately retrieve clusters of approximate clones that originate from copy-pasting followed by independent modifications to the copied fragments. Additional experiments show that both techniques produce clusters that match those produced by human subjects and that are perceived to be standardizable.",0,2,2,0,2,0
32,An update calculus for expressing type-safe program updates,"Program transformation, Meta programming, Type safety, Refactoring, Software evolution","The dominant share of software development costs is spent on software maintenance, particularly the process of updating programs in response to changing requirements. Currently, such program changes tend to be performed using text editors, an unreliable method that often causes many errors. In addition to syntax and type errors, logical errors can be easily introduced since text editors cannot guarantee that changes are performed consistently over the whole program. All these errors can cause a correct and perfectly running program to become instantly unusable. It is not surprising that this situation exists because the ``text-editor method'' reveals a low-level view of programs that fails to reflect the structure of programs. We address this problem by pursuing a programming-language-based approach to program updates. To this end we discuss in this paper the design and requirements of an update language for expressing update programs. We identify as the essential part of any update language a scope update that performs coordinated update of the definition and all uses of a symbol.. As the underlying basis for update languages, we define an update calculus for updating lambda calculus programs. We develop a type system for the update calculus that infers the possible type changes that can be caused by an update program. We demonstrate that type-safe update programs that fulfill certain structural constraints preserve the type correctness of lambda terms. The update calculus can serve as a basis for higher-level update languages, such as for Haskell or Java.",0,"An update calculus for expressing type-safe program updates. The dominant share of software development costs is spent on software maintenance, particularly the process of updating programs in response to changing requirements. Currently, such program changes tend to be performed using text editors, an unreliable method that often causes many errors. In addition to syntax and type errors, logical errors can be easily introduced since text editors cannot guarantee that changes are performed consistently over the whole program. All these errors can cause a correct and perfectly running program to become instantly unusable. It is not surprising that this situation exists because the ``text-editor method'' reveals a low-level view of programs that fails to reflect the structure of programs. We address this problem by pursuing a programming-language-based approach to program updates. To this end we discuss in this paper the design and requirements of an update language for expressing update programs. We identify as the essential part of any update language a scope update that performs coordinated update of the definition and all uses of a symbol.. As the underlying basis for update languages, we define an update calculus for updating lambda calculus programs. We develop a type system for the update calculus that infers the possible type changes that can be caused by an update program. We demonstrate that type-safe update programs that fulfill certain structural constraints preserve the type correctness of lambda terms. The update calculus can serve as a basis for higher-level update languages, such as for Haskell or Java.",1,2,0,0,1,2
33,Empirical analysis of GUI programming concerns,"Graphical user interfaces (GUI), GUI programming, GUI application programming interface (API), GUI concerns, Separation of concerns","The focus of this paper is on identification of typical graphical user interface (GUI) programming concerns. As opposed to some other proposals available in the literature that indicate GUI programming concerns by simple intuition, we have conducted a systematic empirical analysis to derive our proposal. It included an analysis of an existing application programming interface (API), its use in industrial projects, and an analysis of the requirements and issues reported during software maintenance. In addition, we have evaluated more than 50 GUI frameworks and APIs and proved usefulness and generality of our classification of concerns. As an additional proof of applicability of the proposed classification, we have refactored the inheritance hierarchy of the selected GUI API using concern-oriented interfaces. We have implemented a supporting tool that complements the developed API and supports its concern-oriented use. The evaluation of the refactored API showed positive effects on API usability.",0,"Empirical analysis of GUI programming concerns. The focus of this paper is on identification of typical graphical user interface (GUI) programming concerns. As opposed to some other proposals available in the literature that indicate GUI programming concerns by simple intuition, we have conducted a systematic empirical analysis to derive our proposal. It included an analysis of an existing application programming interface (API), its use in industrial projects, and an analysis of the requirements and issues reported during software maintenance. In addition, we have evaluated more than 50 GUI frameworks and APIs and proved usefulness and generality of our classification of concerns. As an additional proof of applicability of the proposed classification, we have refactored the inheritance hierarchy of the selected GUI API using concern-oriented interfaces. We have implemented a supporting tool that complements the developed API and supports its concern-oriented use. The evaluation of the refactored API showed positive effects on API usability.",0,3,2,0,1,3
34,An Abstract Equivalence Notion for Object Models,"equivalence notion, theorem proving, object models","Equivalence notions for object models are usually too concrete in the sense that they assume that the compared models are formed by elements with the same names. This is not adequate in several situations: during model refactoring, when using auxiliary model elements, or when the compared models comprise distinct but corresponding elements. So, in this paper, we propose a more abstract and language-independent equivalence notion for object models. It supports, as desired, abstraction from names and elements when comparing models. We use the PVS system to specify and prove properties of our notion. It is illustrated here by comparing simple models in Alloy, a formal object-oriented modeling language, but has also been applied for deriving a comprehensive set of algebraic laws for Alloy.",0,"An Abstract Equivalence Notion for Object Models. Equivalence notions for object models are usually too concrete in the sense that they assume that the compared models are formed by elements with the same names. This is not adequate in several situations: during model refactoring, when using auxiliary model elements, or when the compared models comprise distinct but corresponding elements. So, in this paper, we propose a more abstract and language-independent equivalence notion for object models. It supports, as desired, abstraction from names and elements when comparing models. We use the PVS system to specify and prove properties of our notion. It is illustrated here by comparing simple models in Alloy, a formal object-oriented modeling language, but has also been applied for deriving a comprehensive set of algebraic laws for Alloy.",1,1,2,1,1,1
35,Constraint-aware Schema Transformation,"Schema transformation, Constraints, Invariants, Data refinement, Strategic rewriting, Point-free program transformation, Haskell","Data schema transformations occur in the context of software evolution, refactoring, and cross-paradigm data mappings. When constraints exist on the initial schema, these need to be transformed into constraints on the target schema. Moreover, when high-level data types are refined to lower level structures, additional target schema constraints must be introduced to balance the loss of structure and preserve semantics. We introduce an algebraic approach to schema transformation that is constraint-aware in the sense that constraints are preserved from source to target schemas and that new constraints are introduced where needed. Our approach is based on refinement theory and point-free program transformation. Data refinements are modeled as rewrite rules on types that carry point-free predicates as constraints. At each rewrite step, the predicate on the reduct is computed from the predicate on the redex. An additional rewrite system on point-free functions is used to normalize the predicates that are built up along rewrite chains. We implemented our rewrite systems in a type-safe way in the functional programming language Haskell. We demonstrate their application to constraint-aware hierarchical-relational mappings.",0,"Constraint-aware Schema Transformation. Data schema transformations occur in the context of software evolution, refactoring, and cross-paradigm data mappings. When constraints exist on the initial schema, these need to be transformed into constraints on the target schema. Moreover, when high-level data types are refined to lower level structures, additional target schema constraints must be introduced to balance the loss of structure and preserve semantics. We introduce an algebraic approach to schema transformation that is constraint-aware in the sense that constraints are preserved from source to target schemas and that new constraints are introduced where needed. Our approach is based on refinement theory and point-free program transformation. Data refinements are modeled as rewrite rules on types that carry point-free predicates as constraints. At each rewrite step, the predicate on the reduct is computed from the predicate on the redex. An additional rewrite system on point-free functions is used to normalize the predicates that are built up along rewrite chains. We implemented our rewrite systems in a type-safe way in the functional programming language Haskell. We demonstrate their application to constraint-aware hierarchical-relational mappings.",1,1,0,0,1,4
36,Chapter 2 - Design Smells,"Design principles, Design quality, Design smells, Naming scheme for smells, PHAME model (principles of hierarchy, abstraction, modularization, and encapsulation), Refactoring smells, Smell catalog, Smell classification","Design smells are certain structures in the design that indicate violation of fundamental design principles and negatively impact design quality. This chapter provides a brief overview of design smells and various factors that lead to the occurrence of smells. This chapter presents the core idea behind a principle-based classification scheme for design smells discussed in this book: ``When we view every smell as a violation of one or more underlying design principle(s), we get a deeper understanding of that smell; but perhaps more importantly, it also naturally directs us toward a potential refactoring approach for that smell.'' Building on this insight, this chapter introduces the PHAME (Principles of Hierarchy, Abstraction, Modularization, and Encapsulation) model that has been used for both the classification and naming of all the smells in this book. The chapter concludes by introducing a template that is used to document design smells described in the next four chapters of the book.",0,"Chapter 2 - Design Smells. Design smells are certain structures in the design that indicate violation of fundamental design principles and negatively impact design quality. This chapter provides a brief overview of design smells and various factors that lead to the occurrence of smells. This chapter presents the core idea behind a principle-based classification scheme for design smells discussed in this book: ``When we view every smell as a violation of one or more underlying design principle(s), we get a deeper understanding of that smell; but perhaps more importantly, it also naturally directs us toward a potential refactoring approach for that smell.'' Building on this insight, this chapter introduces the PHAME (Principles of Hierarchy, Abstraction, Modularization, and Encapsulation) model that has been used for both the classification and naming of all the smells in this book. The chapter concludes by introducing a template that is used to document design smells described in the next four chapters of the book.",0,3,0,0,0,0
37,Termination of High-Level Replacement Units with Application to Model Transformation,"Transformation units, graph transformation, termination, refactoring","Visual rewriting techniques, in particular graph transformations, are increasingly used to model transformations of systems specified through diagrammatic sentences. Several rewriting models have been proposed, differing in the expressivity of the types of rules and in the complexity of the rewriting mechanism; yet basic results concerning the formal properties of these models are still missing for many of them. In this paper, we propose a contribution towards solving the termination problem for rewriting systems with external control mechanisms. In particular, we obtain results of more general validity by extending the concept of transformation unit to high-level replacement systems, a generalization of graph transformation systems. For high-level replacement units, we state and prove several abstract properties based on termination criteria. Then, we instantiate the high-level replacement systems by attributed graph transformation systems and present concrete termination criteria. These are used to show the termination of some replacement units needed to express model transformations as a consequence of software refactoring.",0,"Termination of High-Level Replacement Units with Application to Model Transformation. Visual rewriting techniques, in particular graph transformations, are increasingly used to model transformations of systems specified through diagrammatic sentences. Several rewriting models have been proposed, differing in the expressivity of the types of rules and in the complexity of the rewriting mechanism; yet basic results concerning the formal properties of these models are still missing for many of them. In this paper, we propose a contribution towards solving the termination problem for rewriting systems with external control mechanisms. In particular, we obtain results of more general validity by extending the concept of transformation unit to high-level replacement systems, a generalization of graph transformation systems. For high-level replacement units, we state and prove several abstract properties based on termination criteria. Then, we instantiate the high-level replacement systems by attributed graph transformation systems and present concrete termination criteria. These are used to show the termination of some replacement units needed to express model transformations as a consequence of software refactoring.",1,1,0,0,1,4
38,A methodology to assess the impact of design patterns on software quality,"Structural quality, Design patterns, Object-oriented metrics, Quality","Context
Software quality is considered to be one of the most important concerns of software production teams. Additionally, design patterns are documented solutions to common design problems that are expected to enhance software quality. Until now, the results on the effect of design patterns on software quality are controversial.
Aims
This study aims to propose a methodology for comparing design patterns to alternative designs with an analytical method. Additionally, the study illustrates the methodology by comparing three design patterns with two alternative solutions, with respect to several quality attributes.
Method
The paper introduces a theoretical/analytical methodology to compare sets of ``canonical'' solutions to design problems. The study is theoretical in the sense that the solutions are disconnected from real systems, even though they stem from concrete problems. The study is analytical in the sense that the solutions are compared based on their possible numbers of classes and on equations representing the values of the various structural quality attributes in function of these numbers of classes. The exploratory designs have been produced by studying the literature, by investigating open-source projects and by using design patterns. In addition to that, we have created a tool that helps practitioners in choosing the optimal design solution, according to their special needs.
Results
The results of our research suggest that the decision of applying a design pattern is usually a trade-off, because patterns are not universally good or bad. Patterns typically improve certain aspects of software quality, while they might weaken some other.
Conclusions
Concluding the proposed methodology is applicable for comparing patterns and alternative designs, and highlights existing threshold that when surpassed the design pattern is getting more or less beneficial than the alternative design. More specifically, the identification of such thresholds can become very useful for decision making during system design and refactoring.",0,"A methodology to assess the impact of design patterns on software quality. Context
Software quality is considered to be one of the most important concerns of software production teams. Additionally, design patterns are documented solutions to common design problems that are expected to enhance software quality. Until now, the results on the effect of design patterns on software quality are controversial.
Aims
This study aims to propose a methodology for comparing design patterns to alternative designs with an analytical method. Additionally, the study illustrates the methodology by comparing three design patterns with two alternative solutions, with respect to several quality attributes.
Method
The paper introduces a theoretical/analytical methodology to compare sets of ``canonical'' solutions to design problems. The study is theoretical in the sense that the solutions are disconnected from real systems, even though they stem from concrete problems. The study is analytical in the sense that the solutions are compared based on their possible numbers of classes and on equations representing the values of the various structural quality attributes in function of these numbers of classes. The exploratory designs have been produced by studying the literature, by investigating open-source projects and by using design patterns. In addition to that, we have created a tool that helps practitioners in choosing the optimal design solution, according to their special needs.
Results
The results of our research suggest that the decision of applying a design pattern is usually a trade-off, because patterns are not universally good or bad. Patterns typically improve certain aspects of software quality, while they might weaken some other.
Conclusions
Concluding the proposed methodology is applicable for comparing patterns and alternative designs, and highlights existing threshold that when surpassed the design pattern is getting more or less beneficial than the alternative design. More specifically, the identification of such thresholds can become very useful for decision making during system design and refactoring.",0,2,1,2,0,3
39,Preface: UNIGRA'03 - Uniform Approaches to Graphical Process Specification Techniques,,"This volume contains selected papers of the proceedings of the workshop on Uniform Approaches to Graphical Process Specification Techniques (UNIGRA'03). The workshop was held in Warsaw, Poland, on April 5 and 6, 2003, as a satellite event of the sixth European Joint Conference on Theory and Practice of Software (ETAPS 2003). The workshop continues the UNIGRA workshop in 2001 which has been a successful satellite event of ETAPS 2001. Workshop Objectives Due to the increasing amount of divergent formalisms, the main idea of the UNIGRA workshops is to bring together people working especially in the following three areas: *Low Level and High-Level Petri Nets*Graph Transformation and High-Level Replacement Systems*Visual Modeling Techniques including UML In each of these areas there is a large variety of different approaches, however, first attempts for uniform approaches have been made already. According to the main idea and in order to further stimulate the research in this important area, this volume presents some uniform approaches and further introduce unifying and comparative studies across the borders of the three and related areas. Workshop Program In the first part, unifying approaches for low-level and high-level Petri nets are proposed: The contribution by Ehrig shows how the notions occurrence net and process can be generalized from low-level to high-level Petri nets, and studies the behavior and instantiations of this new view of processes for high-level nets. In his overview on new developments in the area of Petri net transformations for Software Engineering, Urb{'a}{ s}ek presents recent work on net model transformations and net class transformations. Both kinds of transformations are studied with regard to the preservation of system properties such as safety properties or liveness. The formalization of Petri net transformations is originally based on the theory of graph transformation. Padberg considers a case study (the call center of a phone company)which is modeled using Petri net modules for structuring the operational behavior of the system. The notion of Petri net modules was achieved by a transfer from the concepts of algebraic module specifications to the modeling of component-based systems by Petri nets. Desel, Juh{'a}s and Lorenz deal with the semantics of place/transition nets. The authors relate the process semantics based on partial orders (individual token semantics) to the collective token semantics by defining partial orders associated to process terms of place/transition nets. In the second part concerning graph transformation and high-level replacement systems, new aspects of component modeling and application of graph transformation techniques are discussed: In their contribution on components for algebra transformation systems, Ehrig and Orejas define a component transformation semantics in terms of the semantics of the specifications included in the components. The underlying formal basis of the instantiation of their generic component framework are algebra transformation systems and high-level replacement rules. An application of the formal unifying framework of distributed transformation units is presented by Kuske and Knirsch. The authors illustrate how different features of agent systems can be modeled by distributed graph transformation systems in a uniform way. Another application for graph rewriting, presented by Van Eetvelde and Janssens, is the modeling of refactoring operations for programs. The authors propose a hierarchical graph representation for programs to facilitate the study of refactoring operation effects at class level. The third part contains contributions focusing on unifying concepts for visual modeling techniques including UML: Minas describes a graphical specification tool for DIAGEN, a diagram editor generator based on hypergraph transformation. The specification tool simplifies the specification and generation of diagram editors. It uses an XML-based specification language and comes with a generic XML editor. In his contribution on dynamic aspects of visual modeling languages, Bottoni proposes an approach to the definition of the syntax and semantics of visual languages based on a notion of transition of production/consumption of resources. Abstract meta-models for this notion of transition are presented. An approach to the model-based verification and validation of properties of UML models is presented by Engels, K{""\i}ster, Heckel and Lohmann. The authors use graph transformation techniques as a meta-language for the translation and analysis of models. In model-driven architectures, the problem arises to deal with multiple models. Kent and Smith focus in their contribution on bidirectional mappings between models for software requirements and models for software design as basis for tools checking model traceability and consistency. Program Committee The following program committee of UNIGRA'03 has given valuable scientific support: *Hartmut Ehrig (TU Berlin, Germany) [chair]*Roswitha Bardohl (TU Berlin, Germany) [co-chair]*Luciano Baresi (University of Milano, Italy)*Paolo Bottoni (University of Pisa, Italy)*Claudia Ermel (TU Berlin, Germany)*Reiko Heckel (University of Paderborn, Germany)*Dirk Janssens (University of Antwerp, Belgium)*Stuart Kent (University of Kent, Great Britain)*Hans-J{""o}rg Kreowski (University of Bremen, Germany)*Fernando Orejas (University of Catalunya, Espania)*Julia Padberg (University of Bremen, Germany)*Grzegorz Rozenberg (University of Leiden, The Netherlands) Acknowledgement This workshop is supported by the European research training network SegraVis, and by the steering committee of the International Conference on Graph Transformation (ICGT). June 2003, Roswitha Bardohl and Hartmut Ehrig",0,"Preface: UNIGRA'03 - Uniform Approaches to Graphical Process Specification Techniques. This volume contains selected papers of the proceedings of the workshop on Uniform Approaches to Graphical Process Specification Techniques (UNIGRA'03). The workshop was held in Warsaw, Poland, on April 5 and 6, 2003, as a satellite event of the sixth European Joint Conference on Theory and Practice of Software (ETAPS 2003). The workshop continues the UNIGRA workshop in 2001 which has been a successful satellite event of ETAPS 2001. Workshop Objectives Due to the increasing amount of divergent formalisms, the main idea of the UNIGRA workshops is to bring together people working especially in the following three areas: *Low Level and High-Level Petri Nets*Graph Transformation and High-Level Replacement Systems*Visual Modeling Techniques including UML In each of these areas there is a large variety of different approaches, however, first attempts for uniform approaches have been made already. According to the main idea and in order to further stimulate the research in this important area, this volume presents some uniform approaches and further introduce unifying and comparative studies across the borders of the three and related areas. Workshop Program In the first part, unifying approaches for low-level and high-level Petri nets are proposed: The contribution by Ehrig shows how the notions occurrence net and process can be generalized from low-level to high-level Petri nets, and studies the behavior and instantiations of this new view of processes for high-level nets. In his overview on new developments in the area of Petri net transformations for Software Engineering, Urb{'a}{ s}ek presents recent work on net model transformations and net class transformations. Both kinds of transformations are studied with regard to the preservation of system properties such as safety properties or liveness. The formalization of Petri net transformations is originally based on the theory of graph transformation. Padberg considers a case study (the call center of a phone company)which is modeled using Petri net modules for structuring the operational behavior of the system. The notion of Petri net modules was achieved by a transfer from the concepts of algebraic module specifications to the modeling of component-based systems by Petri nets. Desel, Juh{'a}s and Lorenz deal with the semantics of place/transition nets. The authors relate the process semantics based on partial orders (individual token semantics) to the collective token semantics by defining partial orders associated to process terms of place/transition nets. In the second part concerning graph transformation and high-level replacement systems, new aspects of component modeling and application of graph transformation techniques are discussed: In their contribution on components for algebra transformation systems, Ehrig and Orejas define a component transformation semantics in terms of the semantics of the specifications included in the components. The underlying formal basis of the instantiation of their generic component framework are algebra transformation systems and high-level replacement rules. An application of the formal unifying framework of distributed transformation units is presented by Kuske and Knirsch. The authors illustrate how different features of agent systems can be modeled by distributed graph transformation systems in a uniform way. Another application for graph rewriting, presented by Van Eetvelde and Janssens, is the modeling of refactoring operations for programs. The authors propose a hierarchical graph representation for programs to facilitate the study of refactoring operation effects at class level. The third part contains contributions focusing on unifying concepts for visual modeling techniques including UML: Minas describes a graphical specification tool for DIAGEN, a diagram editor generator based on hypergraph transformation. The specification tool simplifies the specification and generation of diagram editors. It uses an XML-based specification language and comes with a generic XML editor. In his contribution on dynamic aspects of visual modeling languages, Bottoni proposes an approach to the definition of the syntax and semantics of visual languages based on a notion of transition of production/consumption of resources. Abstract meta-models for this notion of transition are presented. An approach to the model-based verification and validation of properties of UML models is presented by Engels, K{""\i}ster, Heckel and Lohmann. The authors use graph transformation techniques as a meta-language for the translation and analysis of models. In model-driven architectures, the problem arises to deal with multiple models. Kent and Smith focus in their contribution on bidirectional mappings between models for software requirements and models for software design as basis for tools checking model traceability and consistency. Program Committee The following program committee of UNIGRA'03 has given valuable scientific support: *Hartmut Ehrig (TU Berlin, Germany) [chair]*Roswitha Bardohl (TU Berlin, Germany) [co-chair]*Luciano Baresi (University of Milano, Italy)*Paolo Bottoni (University of Pisa, Italy)*Claudia Ermel (TU Berlin, Germany)*Reiko Heckel (University of Paderborn, Germany)*Dirk Janssens (University of Antwerp, Belgium)*Stuart Kent (University of Kent, Great Britain)*Hans-J{""o}rg Kreowski (University of Bremen, Germany)*Fernando Orejas (University of Catalunya, Espania)*Julia Padberg (University of Bremen, Germany)*Grzegorz Rozenberg (University of Leiden, The Netherlands) Acknowledgement This workshop is supported by the European research training network SegraVis, and by the steering committee of the International Conference on Graph Transformation (ICGT). June 2003, Roswitha Bardohl and Hartmut Ehrig",1,0,0,0,0,1
40,Chapter 7 - SQL Tag Library Using the SQL Actions,,"Publisher Summary
This chapter focuses on the structured query language (SQL) actions. These actions can be used for accessing, updating, and deleting data. It is also possible to use transactions as long as the database ones are using support transactions. The JavaServer Pages Standard Tag Library (JSTL) SQL actions provide functionality that allows for: making database queries, accessing query results, performing database modifications, and performing database transactions. Currently, there are many projects that contain JavaServer Pages code that can utilize the JSTL. A great place to start using these is by refactoring some of the existing pages.",0,"Chapter 7 - SQL Tag Library Using the SQL Actions. Publisher Summary
This chapter focuses on the structured query language (SQL) actions. These actions can be used for accessing, updating, and deleting data. It is also possible to use transactions as long as the database ones are using support transactions. The JavaServer Pages Standard Tag Library (JSTL) SQL actions provide functionality that allows for: making database queries, accessing query results, performing database modifications, and performing database transactions. Currently, there are many projects that contain JavaServer Pages code that can utilize the JSTL. A great place to start using these is by refactoring some of the existing pages.",0,1,0,0,1,4
41,"Preface: Volume 82, Issue 3",,"This volume contains the papers presented at the Third Workshop on Language Descriptions, Tools and Applications (LDTA 03), held in Warsaw, Poland, April 6, 2003, as a satellite event of the European Joint Conferences on Theory and Practice of Software (ETAPS 03). LDTA 03 continues the tradition established with the previous two instances of the workshop which were held in Genoa, Italy (2001) and Grenoble, France (2002), as satellite events of ETAPS. The aim of this workshop is to bring together researchers from academia and industry interested in the field of formal language definitions and language technologies, with a special emphasis on tools developed for or with these language definitions. Benefits of such active research fields are, among others: program analysis, transformation, generation; the formal analysis of language properties, and the automatic generation of language processing tools. The workshop welcomes contributions on all aspects of formal language definitions, with special emphasis on applications and tools developed for or with these language definitions. The LDTA 03 program consists of 11 regular papers, which were selected from 25 submissions, and one invited talk by Hassan A{""\i}t Kaci on An Abstract and Reusable Programming Language Architecture. The selected papers cover a broad range of themes such as visual languages, parsing, refactoring and coverage techniques, attribute grammars, and frameworks for datatype transformation and register allocation. We also wish to express our gratitude to all the members of the program committee, and to all the outside referees for their care in reviewing the papers. We would like to thank the ETAPS organizing committee for taking care of the local organization of the workshop. Furthermore, we are also very pleased with the sponsorship of ACM SIGPLAN and with the publication of these proceedings in the Electronic Notes in Theoretical Computer Science (ENTCS) by Elsevier. Barrett Bryant Jo{\~a}o Saraiva Birmingham (USA) and Braga (Portugal), April, 2003 Organizing Committee Isabelle Attali, INRIA Sophia Antipolis, France Mark van den Brand, CWI Amsterdam, The Netherlands Pierre-Etienne Moreau, Loria Nancy, France Program Committee Don Batory, University of Texas at Austin, USA Barrett Bryant, University of Alabama at Birmingham, USA (co-chair) Uwe Glaesser, Simon Fraser University, Canada Katsuhiko Gondow, Japan Advanced Institute of Science and Technology, Japan Uwe Kastens, University of Paderborn, Germany Paul Klint, CWI, The Netherlands Jan Madey, University of Warsaw, Poland Marjan Mernik, University of Maribor, Slovenia Thomas Noll, University of Aachen, Germany Oege de Moor, Oxford University Computing Laboratory, England Peter D. Mosses, BRICS, University of Aarhus, Denmark Jo{\~a}o Saraiva, University of Minho, Portugal (co-chair) Eelco Visser, University of Utrecht, The Netherlands",0,"Preface: Volume 82, Issue 3. This volume contains the papers presented at the Third Workshop on Language Descriptions, Tools and Applications (LDTA 03), held in Warsaw, Poland, April 6, 2003, as a satellite event of the European Joint Conferences on Theory and Practice of Software (ETAPS 03). LDTA 03 continues the tradition established with the previous two instances of the workshop which were held in Genoa, Italy (2001) and Grenoble, France (2002), as satellite events of ETAPS. The aim of this workshop is to bring together researchers from academia and industry interested in the field of formal language definitions and language technologies, with a special emphasis on tools developed for or with these language definitions. Benefits of such active research fields are, among others: program analysis, transformation, generation; the formal analysis of language properties, and the automatic generation of language processing tools. The workshop welcomes contributions on all aspects of formal language definitions, with special emphasis on applications and tools developed for or with these language definitions. The LDTA 03 program consists of 11 regular papers, which were selected from 25 submissions, and one invited talk by Hassan A{""\i}t Kaci on An Abstract and Reusable Programming Language Architecture. The selected papers cover a broad range of themes such as visual languages, parsing, refactoring and coverage techniques, attribute grammars, and frameworks for datatype transformation and register allocation. We also wish to express our gratitude to all the members of the program committee, and to all the outside referees for their care in reviewing the papers. We would like to thank the ETAPS organizing committee for taking care of the local organization of the workshop. Furthermore, we are also very pleased with the sponsorship of ACM SIGPLAN and with the publication of these proceedings in the Electronic Notes in Theoretical Computer Science (ENTCS) by Elsevier. Barrett Bryant Jo{\~a}o Saraiva Birmingham (USA) and Braga (Portugal), April, 2003 Organizing Committee Isabelle Attali, INRIA Sophia Antipolis, France Mark van den Brand, CWI Amsterdam, The Netherlands Pierre-Etienne Moreau, Loria Nancy, France Program Committee Don Batory, University of Texas at Austin, USA Barrett Bryant, University of Alabama at Birmingham, USA (co-chair) Uwe Glaesser, Simon Fraser University, Canada Katsuhiko Gondow, Japan Advanced Institute of Science and Technology, Japan Uwe Kastens, University of Paderborn, Germany Paul Klint, CWI, The Netherlands Jan Madey, University of Warsaw, Poland Marjan Mernik, University of Maribor, Slovenia Thomas Noll, University of Aachen, Germany Oege de Moor, Oxford University Computing Laboratory, England Peter D. Mosses, BRICS, University of Aarhus, Denmark Jo{\~a}o Saraiva, University of Minho, Portugal (co-chair) Eelco Visser, University of Utrecht, The Netherlands",1,0,0,0,0,1
42,Understanding the role of licenses and evolution in open architecture software ecosystems,"Software architecture, Software ecosystems, Software licenses, Open source software, Software evolution","The role of software ecosystems in the development and evolution of open architecture systems whose components are subject to different licenses has received insufficient consideration. Such systems are composed of components potentially under two or more licenses, open source or proprietary or both, in an architecture in which evolution can occur by evolving existing components, replacing them, or refactoring. The software licenses of the components both facilitate and constrain the system's ecosystem and its evolution, and the licenses' rights and obligations are crucial in producing an acceptable system. Consequently, software component licenses and the architectural composition of a system help to better define the software ecosystem niche in which a given system lies. Understanding and describing software ecosystem niches for open architecture systems is a key contribution of this work. An example open architecture software system that articulates different niches is employed to this end. We examine how the architecture and software component licenses of a composed system at design time, build time, and run time help determine the system's software ecosystem niche and provide insight and guidance for identifying and selecting potential evolutionary paths of system, architecture, and niches.",0,"Understanding the role of licenses and evolution in open architecture software ecosystems. The role of software ecosystems in the development and evolution of open architecture systems whose components are subject to different licenses has received insufficient consideration. Such systems are composed of components potentially under two or more licenses, open source or proprietary or both, in an architecture in which evolution can occur by evolving existing components, replacing them, or refactoring. The software licenses of the components both facilitate and constrain the system's ecosystem and its evolution, and the licenses' rights and obligations are crucial in producing an acceptable system. Consequently, software component licenses and the architectural composition of a system help to better define the software ecosystem niche in which a given system lies. Understanding and describing software ecosystem niches for open architecture systems is a key contribution of this work. An example open architecture software system that articulates different niches is employed to this end. We examine how the architecture and software component licenses of a composed system at design time, build time, and run time help determine the system's software ecosystem niche and provide insight and guidance for identifying and selecting potential evolutionary paths of system, architecture, and niches.",0,3,0,0,0,1
43,"Preface: Volume 72, Issue 4",,"Software Evolution through Transformations
Businesses, organisations and society at large are increasingly reliant on software at all levels. An intrinsic characteristic of software addressing a real-world application is the need to evolve. Such evolution is inevitable if the software is to remain satisfactory to its stakeholders. Changes to software artifacts and related entities tend to be progressive and incremental, driven, for example, by feedback from users and other stakeholders. Changes may be needed for a variety of reasons, such as bug reports, requests for new features or, more generally, changes of functional requirements, or by the need to adapt to new technology, e.g., to interface to other systems. In general, evolutionary characteristics are inescapable when the problem to be solved or the application to be addressed belongs to the real world. Transformations of artifacts like models, schemata, data, program code, or software architectures provide a uniform and systematic way to express and reason about the evolution of software systems. Literally, all activities that lead to the creation or modification of documents have a transformational aspect, i.e., they change a given structure into a new one according to pre-defined rules. In most cases, these rules manifest themselves as user-invoked operations in CASE tools or program editors. More abstract examples include rules for model refinement, for translating models to code, for recovering designs from legacy systems, or for refactoring and restructuring software.
Workshop Objectives
This workshop aimed at providing a forum for the discussion of transformational techniques in software evolution with particular focus on approaches that are generally applicable throughout the software development life-cycle.. Thereby, we have distinguished two co-existing, complementary and mutually reinforcing views of the evolution theme. The more widespread view focuses on the how of software evolution, emphasising the methods and means by which software is evolved. The other focuses on the what and why of the evolution phenomenon, its nature and underlying drivers. Being mutually supportive, both views are required. A better understanding of the phenomenon leads to more appropriate ways of achieving evolution. Both views are supported through the general concept of transformation, i.e., the manual, interactive, or automatic manipulation of artifacts according to pre-defined rules, either as a conceptual abstraction of human software engineering activities, or as the implementation of mappings on and between modelling and programming languages.
Workshop Program
The workshop was scheduled for two half days and included one invited talk by Stuart Kent as well as presentations of contributed papers in two regular sessions on different Transformation Techniques and on the mutual Compatibility of Transformations. In addition, the workshop featured a special session on Case Studies for Visual Modelling Techniques held jointly with the Workshop on Graph Transformation and Visual Modelling Techniques (GT-VMT 2002) as part of the work carried out under the European research training network SegraVis (for Syntactic and Semantic Integration of Visual Modelling Techniques).
Acknowledgement
This workshop has been supported by the following projects: the scientific research network Foundations of Software Evolution, funded by the Fund for Scientific Research - Flanders (Belgium), the scientific research network RELEASE (for REsearch Links to Explore and Advance Software Evolution) funded by the European Science Foundation, and the European research training network SegraVis. Reiko Heckel, Universit{""a}t Paderborn, Germany Tom Mens, Vrije Universiteit Brussel, Belgium Michel Wermelinger, Universidade Nova de Lisboa and ATX Software SA, Lisbon, Portugal",0,"Preface: Volume 72, Issue 4. Software Evolution through Transformations
Businesses, organisations and society at large are increasingly reliant on software at all levels. An intrinsic characteristic of software addressing a real-world application is the need to evolve. Such evolution is inevitable if the software is to remain satisfactory to its stakeholders. Changes to software artifacts and related entities tend to be progressive and incremental, driven, for example, by feedback from users and other stakeholders. Changes may be needed for a variety of reasons, such as bug reports, requests for new features or, more generally, changes of functional requirements, or by the need to adapt to new technology, e.g., to interface to other systems. In general, evolutionary characteristics are inescapable when the problem to be solved or the application to be addressed belongs to the real world. Transformations of artifacts like models, schemata, data, program code, or software architectures provide a uniform and systematic way to express and reason about the evolution of software systems. Literally, all activities that lead to the creation or modification of documents have a transformational aspect, i.e., they change a given structure into a new one according to pre-defined rules. In most cases, these rules manifest themselves as user-invoked operations in CASE tools or program editors. More abstract examples include rules for model refinement, for translating models to code, for recovering designs from legacy systems, or for refactoring and restructuring software.
Workshop Objectives
This workshop aimed at providing a forum for the discussion of transformational techniques in software evolution with particular focus on approaches that are generally applicable throughout the software development life-cycle.. Thereby, we have distinguished two co-existing, complementary and mutually reinforcing views of the evolution theme. The more widespread view focuses on the how of software evolution, emphasising the methods and means by which software is evolved. The other focuses on the what and why of the evolution phenomenon, its nature and underlying drivers. Being mutually supportive, both views are required. A better understanding of the phenomenon leads to more appropriate ways of achieving evolution. Both views are supported through the general concept of transformation, i.e., the manual, interactive, or automatic manipulation of artifacts according to pre-defined rules, either as a conceptual abstraction of human software engineering activities, or as the implementation of mappings on and between modelling and programming languages.
Workshop Program
The workshop was scheduled for two half days and included one invited talk by Stuart Kent as well as presentations of contributed papers in two regular sessions on different Transformation Techniques and on the mutual Compatibility of Transformations. In addition, the workshop featured a special session on Case Studies for Visual Modelling Techniques held jointly with the Workshop on Graph Transformation and Visual Modelling Techniques (GT-VMT 2002) as part of the work carried out under the European research training network SegraVis (for Syntactic and Semantic Integration of Visual Modelling Techniques).
Acknowledgement
This workshop has been supported by the following projects: the scientific research network Foundations of Software Evolution, funded by the Fund for Scientific Research - Flanders (Belgium), the scientific research network RELEASE (for REsearch Links to Explore and Advance Software Evolution) funded by the European Science Foundation, and the European research training network SegraVis. Reiko Heckel, Universit{""a}t Paderborn, Germany Tom Mens, Vrije Universiteit Brussel, Belgium Michel Wermelinger, Universidade Nova de Lisboa and ATX Software SA, Lisbon, Portugal",1,0,0,0,0,2
44,Refactoring code-first Web Services for early avoiding WSDL anti-patterns: Approach and comprehensive assessment,"Web services, Code-first, WSDL anti-patterns, Service understandability, Service retrievability","Previous research of our own [34] has shown that by avoiding certain bad specification practices, or WSDL anti-patterns, contract-first Web Service descriptions expressed in WSDL can be greatly improved in terms of understandability and retrievability. The former means the capability of a human discoverer to effectively reason about a Web Service functionality just by inspecting its associated WSDL description. The latter means correctly retrieving a relevant Web Service by a syntactic service registry upon a meaningful user's query. However, code-first service construction dominates in the industry due to its simplicity. This paper proposes an approach to avoid WSDL anti-patterns in code-first Web Services. We also evaluate the approach in terms of services understandability and retrievability, deeply discuss the experimental results, and delineate some guidelines to help code-first Web Service developers in dealing with the trade-offs that arise between these two dimensions. Certainly, our approach allows services to be more understandable, due to anti-pattern remotion, and retrievable as measured by classical Information Retrieval metrics.",0,"Refactoring code-first Web Services for early avoiding WSDL anti-patterns: Approach and comprehensive assessment. Previous research of our own [34] has shown that by avoiding certain bad specification practices, or WSDL anti-patterns, contract-first Web Service descriptions expressed in WSDL can be greatly improved in terms of understandability and retrievability. The former means the capability of a human discoverer to effectively reason about a Web Service functionality just by inspecting its associated WSDL description. The latter means correctly retrieving a relevant Web Service by a syntactic service registry upon a meaningful user's query. However, code-first service construction dominates in the industry due to its simplicity. This paper proposes an approach to avoid WSDL anti-patterns in code-first Web Services. We also evaluate the approach in terms of services understandability and retrievability, deeply discuss the experimental results, and delineate some guidelines to help code-first Web Service developers in dealing with the trade-offs that arise between these two dimensions. Certainly, our approach allows services to be more understandable, due to anti-pattern remotion, and retrievable as measured by classical Information Retrieval metrics.",0,2,0,0,2,0
45,Making refactoring safer through impact analysis,"Refactoring, Change impact analysis","Currently most developers have to apply manual steps and use test suites to improve confidence that transformations applied to object-oriented (OO) and aspect-oriented (AO) programs are correct. However, it is not simple to do manual reasoning, due to the nontrivial semantics of OO and AO languages. Moreover, most refactoring implementations contain a number of bugs since it is difficult to establish all conditions required for a transformation to be behavior preserving. In this article, we propose a tool (SafeRefactorImpact) that analyzes the transformation and generates tests only for the methods impacted by a transformation identified by our change impact analyzer (Safira). We compare SafeRefactorImpact with our previous tool (SafeRefactor) with respect to correctness, performance, number of methods passed to the automatic test suite generator, change coverage, and number of relevant tests generated in 45 transformations. SafeRefactorImpact identifies behavioral changes undetected by SafeRefactor. Moreover, it reduces the number of methods passed to the test suite generator. Finally, SafeRefactorImpact has a better change coverage in larger subjects, and generates more relevant tests than SafeRefactor.",0,"Making refactoring safer through impact analysis. Currently most developers have to apply manual steps and use test suites to improve confidence that transformations applied to object-oriented (OO) and aspect-oriented (AO) programs are correct. However, it is not simple to do manual reasoning, due to the nontrivial semantics of OO and AO languages. Moreover, most refactoring implementations contain a number of bugs since it is difficult to establish all conditions required for a transformation to be behavior preserving. In this article, we propose a tool (SafeRefactorImpact) that analyzes the transformation and generates tests only for the methods impacted by a transformation identified by our change impact analyzer (Safira). We compare SafeRefactorImpact with our previous tool (SafeRefactor) with respect to correctness, performance, number of methods passed to the automatic test suite generator, change coverage, and number of relevant tests generated in 45 transformations. SafeRefactorImpact identifies behavioral changes undetected by SafeRefactor. Moreover, it reduces the number of methods passed to the test suite generator. Finally, SafeRefactorImpact has a better change coverage in larger subjects, and generates more relevant tests than SafeRefactor.",1,2,0,0,3,4
46,A refactoring method for cache-efficient swarm intelligence algorithms,"Cache, Memory hierarchy, Miss rate, Swarm intelligence, Particle swarm optimization, Genetic algorithm","With advances in hardware technology, conventional approaches to software development are not effective for developing efficient algorithms for run-time environments. The problem comes from the overly simplified hardware abstraction model in the software development procedure. The mismatch between the hypothetical hardware model and real hardware design should be compensated for in designing an efficient algorithm. In this paper, we focus on two schemes: one is the memory hierarchy, and the other is the algorithm design. Both the cache properties and the cache-aware development are investigated. We then propose a few simple guidelines for revising a developed algorithm in order to increase the utilization of the cache. To verify the effectiveness of the guidelines proposed, optimization techniques, including particle swarm optimization (PSO) and the genetic algorithm (GA), are employed. Simulation results demonstrate that the guidelines are potentially helpful for revising various algorithms.",0,"A refactoring method for cache-efficient swarm intelligence algorithms. With advances in hardware technology, conventional approaches to software development are not effective for developing efficient algorithms for run-time environments. The problem comes from the overly simplified hardware abstraction model in the software development procedure. The mismatch between the hypothetical hardware model and real hardware design should be compensated for in designing an efficient algorithm. In this paper, we focus on two schemes: one is the memory hierarchy, and the other is the algorithm design. Both the cache properties and the cache-aware development are investigated. We then propose a few simple guidelines for revising a developed algorithm in order to increase the utilization of the cache. To verify the effectiveness of the guidelines proposed, optimization techniques, including particle swarm optimization (PSO) and the genetic algorithm (GA), are employed. Simulation results demonstrate that the guidelines are potentially helpful for revising various algorithms.",0,3,2,0,1,1
47,The Package Blueprint: Visually analyzing and quantifying packages dependencies,"Software engineering, Software comprehension, Software maintenance, Software visualization","Large object-oriented applications are structured over many packages. Packages are important but complex structural entities that are difficult to understand since they act as containers of classes, which can have many dependencies with other classes spread over multiple packages. However to be able to take decisions (e.g. refactoring and/or assessment decisions), maintainers face the challenges of managing (sorting, grouping) the massive amount of dependencies between classes spread over multiple packages.. To help maintainers, there is a need for at the same time understanding, and quantifying, dependencies between classes as well as understanding how packages as containers of such classes depend on each other. In this paper, we present a visualization, named Package Blueprint, that reveals in detail package internal structure, as well as the dependencies between an observed package and its neighbors, at both package and class levels. Package blueprint aims at assisting maintainers in understanding package structure and dependencies, in particular when they focus on few packages and want to take refactoring decisions and/or to assess the structure of those packages. A package blueprint is a space filling matrix-based visualization, using two placement strategies that are enclosure and adjacency. Package blueprint is structured around the notion of surfaces that group classes and their dependencies by their packages (i.e., enclosure placement); whilst surfaces are placed next to their parent node which is the package under-analysis (i.e., adjacency placement). We present two views: one stressing how an observed package depends upon the rest of the system and another stressing how the system depends upon that package. To evaluate the contribution of package blueprint for understanding packages we performed an exploratory user study comparing package blueprint with an advanced IDE. The results show that users of package blueprint are faster in analyzing and assessing package structure. The results are proved statically significant and they show that package blueprint considerably improves the experience of standard browser users.",0,"The Package Blueprint: Visually analyzing and quantifying packages dependencies. Large object-oriented applications are structured over many packages. Packages are important but complex structural entities that are difficult to understand since they act as containers of classes, which can have many dependencies with other classes spread over multiple packages. However to be able to take decisions (e.g. refactoring and/or assessment decisions), maintainers face the challenges of managing (sorting, grouping) the massive amount of dependencies between classes spread over multiple packages.. To help maintainers, there is a need for at the same time understanding, and quantifying, dependencies between classes as well as understanding how packages as containers of such classes depend on each other. In this paper, we present a visualization, named Package Blueprint, that reveals in detail package internal structure, as well as the dependencies between an observed package and its neighbors, at both package and class levels. Package blueprint aims at assisting maintainers in understanding package structure and dependencies, in particular when they focus on few packages and want to take refactoring decisions and/or to assess the structure of those packages. A package blueprint is a space filling matrix-based visualization, using two placement strategies that are enclosure and adjacency. Package blueprint is structured around the notion of surfaces that group classes and their dependencies by their packages (i.e., enclosure placement); whilst surfaces are placed next to their parent node which is the package under-analysis (i.e., adjacency placement). We present two views: one stressing how an observed package depends upon the rest of the system and another stressing how the system depends upon that package. To evaluate the contribution of package blueprint for understanding packages we performed an exploratory user study comparing package blueprint with an advanced IDE. The results show that users of package blueprint are faster in analyzing and assessing package structure. The results are proved statically significant and they show that package blueprint considerably improves the experience of standard browser users.",0,0,0,0,1,3
48,A measurement framework for object-oriented software testability,,"Testing is an expensive activity in the development process of any software system. Measuring and assessing the testability of software would help in planning testing activities and allocating required resources.. More importantly, measuring software testability early in the development process, during analysis or design stages, can yield the highest payoff as design refactoring can be used to improve testability before the implementation starts. This paper presents a generic and extensible measurement framework for object-oriented software testability, which is based on a theory expressed as a set of operational hypotheses. We identify design attributes that have an impact on testability directly or indirectly, by having an impact on testing activities and sub-activities. We also describe the cause-effect relationships between these attributes and software testability based on thorough review of the literature and our own testing experience. Following the scientific method, we express them as operational hypotheses to be further tested. For each attribute, we provide a set of possible measures whose applicability largely depends on the level of details of the design documents and the testing techniques to be applied. The goal of this framework is twofold: (1) to provide structured guidance for practitioners trying to measure design testability, (2) to provide a theoretical framework for facilitating empirical research on testability.",0,"A measurement framework for object-oriented software testability. Testing is an expensive activity in the development process of any software system. Measuring and assessing the testability of software would help in planning testing activities and allocating required resources.. More importantly, measuring software testability early in the development process, during analysis or design stages, can yield the highest payoff as design refactoring can be used to improve testability before the implementation starts. This paper presents a generic and extensible measurement framework for object-oriented software testability, which is based on a theory expressed as a set of operational hypotheses. We identify design attributes that have an impact on testability directly or indirectly, by having an impact on testing activities and sub-activities. We also describe the cause-effect relationships between these attributes and software testability based on thorough review of the literature and our own testing experience. Following the scientific method, we express them as operational hypotheses to be further tested. For each attribute, we provide a set of possible measures whose applicability largely depends on the level of details of the design documents and the testing techniques to be applied. The goal of this framework is twofold: (1) to provide structured guidance for practitioners trying to measure design testability, (2) to provide a theoretical framework for facilitating empirical research on testability.",0,3,2,1,2,3
49,Refactoring legacy AJAX applications to improve the efficiency of the data exchange component,"XML, JSON, JavaScript, AJAX, Efficiency, Refactoring","The AJAX paradigm encodes data exchange XML formats. Recently, JSON has also become a popular data exchange format. XML has numerous benefits including human-readable structures and self-describing data. However, JSON provides significant performance gains over XML due to its light weight nature and native support for JavaScript. This is especially important for Rich Internet Applications (RIA). Therefore, it is necessary to change the data format from XML to JSON for efficiency purposes. This paper presents a refactoring system (XtoJ) to safely assist programmers migrate existing AJAX-based applications utilizing XML into functionally equivalent AJAX-based applications utilizing JSON. We empirically demonstrate that our transformation system significantly improves the efficiency of AJAX applications..",0,"Refactoring legacy AJAX applications to improve the efficiency of the data exchange component. The AJAX paradigm encodes data exchange XML formats. Recently, JSON has also become a popular data exchange format. XML has numerous benefits including human-readable structures and self-describing data. However, JSON provides significant performance gains over XML due to its light weight nature and native support for JavaScript. This is especially important for Rich Internet Applications (RIA). Therefore, it is necessary to change the data format from XML to JSON for efficiency purposes. This paper presents a refactoring system (XtoJ) to safely assist programmers migrate existing AJAX-based applications utilizing XML into functionally equivalent AJAX-based applications utilizing JSON. We empirically demonstrate that our transformation system significantly improves the efficiency of AJAX applications..",0,1,0,0,1,4
50,Chapter 9 - Other Software Development Approaches,,"Publisher Summary
Developing software is an enormous investment. Good programmers are not inexpensive, and systems of any size take a significant amount of time to develop and cost in infrastructure and manpower, whether one is an internal information technology (IT) department with a big training and rollout budget to manage or a product shop with marketing schedules and sales requests to manage---the problems are the same. In short, software projects are too expensive to let fail, and yet it is estimated that 81% of all IT projects do exactly that. Failures of large software projects cast a long shadow on the few success stories. Unlike digging a trench or stocking retail store shelves for an expected Christmas rush, it is not possible to simply add more human resources to make software projects finish sooner. All too frequently, the rush to get something out to market hits some blocking factor that causes a large chunk of refactoring. When a project begins to fail is not the point to add resources, for the explosion of jobs in IT and software development over the last few decades has done little to separate those who can from those who cannot develop quality software.",0,"Chapter 9 - Other Software Development Approaches. Publisher Summary
Developing software is an enormous investment. Good programmers are not inexpensive, and systems of any size take a significant amount of time to develop and cost in infrastructure and manpower, whether one is an internal information technology (IT) department with a big training and rollout budget to manage or a product shop with marketing schedules and sales requests to manage---the problems are the same. In short, software projects are too expensive to let fail, and yet it is estimated that 81% of all IT projects do exactly that. Failures of large software projects cast a long shadow on the few success stories. Unlike digging a trench or stocking retail store shelves for an expected Christmas rush, it is not possible to simply add more human resources to make software projects finish sooner. All too frequently, the rush to get something out to market hits some blocking factor that causes a large chunk of refactoring. When a project begins to fail is not the point to add resources, for the explosion of jobs in IT and software development over the last few decades has done little to separate those who can from those who cannot develop quality software.",0,3,0,0,0,3
51,Model evolution and refinement,"Evolution, Refinement, Formal methods, Object-Z, Refactoring","Software changes during its lifetime. Likewise, software models change during their design time, e.g. by removing, adding or changing operations and classes. This is referred to as model evolution. In a refinement-based approach to software design, we moreover do not deal with a single but with a chain of models (viz. formal specifications), related via refinement. Changes thus need to be consistently made to all specifications in the chain so as to keep the refinement structure. In this paper, we develop co-evolutions of models in the context of the formal method Object-Z. More specifically, given a particular evolution of a specification we show how to construct a corresponding evolution for its refinements such that the refinement relationship is kept. A chain of models can thus be systematically and consistently evolved, while maintaining the given refinement structure.",0,"Model evolution and refinement. Software changes during its lifetime. Likewise, software models change during their design time, e.g. by removing, adding or changing operations and classes. This is referred to as model evolution. In a refinement-based approach to software design, we moreover do not deal with a single but with a chain of models (viz. formal specifications), related via refinement. Changes thus need to be consistently made to all specifications in the chain so as to keep the refinement structure. In this paper, we develop co-evolutions of models in the context of the formal method Object-Z. More specifically, given a particular evolution of a specification we show how to construct a corresponding evolution for its refinements such that the refinement relationship is kept. A chain of models can thus be systematically and consistently evolved, while maintaining the given refinement structure.",1,1,2,0,0,1
52,Stateful traits and their formalization,"Traits, Mixin, Multiple-inheritance, Eiffel, Jigsaw, Flattening","Traits offer a fine-grained mechanism to compose classes from reusable components while avoiding problems of fragility brought by multiple inheritance and mixins. Traits as originally proposed are stateless, that is, they contain only methods, but no instance variables. State can only be accessed within stateless traits by accessors, which become required methods of the trait. Although this approach works reasonably well in practice, it means that many traits, viewed as software components, are artificially incomplete, and classes that use such traits may contain significant amounts of boilerplate glue code. We present an approach to stateful traits that is faithful to the guiding principle of stateless traits: the client retains control of the composition. Stateful traits consist of a minimal extension to stateless traits in which instance variables are purely local to the scope of a trait, unless they are explicitly made accessible by the composing client of a trait. We demonstrate by means of a formal object calculus that adding state to traits preserves the flattening property: traits contained in a program can be compiled away. We discuss and compare two implementation strategies, and briefly present a case study in which stateful traits have been used to refactor the trait-based version of the Smalltalk collection hierarchy.",0,"Stateful traits and their formalization. Traits offer a fine-grained mechanism to compose classes from reusable components while avoiding problems of fragility brought by multiple inheritance and mixins. Traits as originally proposed are stateless, that is, they contain only methods, but no instance variables. State can only be accessed within stateless traits by accessors, which become required methods of the trait. Although this approach works reasonably well in practice, it means that many traits, viewed as software components, are artificially incomplete, and classes that use such traits may contain significant amounts of boilerplate glue code. We present an approach to stateful traits that is faithful to the guiding principle of stateless traits: the client retains control of the composition. Stateful traits consist of a minimal extension to stateless traits in which instance variables are purely local to the scope of a trait, unless they are explicitly made accessible by the composing client of a trait. We demonstrate by means of a formal object calculus that adding state to traits preserves the flattening property: traits contained in a program can be compiled away. We discuss and compare two implementation strategies, and briefly present a case study in which stateful traits have been used to refactor the trait-based version of the Smalltalk collection hierarchy.",1,1,0,0,1,4
53,Chapter 8 - Detecting Duplicated Code,,"Publisher Summary
This chapter focuses on detecting a duplicated code. Whenever a piece of code is duplicated, it's similar to taking a loan, in the sense that something is being taken now for which the person has to pay later. There is nothing wrong with taking out a loan, but there is a choice between paying back a small amount now and paying back a lot later. Data from empirical studies show that typically between 8% and 12% of industrial software consists of duplicated code. It is important to identify duplicated code for the following reasons: (1) duplicated code hampers the introduction of changes, (2) duplicated code replicates and scatters the logic of a system, instead of grouping it into identifiable artifacts, and (3) Duplicated code creates a problem while maintaining multiple versions simultaneously, or merging different applications or versions. From a re-engineering perspective, usually people know whether or not a system suffers from duplication. Detecting Duplicated Code consists of two patterns: (1) Compare Code Mechanically that describes the process to detect duplicated code and (2) Visualize code as dot plots, which shows the way to understand the duplicated code better by simple matrix visualization. Once the duplication has been detected and understood in the system, the variety of tactics may be decided upon. Various refactoring patterns, such as extract method, may help to eliminate the duplication. Duplication may be a sign of misplaced responsibilities, in which case behavior close to data may be moved. Complex conditional statements are also a form of duplication and may indicate that multiple clients have to duplicate actions that should belong to the target class.",0,"Chapter 8 - Detecting Duplicated Code. Publisher Summary
This chapter focuses on detecting a duplicated code. Whenever a piece of code is duplicated, it's similar to taking a loan, in the sense that something is being taken now for which the person has to pay later. There is nothing wrong with taking out a loan, but there is a choice between paying back a small amount now and paying back a lot later. Data from empirical studies show that typically between 8% and 12% of industrial software consists of duplicated code. It is important to identify duplicated code for the following reasons: (1) duplicated code hampers the introduction of changes, (2) duplicated code replicates and scatters the logic of a system, instead of grouping it into identifiable artifacts, and (3) Duplicated code creates a problem while maintaining multiple versions simultaneously, or merging different applications or versions. From a re-engineering perspective, usually people know whether or not a system suffers from duplication. Detecting Duplicated Code consists of two patterns: (1) Compare Code Mechanically that describes the process to detect duplicated code and (2) Visualize code as dot plots, which shows the way to understand the duplicated code better by simple matrix visualization. Once the duplication has been detected and understood in the system, the variety of tactics may be decided upon. Various refactoring patterns, such as extract method, may help to eliminate the duplication. Duplication may be a sign of misplaced responsibilities, in which case behavior close to data may be moved. Complex conditional statements are also a form of duplication and may indicate that multiple clients have to duplicate actions that should belong to the target class.",0,2,0,0,3,0
54,"Preface: Volume 72, Issue 3",,"This volume contains the Proceedings of the Workshop on Graph Transformation and Visual Modelling Techniques (GT-VMT 2002). The Workshop was held in Barcelona, Spain, on October 11 and 12, 2002, as satellite event of the First International Conference on Graph Transformation (ICGT 2002).
Background
Diagrammatic notations have accompanied the development of technical and scientific disciplines in fields as diverse as mechanical engineering, quantum physics, category theory, and software engineering. In general, diagrammatic notations allow the construction of images associated with an interpretation based on considering as significant some well-defined spatial relations among graphical tokens. These tokens either derive from conventional notations employed in a user community or are elements specially designed to convey some meaning. The notations serve the purpose of defining the (types of) entities one is interested in and the types of relations among these entities. Hence, types must be distinguishable from one another and no ambiguity may arise as to their interpretation. Moreover, the set of spatial relations to be considered must be clearly defined, and the holding of any relation among any set of elements must be decidable. The evolution of diagrammatic notations usually follows a pattern that, from their usage as illustrations of sentences written in some formal or natural language, leads to the definition of modelling languages. These languages are endowed with rules for the construction of visual sentences from some elementary graphical components, and for interpreting the meaning of these sentences with respect to the modeled domain, up to rules for mapping the behaviour of the modeled systems onto the behaviour of the visual elements in the model.
Workshop Objectives
As diagrammatic notations, such as UML, become widespread in software engineering and visual end user environments, there is an increasing need of formal methods to precisely define the syntax and semantics of such diagrams. In particular, when visual models of systems or processes constitute executable specifications of systems, not only is a non-ambiguous specifications of their static syntax and semantics needed, but also an adequate notion of diagram dynamics. Such a notion must establish links (e.g., morphisms) which relate diagram transformations and transformations of the objects of the underlying domain. The field of Graph Grammars and Graph Transformation Systems has contributed much insight into the solution of these problems, but also other approaches (e.g., meta modelling, constraint-based and other rule-based systems), have been developed to tackle specific issues. The workshop has followed in the line of successful workshops on Graph Transformations and Visual Modelling Techniques, which were before held as satellite events of ICALP'00 and ICALP'01. It has gathered researchers working with different methodologies to discuss the relative merits and weaknesses of the different approaches to problems such as diagram parsing, diagram transformation, integrated management of syntactic and semantic aspects, tool support for working with visual models. The focus has been on methodological aspects rather than on particular technical aspects.
Program Committee
The papers in this volume were reviewed by the program committee consisting of Paolo Bottoni (Co-Chair)Department of Computer Science, University of Rome 'La Sapienza', ItalyAndrea CorradiniComputer Science Department, University of Pisa, ItalyGregor EngelsMathematics/Computer Science Department, University of Paderborn, GermanyReiko HeckelMathematics/Computer Science Department, University of Paderborn, GermanyStuart KentComputing Laboratory, University of Kent, UKBernd MeyerSchool of Computer Science and Software Engineering, Monash University, Melbourne, AustraliaMark Minas (Co-Chair)Department of Computer Science, University of the Federal Armed Forces, Munich, GermanyFrancesco Parisi PresicceDepartment of Computer Science, University of Rome 'La Sapienza', ItalyMauro Pezz{\`e}Department of Information Sciences, University of Milan Bicocca, ItalyGrzegorz RozenbergInstitute of Advanced Computer Science, Leiden University, The NetherlandsGabriele TaentzerComputer Science Department, Technical University of Berlin, Germany
Workshop program
The workshop was scheduled for one and a half day and included a session with an invited talk by Martin Gogolla as well as 12presentations of papers in four regular sessions on Geometry and Visualization, on Frameworks and Tools, on Euler/Venn Diagrams, and on Components, Models, and Semantics.
Joint Session
The workshop featured a special session on Case Studies for Visual Modelling Techniques held jointly with the Workshop on Software Evolution Through Transformations (SET 2002). This session was part of the work carried out under the European research training network SegraVis (for Syntactic and Semantic Integration of Visual Modelling Techniques) with the objective to employ, evaluate, and improve visual modelling techniques in specific domains, including (but not limited to) modelling support for software evolution and refactoringmodelling of component-based software architecturesspecification of applications with mobile soft- and hardware Beside a general discussion of these objectives, the session consisted in presentations of three submitted case studies and position statements by the SegraVis objective coordinators.
Acknowledgement
This workshop was supported by the European research training network Segra Vis.",0,"Preface: Volume 72, Issue 3. This volume contains the Proceedings of the Workshop on Graph Transformation and Visual Modelling Techniques (GT-VMT 2002). The Workshop was held in Barcelona, Spain, on October 11 and 12, 2002, as satellite event of the First International Conference on Graph Transformation (ICGT 2002).
Background
Diagrammatic notations have accompanied the development of technical and scientific disciplines in fields as diverse as mechanical engineering, quantum physics, category theory, and software engineering. In general, diagrammatic notations allow the construction of images associated with an interpretation based on considering as significant some well-defined spatial relations among graphical tokens. These tokens either derive from conventional notations employed in a user community or are elements specially designed to convey some meaning. The notations serve the purpose of defining the (types of) entities one is interested in and the types of relations among these entities. Hence, types must be distinguishable from one another and no ambiguity may arise as to their interpretation. Moreover, the set of spatial relations to be considered must be clearly defined, and the holding of any relation among any set of elements must be decidable. The evolution of diagrammatic notations usually follows a pattern that, from their usage as illustrations of sentences written in some formal or natural language, leads to the definition of modelling languages. These languages are endowed with rules for the construction of visual sentences from some elementary graphical components, and for interpreting the meaning of these sentences with respect to the modeled domain, up to rules for mapping the behaviour of the modeled systems onto the behaviour of the visual elements in the model.
Workshop Objectives
As diagrammatic notations, such as UML, become widespread in software engineering and visual end user environments, there is an increasing need of formal methods to precisely define the syntax and semantics of such diagrams. In particular, when visual models of systems or processes constitute executable specifications of systems, not only is a non-ambiguous specifications of their static syntax and semantics needed, but also an adequate notion of diagram dynamics. Such a notion must establish links (e.g., morphisms) which relate diagram transformations and transformations of the objects of the underlying domain. The field of Graph Grammars and Graph Transformation Systems has contributed much insight into the solution of these problems, but also other approaches (e.g., meta modelling, constraint-based and other rule-based systems), have been developed to tackle specific issues. The workshop has followed in the line of successful workshops on Graph Transformations and Visual Modelling Techniques, which were before held as satellite events of ICALP'00 and ICALP'01. It has gathered researchers working with different methodologies to discuss the relative merits and weaknesses of the different approaches to problems such as diagram parsing, diagram transformation, integrated management of syntactic and semantic aspects, tool support for working with visual models. The focus has been on methodological aspects rather than on particular technical aspects.
Program Committee
The papers in this volume were reviewed by the program committee consisting of Paolo Bottoni (Co-Chair)Department of Computer Science, University of Rome 'La Sapienza', ItalyAndrea CorradiniComputer Science Department, University of Pisa, ItalyGregor EngelsMathematics/Computer Science Department, University of Paderborn, GermanyReiko HeckelMathematics/Computer Science Department, University of Paderborn, GermanyStuart KentComputing Laboratory, University of Kent, UKBernd MeyerSchool of Computer Science and Software Engineering, Monash University, Melbourne, AustraliaMark Minas (Co-Chair)Department of Computer Science, University of the Federal Armed Forces, Munich, GermanyFrancesco Parisi PresicceDepartment of Computer Science, University of Rome 'La Sapienza', ItalyMauro Pezz{\`e}Department of Information Sciences, University of Milan Bicocca, ItalyGrzegorz RozenbergInstitute of Advanced Computer Science, Leiden University, The NetherlandsGabriele TaentzerComputer Science Department, Technical University of Berlin, Germany
Workshop program
The workshop was scheduled for one and a half day and included a session with an invited talk by Martin Gogolla as well as 12presentations of papers in four regular sessions on Geometry and Visualization, on Frameworks and Tools, on Euler/Venn Diagrams, and on Components, Models, and Semantics.
Joint Session
The workshop featured a special session on Case Studies for Visual Modelling Techniques held jointly with the Workshop on Software Evolution Through Transformations (SET 2002). This session was part of the work carried out under the European research training network SegraVis (for Syntactic and Semantic Integration of Visual Modelling Techniques) with the objective to employ, evaluate, and improve visual modelling techniques in specific domains, including (but not limited to) modelling support for software evolution and refactoringmodelling of component-based software architecturesspecification of applications with mobile soft- and hardware Beside a general discussion of these objectives, the session consisted in presentations of three submitted case studies and position statements by the SegraVis objective coordinators.
Acknowledgement
This workshop was supported by the European research training network Segra Vis.",1,0,0,0,0,1
55,A theory of software product line refinement,"Software product lines, Software evolution, Refinement, Refactoring","To safely evolve a software product line, it is important to have a notion of product line refinement that assures behavior preservation of the original product line products. So in this article we present a language independent theory of product line refinement, establishing refinement properties that justify stepwise and compositional product line evolution.. Moreover, we instantiate our theory with the formalization of specific languages for typical product lines artifacts, and then introduce and prove soundness of a number of associated product line refinement transformation templates. These templates can be used to reason about specific product lines and as a basis to derive comprehensive product line refinement catalogues..",0,"A theory of software product line refinement. To safely evolve a software product line, it is important to have a notion of product line refinement that assures behavior preservation of the original product line products. So in this article we present a language independent theory of product line refinement, establishing refinement properties that justify stepwise and compositional product line evolution.. Moreover, we instantiate our theory with the formalization of specific languages for typical product lines artifacts, and then introduce and prove soundness of a number of associated product line refinement transformation templates. These templates can be used to reason about specific product lines and as a basis to derive comprehensive product line refinement catalogues..",1,1,0,0,0,1
56,Software Testing for security,,"Software testing in the form of unit, integration and acceptance tests are key phases of many development methodologies and particularly favoured by agile development1 proponents. These tests serve to prevent regressions, assist refactoring and of course prove that the software meets the functional requirements. Software tests provide a natural, but often overlooked, opportunity for integrating security early on in the software development lifecycle (SDLC) where identified vulnerabilities can be corrected while development is still ongoing.",0,"Software Testing for security. Software testing in the form of unit, integration and acceptance tests are key phases of many development methodologies and particularly favoured by agile development1 proponents. These tests serve to prevent regressions, assist refactoring and of course prove that the software meets the functional requirements. Software tests provide a natural, but often overlooked, opportunity for integrating security early on in the software development lifecycle (SDLC) where identified vulnerabilities can be corrected while development is still ongoing.",0,3,0,0,0,2
57,A comparison of some soft computing methods for software fault prediction,"Software fault prediction, McCabe metrics, Adaptive neuro fuzzy systems, Artificial Neural Networks, Support Vector Machines","The main expectation from reliable software is the minimization of the number of failures that occur when the program runs. Determining whether software modules are prone to fault is important because doing so assists in identifying modules that require refactoring or detailed testing. Software fault prediction is a discipline that predicts the fault proneness of future modules by using essential prediction metrics and historical fault data. This study presents the first application of the Adaptive Neuro Fuzzy Inference System (ANFIS) for the software fault prediction problem. Moreover, Artificial Neural Network (ANN) and Support Vector Machine (SVM) methods, which were experienced previously, are built to discuss the performance of ANFIS. Data used in this study are collected from the PROMISE Software Engineering Repository, and McCabe metrics are selected because they comprehensively address the programming effort. ROC-AUC is used as a performance measure. The results achieved were 0.7795, 0.8685, and 0.8573 for the SVM, ANN and ANFIS methods, respectively.",0,"A comparison of some soft computing methods for software fault prediction. The main expectation from reliable software is the minimization of the number of failures that occur when the program runs. Determining whether software modules are prone to fault is important because doing so assists in identifying modules that require refactoring or detailed testing. Software fault prediction is a discipline that predicts the fault proneness of future modules by using essential prediction metrics and historical fault data. This study presents the first application of the Adaptive Neuro Fuzzy Inference System (ANFIS) for the software fault prediction problem. Moreover, Artificial Neural Network (ANN) and Support Vector Machine (SVM) methods, which were experienced previously, are built to discuss the performance of ANFIS. Data used in this study are collected from the PROMISE Software Engineering Repository, and McCabe metrics are selected because they comprehensively address the programming effort. ROC-AUC is used as a performance measure. The results achieved were 0.7795, 0.8685, and 0.8573 for the SVM, ANN and ANFIS methods, respectively.",0,2,2,0,2,3
58,Decision Model for Software Architectural Tactics Selection Based on Quality Attributes Requirements,"Software Architecture, Architecture Styles and Tactics, Quality Attributes (QA), Mining Techniques, Business Intelligence (BI)","Due to increasing industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture and implementation mechanisms become an important activity. The decisions made during architecture design have significant implications on quality goals. As addressed, there is a lack of available standard models, architectures or frameworks for enabling implementation of quality attributes specially for business intelligence environment and applications in order to rapidly and efficiently supports decision-making. In addition, a lack of researches related to Quality Attributes (QA) requirements, its implementation tactics, and interrelations or correlations between them. The increasing systems complexity mandates software architects to choose from a growing number of design options (decisions) when searching for an optimal architecture design in a specific domain with respect to a defined (set of) quality attributes and constraints. This results in a design space search that is over human capabilities and makes the architectural design task more complicated. In this paper, researcher aimed to reveal most of quality attributes implementation tactics affecting applications architectures, properties. Several quality attributes of software investigated using applied research methods with mixed quantitative (linear) and non-linear analysis techniques. It proposes an initiative for finding an easy and systematic way of addressing quality attributes requirements to a set of implementing architectural tactics. Finally, the findings analyzed and visualized in a way that can support decision stakeholders in addition to a new concept of ``safe-tactics'' introduced as reduced (pruned) set of tactics that are claimed to be better used in general refactoring cases. In addition, a software tool is developed throughout this research effort as result of gained knowledge and addressing the research findings.",0,"Decision Model for Software Architectural Tactics Selection Based on Quality Attributes Requirements. Due to increasing industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture and implementation mechanisms become an important activity. The decisions made during architecture design have significant implications on quality goals. As addressed, there is a lack of available standard models, architectures or frameworks for enabling implementation of quality attributes specially for business intelligence environment and applications in order to rapidly and efficiently supports decision-making. In addition, a lack of researches related to Quality Attributes (QA) requirements, its implementation tactics, and interrelations or correlations between them. The increasing systems complexity mandates software architects to choose from a growing number of design options (decisions) when searching for an optimal architecture design in a specific domain with respect to a defined (set of) quality attributes and constraints. This results in a design space search that is over human capabilities and makes the architectural design task more complicated. In this paper, researcher aimed to reveal most of quality attributes implementation tactics affecting applications architectures, properties. Several quality attributes of software investigated using applied research methods with mixed quantitative (linear) and non-linear analysis techniques. It proposes an initiative for finding an easy and systematic way of addressing quality attributes requirements to a set of implementing architectural tactics. Finally, the findings analyzed and visualized in a way that can support decision stakeholders in addition to a new concept of ``safe-tactics'' introduced as reduced (pruned) set of tactics that are claimed to be better used in general refactoring cases. In addition, a software tool is developed throughout this research effort as result of gained knowledge and addressing the research findings.",0,3,0,0,0,3
59,Attributed graph transformation with inheritance: Efficient conflict detection and local confluence analysis using abstract critical pairs,"Typed attributed graph transformation, Critical pair analysis, Inheritance, -adhesive category with NACs","Inheritance is an important and widely spread concept enabling the elegant expression of hierarchy in object-oriented software programs or models. It has been defined for graphs and graph transformations enhancing the applicability of this formal technique. Up to now, for the analysis of transformations with inheritance a flattening construction has been used, which yields all the well-known results for graph transformation but results in a large number of graphs and rules that have to be analyzed. In this paper, we introduce a new category of typed attributed graphs with inheritance. For the detection of conflicts between graph transformations on these graphs, the notion of abstract critical pairs is defined. This allows us to perform the analysis on polymorphic rules and transformations without the need for flattening, which significantly increases the efficiency of the analysis and eases the interpretation of the analysis results. The new main result is the Local Confluence Theorem for typed attributed graph transformation with inheritance using abstract critical pairs. All constructions and results are demonstrated on an example for the analysis of refactorings.",0,"Attributed graph transformation with inheritance: Efficient conflict detection and local confluence analysis using abstract critical pairs. Inheritance is an important and widely spread concept enabling the elegant expression of hierarchy in object-oriented software programs or models. It has been defined for graphs and graph transformations enhancing the applicability of this formal technique. Up to now, for the analysis of transformations with inheritance a flattening construction has been used, which yields all the well-known results for graph transformation but results in a large number of graphs and rules that have to be analyzed. In this paper, we introduce a new category of typed attributed graphs with inheritance. For the detection of conflicts between graph transformations on these graphs, the notion of abstract critical pairs is defined. This allows us to perform the analysis on polymorphic rules and transformations without the need for flattening, which significantly increases the efficiency of the analysis and eases the interpretation of the analysis results. The new main result is the Local Confluence Theorem for typed attributed graph transformation with inheritance using abstract critical pairs. All constructions and results are demonstrated on an example for the analysis of refactorings.",1,1,0,0,1,1
60,Improving the performance of load balancing in software-defined networks through load variance-based synchronization,"Software-Defined Networking, Load balancing, Multiple controllers, Controller state synchronization","Software-Defined Networking (SDN) is a new network technology that decouples the control plane logic from the data plane and uses a programmable software controller to manage network operation and the state of network components. In an SDN network, a logically centralized controller uses a global network view to conduct management and operation of the network.. The centralized control of the SDN network presents a tremendous opportunity for network operators to refactor the control plane and to improve the performance of applications. For the application of load balancing, the logically centralized controller conducts Real-time Least loaded Server selection (RLS) for multiple domains, where new flows pass by for the first time. The function of RLS is to enable the new flows to be forwarded to the least loaded server in the entire network. However, in a large-scale SDN network, the logically centralized controller usually consists of multiple distributed controllers. Existing multiple controller state synchronization schemes are based on Periodic Synchronization (PS), which can cause undesirable situations. For example, frequent synchronizations may result in high synchronization overhead of controllers. State desynchronization among controllers during the interval between two consecutive synchronizations could lead to forwarding loops and black holes. In this paper, we propose a new type of controller state synchronization scheme, Load Variance-based Synchronization (LVS), to improve the load-balancing performance in the multi-controller multi-domain SDN network. Compared with PS-based schemes, LVS-based schemes conduct effective state synchronizations among controllers only when the load of a specific server or domain exceeds a certain threshold, which significantly reduces the synchronization overhead of controllers. The results of simulations show that LVS achieves loop-free forwarding and good load-balancing performance with much less synchronization overhead, as compared with existing schemes.",0,"Improving the performance of load balancing in software-defined networks through load variance-based synchronization. Software-Defined Networking (SDN) is a new network technology that decouples the control plane logic from the data plane and uses a programmable software controller to manage network operation and the state of network components. In an SDN network, a logically centralized controller uses a global network view to conduct management and operation of the network.. The centralized control of the SDN network presents a tremendous opportunity for network operators to refactor the control plane and to improve the performance of applications. For the application of load balancing, the logically centralized controller conducts Real-time Least loaded Server selection (RLS) for multiple domains, where new flows pass by for the first time. The function of RLS is to enable the new flows to be forwarded to the least loaded server in the entire network. However, in a large-scale SDN network, the logically centralized controller usually consists of multiple distributed controllers. Existing multiple controller state synchronization schemes are based on Periodic Synchronization (PS), which can cause undesirable situations. For example, frequent synchronizations may result in high synchronization overhead of controllers. State desynchronization among controllers during the interval between two consecutive synchronizations could lead to forwarding loops and black holes. In this paper, we propose a new type of controller state synchronization scheme, Load Variance-based Synchronization (LVS), to improve the load-balancing performance in the multi-controller multi-domain SDN network. Compared with PS-based schemes, LVS-based schemes conduct effective state synchronizations among controllers only when the load of a specific server or domain exceeds a certain threshold, which significantly reduces the synchronization overhead of controllers. The results of simulations show that LVS achieves loop-free forwarding and good load-balancing performance with much less synchronization overhead, as compared with existing schemes.",0,0,0,0,1,1
61,A Framework for Establishing Formal Conformance between Object Models and Object-Oriented Programs,"Object Model, Semantics, Conformance","Conformance between structural models and their implementations are usually simplified in practice, restraining reasoning to simple mappings between modeling and implementation constructs. This is not appropriate to accommodate the usual freedom of implementation for abstract concepts. A more flexible conformance notion must be addressed by conformance checking tools and model-driven development. In this paper, we propose a formal framework for defining conformance relationships between structural object models and object-oriented programs. In our framework, a syntactic mapping between model and program elements must be provided, yielding a coupling relation, used in framework instantiations for specific conformance relationships. Additionally, as in practice some intermediate program states are not relevant to conformance, we include the notion of heaps of interest, encompassing the filtered stable states for a less strict conformance checking. The framework is applied for establishing a conformance relationship in a technique of model-driven refactoring of programs.",0,"A Framework for Establishing Formal Conformance between Object Models and Object-Oriented Programs. Conformance between structural models and their implementations are usually simplified in practice, restraining reasoning to simple mappings between modeling and implementation constructs. This is not appropriate to accommodate the usual freedom of implementation for abstract concepts. A more flexible conformance notion must be addressed by conformance checking tools and model-driven development. In this paper, we propose a formal framework for defining conformance relationships between structural object models and object-oriented programs. In our framework, a syntactic mapping between model and program elements must be provided, yielding a coupling relation, used in framework instantiations for specific conformance relationships. Additionally, as in practice some intermediate program states are not relevant to conformance, we include the notion of heaps of interest, encompassing the filtered stable states for a less strict conformance checking. The framework is applied for establishing a conformance relationship in a technique of model-driven refactoring of programs.",1,1,0,1,1,1
62,A Survey of Rewriting Strategies in Program Transformation Systems,,"Program transformation is used in a wide range of applications including compiler construction, optimization, program synthesis, refactoring, software renovation, and reverse engineering. Complex program transformations are achieved through a number of consecutive modifications of a program. Transformation rules define basic modifications. A transformation strategy is an algorithm for choosing a path in the rewrite relation induced by a set of rules. This paper surveys the support for the definition of strategies in program transformation systems. After a discussion of kinds of program transformation and choices in program representation, the basic elements of a strategy system are discussed and the choices in the design of a strategy language are considered. Several styles of strategy systems as provided in existing languages are then analyzed.",0,"A Survey of Rewriting Strategies in Program Transformation Systems. Program transformation is used in a wide range of applications including compiler construction, optimization, program synthesis, refactoring, software renovation, and reverse engineering. Complex program transformations are achieved through a number of consecutive modifications of a program. Transformation rules define basic modifications. A transformation strategy is an algorithm for choosing a path in the rewrite relation induced by a set of rules. This paper surveys the support for the definition of strategies in program transformation systems. After a discussion of kinds of program transformation and choices in program representation, the basic elements of a strategy system are discussed and the choices in the design of a strategy language are considered. Several styles of strategy systems as provided in existing languages are then analyzed.",1,1,2,0,0,4
63,Coherent clusters in source code,"Dependence analysis, Program comprehension, Software clustering","This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have `coherent' shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution.",0,"Coherent clusters in source code. This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have `coherent' shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution.",1,2,0,0,1,3
64,Short-term forecasting of high-speed rail demand: A hybrid approach combining ensemble empirical mode decomposition and gray support vector machine with real-world applications in China,"High-speed rail (HSR), Demand forecasting, Hybrid model, Ensemble empirical mode decomposition (EEMD), Grey support vector machine (GSVM)","Short-term forecasting of high-speed rail (HSR) passenger flow provides daily ridership estimates that account for day-to-day demand variations in the near future (e.g., next week, next month). It is one of the most critical tasks in high-speed passenger rail planning, operational decision-making and dynamic operation adjustment. An accurate short-term HSR demand prediction provides a basis for effective rail revenue management. In this paper, a hybrid short-term demand forecasting approach is developed by combining the ensemble empirical mode decomposition (EEMD) and grey support vector machine (GSVM) models. There are three steps in this hybrid forecasting approach: (i) decompose short-term passenger flow data with noises into a number of intrinsic mode functions (IMFs) and a trend term; (ii) predict each IMF using GSVM calibrated by the particle swarm optimization (PSO); (iii) reconstruct the refined IMF components to produce the final predicted daily HSR passenger flow, where the PSO is also applied to achieve the optimal refactoring combination. This innovative hybrid approach is demonstrated with three typical origin--destination pairs along the Wuhan-Guangzhou HSR in China. Mean absolute percentage errors of the EEMD-GSVM predictions using testing sets are 6.7%, 5.1% and 6.5%, respectively, which are much lower than those of two existing forecasting approaches (support vector machine and autoregressive integrated moving average). Application results indicate that the proposed hybrid forecasting approach performs well in terms of prediction accuracy and is especially suitable for short-term HSR passenger flow forecasting.",0,"Short-term forecasting of high-speed rail demand: A hybrid approach combining ensemble empirical mode decomposition and gray support vector machine with real-world applications in China. Short-term forecasting of high-speed rail (HSR) passenger flow provides daily ridership estimates that account for day-to-day demand variations in the near future (e.g., next week, next month). It is one of the most critical tasks in high-speed passenger rail planning, operational decision-making and dynamic operation adjustment. An accurate short-term HSR demand prediction provides a basis for effective rail revenue management. In this paper, a hybrid short-term demand forecasting approach is developed by combining the ensemble empirical mode decomposition (EEMD) and grey support vector machine (GSVM) models. There are three steps in this hybrid forecasting approach: (i) decompose short-term passenger flow data with noises into a number of intrinsic mode functions (IMFs) and a trend term; (ii) predict each IMF using GSVM calibrated by the particle swarm optimization (PSO); (iii) reconstruct the refined IMF components to produce the final predicted daily HSR passenger flow, where the PSO is also applied to achieve the optimal refactoring combination. This innovative hybrid approach is demonstrated with three typical origin--destination pairs along the Wuhan-Guangzhou HSR in China. Mean absolute percentage errors of the EEMD-GSVM predictions using testing sets are 6.7%, 5.1% and 6.5%, respectively, which are much lower than those of two existing forecasting approaches (support vector machine and autoregressive integrated moving average). Application results indicate that the proposed hybrid forecasting approach performs well in terms of prediction accuracy and is especially suitable for short-term HSR passenger flow forecasting.",0,2,0,0,2,1
65,"A component composition model providing dynamic, flexible, and hierarchical composition of components for supporting software evolution","Software evolution, Software composition technique, Component composition model, Component reuse","Component composition is one of the practical and effective approaches for supporting software evolution. However, existing component composition techniques need to be complemented by advanced features which address various sophisticated composition issues. In this paper, we introduce a set of features that supports and manages dynamic as well as flexible composition of components in a controlled way. We also propose a component composition model that supports these features. The proposed model enables dynamic, flexible, and hierarchical composition of components by providing and manipulating dedicated composition information, which in turn increases reusability of components and capabilities for supporting software evolution. To show the benefits of our model concretely, we provide a Hotel Reservation System case study. The experimental results show that our model supports software evolution effectively and provides efficient and modular structures, refactoring, and collaboration-level extensions as well.",0,"A component composition model providing dynamic, flexible, and hierarchical composition of components for supporting software evolution. Component composition is one of the practical and effective approaches for supporting software evolution. However, existing component composition techniques need to be complemented by advanced features which address various sophisticated composition issues. In this paper, we introduce a set of features that supports and manages dynamic as well as flexible composition of components in a controlled way. We also propose a component composition model that supports these features. The proposed model enables dynamic, flexible, and hierarchical composition of components by providing and manipulating dedicated composition information, which in turn increases reusability of components and capabilities for supporting software evolution. To show the benefits of our model concretely, we provide a Hotel Reservation System case study. The experimental results show that our model supports software evolution effectively and provides efficient and modular structures, refactoring, and collaboration-level extensions as well.",0,3,2,0,2,1
66,Evolution of rule-based programs,"Software evolution, Program transformation, Rule-based programming, Meta-programming, Natural semantics, Structural operational semantics, Logic programming, Attribute grammars, Constructive algebraic specification, Refactoring, Extensibility, Reuse","The term rule-based program is meant to include definite clause programs, SOS specifications, attribute grammars, and conditional rewrite systems. These setups are widely used for the executable specification or implementation of language-based tools, e.g., interpreters, translators, type checkers, program analysers, and program transformations. We provide a pragmatic, transformation-based approach for expressing and tracking changes in rule-based programs in the course of program evolution. To this end, we design an operator suite for the transformation of rule-based programs. The operators facilitate steps for clean-up, refactoring, and enhancement. We use SOS-based interpreter examples to illustrate evolution of rule-based programs. We use logic programming to execute the examples, while the relevant evolution operators are made available as logic meta-programs.",0,"Evolution of rule-based programs. The term rule-based program is meant to include definite clause programs, SOS specifications, attribute grammars, and conditional rewrite systems. These setups are widely used for the executable specification or implementation of language-based tools, e.g., interpreters, translators, type checkers, program analysers, and program transformations. We provide a pragmatic, transformation-based approach for expressing and tracking changes in rule-based programs in the course of program evolution. To this end, we design an operator suite for the transformation of rule-based programs. The operators facilitate steps for clean-up, refactoring, and enhancement. We use SOS-based interpreter examples to illustrate evolution of rule-based programs. We use logic programming to execute the examples, while the relevant evolution operators are made available as logic meta-programs.",1,1,1,1,1,4
67,On flexible dynamic trait replacement for Java-like languages,"Featherweight Java, Trait, Type system","Dynamic trait replacement is a programming language feature for changing the objects' behavior at runtime by replacing some of the objects' methods. In previous work on dynamic trait replacement for Java-like languages, the objects' methods that may be replaced must correspond exactly to a named trait used in the object's class definition. In this paper we propose the notion of replaceable: a programming language feature that decouples the trait replacement operation code and the class declaration code, thus making it possible to refactor classes and to perform unanticipated trait replacement operations without invalidating existing code. We give a formal account of our proposal through a core calculus, FDTJ (Featherweight Dynamic Trait Java), equipped with a static type system guaranteeing that in a well-typed program no runtime type error will take place.",0,"On flexible dynamic trait replacement for Java-like languages. Dynamic trait replacement is a programming language feature for changing the objects' behavior at runtime by replacing some of the objects' methods. In previous work on dynamic trait replacement for Java-like languages, the objects' methods that may be replaced must correspond exactly to a named trait used in the object's class definition. In this paper we propose the notion of replaceable: a programming language feature that decouples the trait replacement operation code and the class declaration code, thus making it possible to refactor classes and to perform unanticipated trait replacement operations without invalidating existing code. We give a formal account of our proposal through a core calculus, FDTJ (Featherweight Dynamic Trait Java), equipped with a static type system guaranteeing that in a well-typed program no runtime type error will take place.",1,1,0,0,1,4
68,Software architecture graphs as complex networks: A novel partitioning scheme to measure stability and evolution,"Software measurement, Software metric, Lehman's laws, Complexity, Scale-free, Directed network, Power law","The stability and evolution of the structure of consecutive versions of a series of software architecture graphs are analysed using the theory of complex networks. Brief comparisons are drawn between the scale-free behaviour and second order phase transitions. On this basis a software design metric Icc is proposed. This software metric is used to quantify the evolution of the stability vs. maintainability of the software through various releases. It is demonstrated that the classes in the software graph are acquiring more out-going calls than incoming calls as the software ages. Three examples of software applications where maintainability and continuous refactoring are an inherent part of their development process are presented, in addition to a Sun Java2 framework where growth and backward compatibility are the more important factors for the development. Further to this a projected future evolution of the software structure and maintainability is calculated. Suggestions for future applications to software engineering and the natural sciences are briefly presented.",0,"Software architecture graphs as complex networks: A novel partitioning scheme to measure stability and evolution. The stability and evolution of the structure of consecutive versions of a series of software architecture graphs are analysed using the theory of complex networks. Brief comparisons are drawn between the scale-free behaviour and second order phase transitions. On this basis a software design metric Icc is proposed. This software metric is used to quantify the evolution of the stability vs. maintainability of the software through various releases. It is demonstrated that the classes in the software graph are acquiring more out-going calls than incoming calls as the software ages. Three examples of software applications where maintainability and continuous refactoring are an inherent part of their development process are presented, in addition to a Sun Java2 framework where growth and backward compatibility are the more important factors for the development. Further to this a projected future evolution of the software structure and maintainability is calculated. Suggestions for future applications to software engineering and the natural sciences are briefly presented.",0,3,0,0,0,3
69,A fully decompressed synthetic bacteriophage {\o}X174 genome assembled and archived in yeast,"Refactoring, {\o}X174, Phage, Yeast, Recombination, Synthetic biology, Genome engineering, Synthetic genomics, DNA assembly, Irreducible complexity, Overprinting, Overlapping genes","The 5386 nucleotide bacteriophage {\o}X174 genome has a complicated architecture that encodes 11 gene products via overlapping protein coding sequences spanning multiple reading frames. We designed a 6302 nucleotide synthetic surrogate, {\o}X174.1, that fully separates all primary phage protein coding sequences along with cognate translation control elements. To specify {\o}X174.1f, a decompressed genome the same length as wild type, we truncated the gene F coding sequence. We synthesized DNA encoding fragments of {\o}X174.1f and used a combination of in vitro- and yeast-based assembly to produce yeast vectors encoding natural or designer bacteriophage genomes. We isolated clonal preparations of yeast plasmid DNA and transfected E. coli C strains. We recovered viable {\o}X174 particles containing the {\o}X174.1f genome from E. coli C strains that independently express full-length gene F. We expect that yeast can serve as a genomic `drydock' within which to maintain and manipulate clonal lineages of other obligate lytic phage.",0,"A fully decompressed synthetic bacteriophage {\o}X174 genome assembled and archived in yeast. The 5386 nucleotide bacteriophage {\o}X174 genome has a complicated architecture that encodes 11 gene products via overlapping protein coding sequences spanning multiple reading frames. We designed a 6302 nucleotide synthetic surrogate, {\o}X174.1, that fully separates all primary phage protein coding sequences along with cognate translation control elements. To specify {\o}X174.1f, a decompressed genome the same length as wild type, we truncated the gene F coding sequence. We synthesized DNA encoding fragments of {\o}X174.1f and used a combination of in vitro- and yeast-based assembly to produce yeast vectors encoding natural or designer bacteriophage genomes. We isolated clonal preparations of yeast plasmid DNA and transfected E. coli C strains. We recovered viable {\o}X174 particles containing the {\o}X174.1f genome from E. coli C strains that independently express full-length gene F. We expect that yeast can serve as a genomic `drydock' within which to maintain and manipulate clonal lineages of other obligate lytic phage.",1,2,0,0,1,0
70,A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines,"Software product lines, Feature-oriented programming, Aspect-oriented programming, Aspectual feature modules, Variability mechanisms","Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from two SPLs developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the SPL structure.",0,"A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines. Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from two SPLs developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the SPL structure.",0,2,0,0,0,1
71,Chapter 18 - Document Literal Interfaces,,"Publisher Summary
The service-oriented architecture (SOA) loose-coupling principle becomes a critical consideration for the service interface design. The exchange of messages to represent and comply with the service interface is a fundamental servicing approach. However, within those messages, there are different structural designs that comply with the loose-coupling principle and those that are in violation. The two most common structural forms for a message are RPC style and document literal. While the RPC style of service operations and interfaces is well known and simple, it also has potential disadvantages to the service interface. Consumers that are bound to an RPC-style interface are more likely to be negatively impacted by change. If the service were modified to include a new parameter element for the RPC-style operation, it is highly likely that existing consumers of that service and operation would also require a refactoring of the object model and a rebinding to the service interface. An alternative to the RPC style is the document literal style of operation and interface. With the document literal style, the service interface defines a document context. The resulting messages are not structured around discretely defined message elements and message-specified data types but appear in more of a document context. The document literal style of service operations and interface design is the most effective and recommended design approach.",0,"Chapter 18 - Document Literal Interfaces. Publisher Summary
The service-oriented architecture (SOA) loose-coupling principle becomes a critical consideration for the service interface design. The exchange of messages to represent and comply with the service interface is a fundamental servicing approach. However, within those messages, there are different structural designs that comply with the loose-coupling principle and those that are in violation. The two most common structural forms for a message are RPC style and document literal. While the RPC style of service operations and interfaces is well known and simple, it also has potential disadvantages to the service interface. Consumers that are bound to an RPC-style interface are more likely to be negatively impacted by change. If the service were modified to include a new parameter element for the RPC-style operation, it is highly likely that existing consumers of that service and operation would also require a refactoring of the object model and a rebinding to the service interface. An alternative to the RPC style is the document literal style of operation and interface. With the document literal style, the service interface defines a document context. The resulting messages are not structured around discretely defined message elements and message-specified data types but appear in more of a document context. The document literal style of service operations and interface design is the most effective and recommended design approach.",0,2,0,0,1,1
72,Investigating Architectural Technical Debt accumulation and refactoring over time: A multiple-case study,"Architectural Technical Debt, Software management, Software architecture, Agile software development, Software life-cycle, Qualitative model","Context
A known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility.
Objective
This paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time.
Method
We conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models.
Results
We compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research.
Conclusion
Several factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. ATD needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.",0,"Investigating Architectural Technical Debt accumulation and refactoring over time: A multiple-case study. Context
A known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility.
Objective
This paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time.
Method
We conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models.
Results
We compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research.
Conclusion
Several factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. ATD needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.",0,2,0,0,0,3
73,Pattern-based model refactoring for the introduction association relationship,"Model refactoring, UML, B, CSP, Association relationship","Refactoring is an important software development process involving the restructuring of a model to improve its internal qualities without changing its external behavior. In this paper, we propose a new approach of model refactoring based on the combined use of UML, B and CSP. UML models are described by class diagrams, OCL constraints, and state machine diagrams. We detail a refactoring pattern that allows for the introduction of an association relationship between two existing classes. We illustrate our proposal by giving a case study involving the SAAT (Software Architecture Analysis Tool) system.",0,"Pattern-based model refactoring for the introduction association relationship. Refactoring is an important software development process involving the restructuring of a model to improve its internal qualities without changing its external behavior. In this paper, we propose a new approach of model refactoring based on the combined use of UML, B and CSP. UML models are described by class diagrams, OCL constraints, and state machine diagrams. We detail a refactoring pattern that allows for the introduction of an association relationship between two existing classes. We illustrate our proposal by giving a case study involving the SAAT (Software Architecture Analysis Tool) system.",1,1,1,1,2,4
74,Cloning and Expanding Graph Transformation Rules for Refactoring,"object-oriented programming, refactoring, graph transformation, variables","Refactoring is a software engineering technique that aims at enhancing the structure of object-oriented software while preserving its behavior. Several authors have studied how graph transformation can be used to specify refactoring, because such specifications are more precise and can thus, in principle, easier be verified to preserve a program's behavior. It has turned out that ``standard'' ways of graph transformation do not suffice to define refactoring: their expressive power must be increased if they shall be useful in this application area. Two mechanisms have been proposed so far: one for cloning, and one for expanding nodes by graphs. However, the mechanisms and notations needed are rather complex. In this paper we provide, in the context of double pushout graph transformation, a more elegant and intuitive description. It is based on a notion of rule instantiation, where the instantiation transforms rule schemes into rule instances by cloning and expansion. The power of the technique is demonstrated by an application to two well-known refactoring operations.",0,"Cloning and Expanding Graph Transformation Rules for Refactoring. Refactoring is a software engineering technique that aims at enhancing the structure of object-oriented software while preserving its behavior. Several authors have studied how graph transformation can be used to specify refactoring, because such specifications are more precise and can thus, in principle, easier be verified to preserve a program's behavior. It has turned out that ``standard'' ways of graph transformation do not suffice to define refactoring: their expressive power must be increased if they shall be useful in this application area. Two mechanisms have been proposed so far: one for cloning, and one for expanding nodes by graphs. However, the mechanisms and notations needed are rather complex. In this paper we provide, in the context of double pushout graph transformation, a more elegant and intuitive description. It is based on a notion of rule instantiation, where the instantiation transforms rule schemes into rule instances by cloning and expansion. The power of the technique is demonstrated by an application to two well-known refactoring operations.",1,1,0,0,3,4
75,Repetitive model refactoring strategy for the design space exploration of intensive signal processing applications,"Specification language, Parallelism, Optimizations, High-level code transformations, Design space exploration, Refactoring strategy","The efficient design of computation intensive multidimensional signal processing applications requires dealing with three kinds of constraints: those implied by the data dependencies, the non-functional requirements (real-time, power consumption) and resources availability of the execution platform. Modeling and Analysis of Real-time and Embedded systems (MARTE) UML profile through its repetitive structure modeling (RSM) package is well suited to model the inherent parallelism within these applications, a compact representation of parallel execution platforms and the distributive mapping of one on another. The execution of such a specification respects the whole set of constraints defined upon, while the quality of the scheduling is directly linked to the quality of the mapping of the multidimensional structures (data arrays or parallel loop nests) into time and space. We propose here a strategy to use a refactoring tool dedicated to this kind of application that allows to find good trade-offs in the usage of storage and computation resources and in parallelism (both task and data parallelism) exploitation. This strategy is illustrated on an industrial radar application.",0,"Repetitive model refactoring strategy for the design space exploration of intensive signal processing applications. The efficient design of computation intensive multidimensional signal processing applications requires dealing with three kinds of constraints: those implied by the data dependencies, the non-functional requirements (real-time, power consumption) and resources availability of the execution platform. Modeling and Analysis of Real-time and Embedded systems (MARTE) UML profile through its repetitive structure modeling (RSM) package is well suited to model the inherent parallelism within these applications, a compact representation of parallel execution platforms and the distributive mapping of one on another. The execution of such a specification respects the whole set of constraints defined upon, while the quality of the scheduling is directly linked to the quality of the mapping of the multidimensional structures (data arrays or parallel loop nests) into time and space. We propose here a strategy to use a refactoring tool dedicated to this kind of application that allows to find good trade-offs in the usage of storage and computation resources and in parallelism (both task and data parallelism) exploitation. This strategy is illustrated on an industrial radar application.",0,2,0,0,2,1
76,Exception handling refactorings: Directed by goals and driven by bug fixing,"Refactoring, Java exception handling, Object-oriented design","Exception handling design can improve robustness, which is an important quality attribute of software. However, exception handling design remains one of the less understood and considered parts in software development. In addition, like most software design problems, even if developers are requested to design with exception handling beforehand, it is very difficult to get the right design at the first shot. Therefore, improving exception handling design after software is constructed is necessary. This paper applies refactoring to incrementally improve exception handling design. We first establish four exception handling goals to stage the refactoring actions. Next, we introduce exception handling smells that hinder the achievement of the goals and propose exception handling refactorings to eliminate the smells. We suggest exception handling refactoring is best driven by bug fixing because it provides measurable quality improvement results that explicitly reveal the benefits of refactoring. We conduct a case study with the proposed refactorings on a real world banking application and provide a cost-effectiveness analysis. The result shows that our approach can effectively improve exception handling design, enhance software robustness, and save maintenance cost. Our approach simplifies the process of applying big exception handling refactoring by dividing the process into clearly defined intermediate milestones that are easily exercised and verified. The approach can be applied in general software development and in legacy system maintenance.",0,"Exception handling refactorings: Directed by goals and driven by bug fixing. Exception handling design can improve robustness, which is an important quality attribute of software. However, exception handling design remains one of the less understood and considered parts in software development. In addition, like most software design problems, even if developers are requested to design with exception handling beforehand, it is very difficult to get the right design at the first shot. Therefore, improving exception handling design after software is constructed is necessary. This paper applies refactoring to incrementally improve exception handling design. We first establish four exception handling goals to stage the refactoring actions. Next, we introduce exception handling smells that hinder the achievement of the goals and propose exception handling refactorings to eliminate the smells. We suggest exception handling refactoring is best driven by bug fixing because it provides measurable quality improvement results that explicitly reveal the benefits of refactoring. We conduct a case study with the proposed refactorings on a real world banking application and provide a cost-effectiveness analysis. The result shows that our approach can effectively improve exception handling design, enhance software robustness, and save maintenance cost. Our approach simplifies the process of applying big exception handling refactoring by dividing the process into clearly defined intermediate milestones that are easily exercised and verified. The approach can be applied in general software development and in legacy system maintenance.",0,2,1,1,3,2
77,Chapter 3 - Refactoring Software Architectures,"Software architecture, Refactoring, Reengineering, Architecture assessment, Quality, Patterns, Change, Evolution, Design erosion, Architecture drift","This chapter describes how to systematically prevent software architecture erosion by applying refactoring techniques. Software architecture modifications are common rather than the exception in software development. Modifications come in different flavors, such as redefining or adding requirements, changing infrastructure and technology, or causing changes by bugs and incorrect decisions. But no matter where these changes originate, they need special attention from software architects. Otherwise, if software architects merely focus on adding new features---(changes or extensions that by themselves might not be adequate), design erosion will be the final result. In a systematic approach, software architects evaluate the existing software design before adding new artifacts or changing existing ones. Whenever they identify architecture problems, they immediately resolve architectural issues, thus assuring high quality and sustainability. Software architecture refactoring enables such iterative architecture improvement. It consists of indentifying problems, applying the right refactorings, and testing the results. Architecture refactoring is often combined with code refactoring activities to add the best value. Refactoring patterns offer proven solutions for recurring architectural problems, hence providing a toolset to software engineers.",0,"Chapter 3 - Refactoring Software Architectures. This chapter describes how to systematically prevent software architecture erosion by applying refactoring techniques. Software architecture modifications are common rather than the exception in software development. Modifications come in different flavors, such as redefining or adding requirements, changing infrastructure and technology, or causing changes by bugs and incorrect decisions. But no matter where these changes originate, they need special attention from software architects. Otherwise, if software architects merely focus on adding new features---(changes or extensions that by themselves might not be adequate), design erosion will be the final result. In a systematic approach, software architects evaluate the existing software design before adding new artifacts or changing existing ones. Whenever they identify architecture problems, they immediately resolve architectural issues, thus assuring high quality and sustainability. Software architecture refactoring enables such iterative architecture improvement. It consists of indentifying problems, applying the right refactorings, and testing the results. Architecture refactoring is often combined with code refactoring activities to add the best value. Refactoring patterns offer proven solutions for recurring architectural problems, hence providing a toolset to software engineers.",0,3,2,1,3,2
78,Chapter 1 - Reengineering Patterns,,"Publisher Summary
This chapter focuses on re-engineering patterns. Re-engineering is the examination and alteration of a subject system to reconstitute it in a new form and the subsequent implementation of the new form. Re-engineering is concerned with restructuring a system, generally to fix some real or perceived problems, but more specifically, in preparation for further development and extension. The process of re-engineering is, like any other process, one in which many standard techniques have emerged, each of which resolves various forces and may entail many trade-offs. Patterns, as a way of communicating best practice, are particularly well suited for presenting and discussing these techniques. Re-engineering patterns codify and record knowledge about modifying legacy software; they help in diagnosing problems and identifying weaknesses that may hinder further development of the system, and they aid in finding solutions that are more appropriate to the new requirements. The re-engineering patterns are stable units of expertise that can be consulted in any re-engineering effort; they describe a process without proposing a complete methodology, and they suggest appropriate tools without ``selling'' a specific one. Many of the reverse engineering and re-engineering patterns have some superficial resemblance to design patterns, in the sense that they have something to do with the design of software. Re-engineering patterns entail more than code refactorings. A re-engineering pattern may describe a process that starts with the detection of the symptoms and ends with the refactoring of the code to arrive at the new solution. Refactoring is only the last stage of this process and addresses the technical issue of, automatically or semi automatically, modifying the code to implement the new solution.",0,"Chapter 1 - Reengineering Patterns. Publisher Summary
This chapter focuses on re-engineering patterns. Re-engineering is the examination and alteration of a subject system to reconstitute it in a new form and the subsequent implementation of the new form. Re-engineering is concerned with restructuring a system, generally to fix some real or perceived problems, but more specifically, in preparation for further development and extension. The process of re-engineering is, like any other process, one in which many standard techniques have emerged, each of which resolves various forces and may entail many trade-offs. Patterns, as a way of communicating best practice, are particularly well suited for presenting and discussing these techniques. Re-engineering patterns codify and record knowledge about modifying legacy software; they help in diagnosing problems and identifying weaknesses that may hinder further development of the system, and they aid in finding solutions that are more appropriate to the new requirements. The re-engineering patterns are stable units of expertise that can be consulted in any re-engineering effort; they describe a process without proposing a complete methodology, and they suggest appropriate tools without ``selling'' a specific one. Many of the reverse engineering and re-engineering patterns have some superficial resemblance to design patterns, in the sense that they have something to do with the design of software. Re-engineering patterns entail more than code refactorings. A re-engineering pattern may describe a process that starts with the detection of the symptoms and ends with the refactoring of the code to arrive at the new solution. Refactoring is only the last stage of this process and addresses the technical issue of, automatically or semi automatically, modifying the code to implement the new solution.",0,2,0,0,0,2
79,Constructing models for predicting extract subclass refactoring opportunities using object-oriented quality metrics,"Object-oriented design, Extract subclass refactoring, Class quality, Class cohesion, Class coupling, Logistic regression analysis","Context
Refactoring is a maintenance task that refers to the process of restructuring software source code to enhance its quality without affecting its external behavior. Inspecting and analyzing the source code of the system under consideration to identify the classes in need of extract subclass refactoring (ESR) is a time consuming and costly process.
Objective
This paper explores the abilities of several quality metrics considered individually and in combination to predict the classes in need of ESR.
Method
For a given a class, this paper empirically investigates, using univariate logistic regression analysis, the abilities of 25 existing size, cohesion, and coupling metrics to predict whether the class is in need of restructuring by extracting a subclass from it. In addition, models of combined metrics based on multivariate logistic regression analysis were constructed and validated to predict the classes in need of ESR, and the best model is justifiably recommended. We explored the statistical relations between the values of the selected metrics and the decisions of the developers of six open source Java systems with respect to whether the classes require ESR.
Results
The results indicate that there was a strong statistical relation between some of the quality metrics and the decision of whether ESR activity was required. From a statistical point of view, the recommended model of metrics has practical thresholds that lead to an outstanding classification of the classes into those that require ESR and those that do not.
Conclusion
The proposed model can be applied to automatically predict the classes in need of ESR and present them as suggestions to developers working to enhance the system during the maintenance phase. In addition, the model is capable of ranking the classes of the system under consideration according to their degree of need of ESR.",0,"Constructing models for predicting extract subclass refactoring opportunities using object-oriented quality metrics. Context
Refactoring is a maintenance task that refers to the process of restructuring software source code to enhance its quality without affecting its external behavior. Inspecting and analyzing the source code of the system under consideration to identify the classes in need of extract subclass refactoring (ESR) is a time consuming and costly process.
Objective
This paper explores the abilities of several quality metrics considered individually and in combination to predict the classes in need of ESR.
Method
For a given a class, this paper empirically investigates, using univariate logistic regression analysis, the abilities of 25 existing size, cohesion, and coupling metrics to predict whether the class is in need of restructuring by extracting a subclass from it. In addition, models of combined metrics based on multivariate logistic regression analysis were constructed and validated to predict the classes in need of ESR, and the best model is justifiably recommended. We explored the statistical relations between the values of the selected metrics and the decisions of the developers of six open source Java systems with respect to whether the classes require ESR.
Results
The results indicate that there was a strong statistical relation between some of the quality metrics and the decision of whether ESR activity was required. From a statistical point of view, the recommended model of metrics has practical thresholds that lead to an outstanding classification of the classes into those that require ESR and those that do not.
Conclusion
The proposed model can be applied to automatically predict the classes in need of ESR and present them as suggestions to developers working to enhance the system during the maintenance phase. In addition, the model is capable of ranking the classes of the system under consideration according to their degree of need of ESR.",0,2,1,2,2,0
80,Using CafeOBJ to Mechanise Refactoring Proofs and Application,"Rewriting Systems, Refactorings, CafeOBJ","In this paper we show how rewriting systems, in particular CafeOBJ, can be used to automatically prove refactoring rules. In addition, a small case study that illustrates the application of a refactoring rule in an arbitrary program is also developed. Our approach is based on a sequential object-oriented language of refinement (rool) similar to Java. We have implemented the rool grammar in CafeOBJ, as well as the laws that define its semantics. Each refactoring rule is derived by the application of these laws, in a constructive way. The refactorings are also implemented in CafeOBJ, allowing the reduction of an arbitrary program.",0,"Using CafeOBJ to Mechanise Refactoring Proofs and Application. In this paper we show how rewriting systems, in particular CafeOBJ, can be used to automatically prove refactoring rules. In addition, a small case study that illustrates the application of a refactoring rule in an arbitrary program is also developed. Our approach is based on a sequential object-oriented language of refinement (rool) similar to Java. We have implemented the rool grammar in CafeOBJ, as well as the laws that define its semantics. Each refactoring rule is derived by the application of these laws, in a constructive way. The refactorings are also implemented in CafeOBJ, allowing the reduction of an arbitrary program.",1,1,2,1,3,4
81,Chapter 8 - Repaying Technical Debt in Practice,"Best practices, Buy-in for refactoring, IMPACT refactoring process model, Managing technical debt, Repaying debt, Role of people, Tools to repay technical debt, Training","This final chapter offers practical guidance and tips on how to approach refactoring in order to manage technical debt in real-world projects. The chapter focuses the discussion along three important dimensions for repaying technical debt: tools, processes, and people. It first presents an overview of different kinds of tools that can help a developer in detecting, analyzing, and addressing smells. Following that, the chapter outlines a few best practices that can be adopted in a real-world setting to ensure backing from the management for refactoring. It also presents a refactoring process model named ``IMPACT'' that covers the essential steps of a refactoring exercise. Finally, this chapter outlines a few effective ways to build a competent and committed team of software engineers.",0,"Chapter 8 - Repaying Technical Debt in Practice. This final chapter offers practical guidance and tips on how to approach refactoring in order to manage technical debt in real-world projects. The chapter focuses the discussion along three important dimensions for repaying technical debt: tools, processes, and people. It first presents an overview of different kinds of tools that can help a developer in detecting, analyzing, and addressing smells. Following that, the chapter outlines a few best practices that can be adopted in a real-world setting to ensure backing from the management for refactoring. It also presents a refactoring process model named ``IMPACT'' that covers the essential steps of a refactoring exercise. Finally, this chapter outlines a few effective ways to build a competent and committed team of software engineers.",0,3,0,0,0,0
82,Identification of refactoring opportunities introducing polymorphism,"Refactoring, Polymorphism, design pattern, Object-oriented design","Polymorphism is one of the most important features offered by object-oriented programming languages, since it allows to extend/modify the behavior of a class without altering its source code, in accordance to the Open/Closed Principle. However, there is a lack of methods and tools for the identification of places in the code of an existing system that could benefit from the employment of polymorphism. In this paper we propose a technique that extracts refactoring suggestions introducing polymorphism. The approach ensures the behavior preservation of the code and the applicability of the refactoring suggestions based on the examination of a set of preconditions.",1,"Identification of refactoring opportunities introducing polymorphism. Polymorphism is one of the most important features offered by object-oriented programming languages, since it allows to extend/modify the behavior of a class without altering its source code, in accordance to the Open/Closed Principle. However, there is a lack of methods and tools for the identification of places in the code of an existing system that could benefit from the employment of polymorphism. In this paper we propose a technique that extracts refactoring suggestions introducing polymorphism. The approach ensures the behavior preservation of the code and the applicability of the refactoring suggestions based on the examination of a set of preconditions.",0,1,2,1,3,0
83,Improving multi-objective code-smells correction using development history,"Search-based software engineering, Refactoring, Code-smells","One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.",1,"Improving multi-objective code-smells correction using development history. One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.",0,2,1,1,3,2
84,Towards multilingual programming environments,"Programming environments, Language interoperability, Metaprogramming","Software projects consist of different kinds of artifacts: build files, configuration files, markup files, source code in different software languages, and so on. At the same time, however, most integrated development environments (IDEs) are focused on a single (programming) language. Even if a programming environment supports multiple languages (e.g., Eclipse), IDE features such as cross-referencing, refactoring, or debugging, do not often cross language boundaries. What would it mean for programming environment to be truly multilingual? In this short paper we sketch a vision of a system that integrates IDE support across language boundaries. We propose to build this system on a foundation of unified source code models and metaprogramming. Nevertheless, a number of important and hard research questions still need to be addressed.",0,"Towards multilingual programming environments. Software projects consist of different kinds of artifacts: build files, configuration files, markup files, source code in different software languages, and so on. At the same time, however, most integrated development environments (IDEs) are focused on a single (programming) language. Even if a programming environment supports multiple languages (e.g., Eclipse), IDE features such as cross-referencing, refactoring, or debugging, do not often cross language boundaries. What would it mean for programming environment to be truly multilingual? In this short paper we sketch a vision of a system that integrates IDE support across language boundaries. We propose to build this system on a foundation of unified source code models and metaprogramming. Nevertheless, a number of important and hard research questions still need to be addressed.",0,3,2,0,1,1
85,Dynamic profiling-based approach to identifying cost-effective refactorings,"Cost-effective refactoring, Refactoring identification, Dynamic profiling, Dynamic method call, Maintainability improvement, Refactoring cost","Context
Object-oriented software undergoes continuous changes---changes often made without consideration of the software's overall structure and design rationale. Hence, over time, the design quality of the software degrades causing software aging or software decay. Refactoring offers a means of restructuring software design to improve maintainability. In practice, efforts to invest in refactoring are restricted; therefore, the problem calls for a method for identifying cost-effective refactorings that efficiently improve maintainability. Cost-effectiveness of applied refactorings can be explained as maintainability improvement over invested refactoring effort (cost). For the system, the more cost-effective refactorings are applied, the greater maintainability would be improved. There have been several studies of supporting the arguments that changes are more prone to occur in the pieces of codes more frequently utilized by users; hence, applying refactorings in these parts would fast improve maintainability of software. For this reason, dynamic information is needed for identifying the entities involved in given scenarios/functions of a system, and within these entities, refactoring candidates need to be extracted.
Objective
This paper provides an automated approach to identifying cost-effective refactorings using dynamic information in object-oriented software.
Method
To perform cost-effective refactoring, refactoring candidates are extracted in a way that reduces dependencies; these are referred to as the dynamic information. The dynamic profiling technique is used to obtain the dependencies of entities based on dynamic method calls. Based on those dynamic dependencies, refactoring-candidate extraction rules are defined, and a maintainability evaluation function is established. Then, refactoring candidates are extracted and assessed using the defined rules and the evaluation function, respectively. The best refactoring (i.e., that which most improves maintainability) is selected from among refactoring candidates, then refactoring candidate extraction and assessment are re-performed to select the next refactoring, and the refactoring identification process is iterated until no more refactoring candidates for improving maintainability are found.
Results
We evaluate our proposed approach in three open-source projects. The first results show that dynamic information is helpful in identifying cost-effective refactorings that fast improve maintainability; and, considering dynamic information in addition to static information provides even more opportunities to identify cost-effective refactorings. The second results show that dynamic information is helpful in extracting refactoring candidates in the classes where real changes had occurred; in addition, the results also offer the promising support for the contention that using dynamic information helps to extracting refactoring candidates from highly-ranked frequently changed classes.
Conclusion
Our proposed approach helps to identify cost-effective refactorings and supports an automated refactoring identification process.",0,"Dynamic profiling-based approach to identifying cost-effective refactorings. Context
Object-oriented software undergoes continuous changes---changes often made without consideration of the software's overall structure and design rationale. Hence, over time, the design quality of the software degrades causing software aging or software decay. Refactoring offers a means of restructuring software design to improve maintainability. In practice, efforts to invest in refactoring are restricted; therefore, the problem calls for a method for identifying cost-effective refactorings that efficiently improve maintainability. Cost-effectiveness of applied refactorings can be explained as maintainability improvement over invested refactoring effort (cost). For the system, the more cost-effective refactorings are applied, the greater maintainability would be improved. There have been several studies of supporting the arguments that changes are more prone to occur in the pieces of codes more frequently utilized by users; hence, applying refactorings in these parts would fast improve maintainability of software. For this reason, dynamic information is needed for identifying the entities involved in given scenarios/functions of a system, and within these entities, refactoring candidates need to be extracted.
Objective
This paper provides an automated approach to identifying cost-effective refactorings using dynamic information in object-oriented software.
Method
To perform cost-effective refactoring, refactoring candidates are extracted in a way that reduces dependencies; these are referred to as the dynamic information. The dynamic profiling technique is used to obtain the dependencies of entities based on dynamic method calls. Based on those dynamic dependencies, refactoring-candidate extraction rules are defined, and a maintainability evaluation function is established. Then, refactoring candidates are extracted and assessed using the defined rules and the evaluation function, respectively. The best refactoring (i.e., that which most improves maintainability) is selected from among refactoring candidates, then refactoring candidate extraction and assessment are re-performed to select the next refactoring, and the refactoring identification process is iterated until no more refactoring candidates for improving maintainability are found.
Results
We evaluate our proposed approach in three open-source projects. The first results show that dynamic information is helpful in identifying cost-effective refactorings that fast improve maintainability; and, considering dynamic information in addition to static information provides even more opportunities to identify cost-effective refactorings. The second results show that dynamic information is helpful in extracting refactoring candidates in the classes where real changes had occurred; in addition, the results also offer the promising support for the contention that using dynamic information helps to extracting refactoring candidates from highly-ranked frequently changed classes.
Conclusion
Our proposed approach helps to identify cost-effective refactorings and supports an automated refactoring identification process.",0,0,2,1,3,2
86,An empirical study of refactoring decisions in embedded software and systems,"System refactoring, system evolution, system architecture, software architecture, decision-making, empirical study","This paper describes an empirical study of decision-making when changing the architecture in embedded systems. A refactoring of the system architecture often gives effects on both system properties and functions in the company organization, and there is a lack of efficient analysis methods for decision support in the system architecture process. This study investigates the information needed to make a decision about a system refactoring. Scenario-based interviews have been conducted with managers and system architects from companies developing embedded systems. The results show that the companies investigate similar issues regardless of their industry sector. The most wanted information prior to a decision is also presented.",0,"An empirical study of refactoring decisions in embedded software and systems. This paper describes an empirical study of decision-making when changing the architecture in embedded systems. A refactoring of the system architecture often gives effects on both system properties and functions in the company organization, and there is a lack of efficient analysis methods for decision support in the system architecture process. This study investigates the information needed to make a decision about a system refactoring. Scenario-based interviews have been conducted with managers and system architects from companies developing embedded systems. The results show that the companies investigate similar issues regardless of their industry sector. The most wanted information prior to a decision is also presented.",0,3,2,0,0,3
87,Refactoring of Crosscutting Concerns with Metaphor-Based Heuristics,"Refactoring, Aspect-oriented programming, Crosscutting concerns, Metaphor-based classification, Design heuristics","It has been advocated that Aspect-Oriented Programming (AOP) is an effective technique to improve software maintainability through explicit support for modularising crosscutting concerns. However, in order to take the advantages of AOP, there is a need for supporting the systematic refactoring of crosscutting concerns to aspects. Existing techniques for aspect-oriented refactoring are too fine-grained and do not take the concern structure into consideration. This paper presents two categories towards a metaphor-based classification of crosscutting concerns driven by their manifested shapes through a system's modular structure. The proposed categories provide an intuitive and fundamental terminology for detecting concern-oriented design flaws and identifying refactorings in terms of recurring crosscutting structures. On top of this classification, we define a suite of metaphor-based refactorings to guide the ``aspectisation'' of each concern category. We evaluate our technique by classifying concerns of 23 design patterns and by proposing refactorings to aspectise them according to observations made in previous empirical studies. Based on our experience, we also determine a catalogue of potential additional categories and heuristics for refactoring of crosscutting concerns.",0,"Refactoring of Crosscutting Concerns with Metaphor-Based Heuristics. It has been advocated that Aspect-Oriented Programming (AOP) is an effective technique to improve software maintainability through explicit support for modularising crosscutting concerns. However, in order to take the advantages of AOP, there is a need for supporting the systematic refactoring of crosscutting concerns to aspects. Existing techniques for aspect-oriented refactoring are too fine-grained and do not take the concern structure into consideration. This paper presents two categories towards a metaphor-based classification of crosscutting concerns driven by their manifested shapes through a system's modular structure. The proposed categories provide an intuitive and fundamental terminology for detecting concern-oriented design flaws and identifying refactorings in terms of recurring crosscutting structures. On top of this classification, we define a suite of metaphor-based refactorings to guide the ``aspectisation'' of each concern category. We evaluate our technique by classifying concerns of 23 design patterns and by proposing refactorings to aspectise them according to observations made in previous empirical studies. Based on our experience, we also determine a catalogue of potential additional categories and heuristics for refactoring of crosscutting concerns.",0,2,0,0,0,2
88,Empirical investigation of refactoring effect on software quality,"Software metrics, Refactoring, Quality improvement, Empirical study","Developers and designers always strive for quality software. Quality software tends to be robust, reliable and easy to maintain, and thus reduces the cost of software development and maintenance. Several methods have been applied to improve software quality. Refactoring is one of those methods. The goal of this paper is to validate/invalidate the claims that refactoring improves software quality. We focused this study on different external quality attributes, which are adaptability, maintainability, understandability, reusability, and testability. We found that refactoring does not necessarily improve these quality attributes.",0,"Empirical investigation of refactoring effect on software quality. Developers and designers always strive for quality software. Quality software tends to be robust, reliable and easy to maintain, and thus reduces the cost of software development and maintenance. Several methods have been applied to improve software quality. Refactoring is one of those methods. The goal of this paper is to validate/invalidate the claims that refactoring improves software quality. We focused this study on different external quality attributes, which are adaptability, maintainability, understandability, reusability, and testability. We found that refactoring does not necessarily improve these quality attributes.",0,3,1,1,3,2
89,Identification of extract method refactoring opportunities for the decomposition of methods,"Extract Method refactoring, Program slicing, Module decomposition","The extraction of a code fragment into a separate method is one of the most widely performed refactoring activities, since it allows the decomposition of large and complex methods and can be used in combination with other code transformations for fixing a variety of design problems. Despite the significance of Extract Method refactoring towards code quality improvement, there is limited support for the identification of code fragments with distinct functionality that could be extracted into new methods. The goal of our approach is to automatically identify Extract Method refactoring opportunities which are related with the complete computation of a given variable (complete computation slice) and the statements affecting the state of a given object (object state slice). Moreover, a set of rules regarding the preservation of existing dependences is proposed that exclude refactoring opportunities corresponding to slices whose extraction could possibly cause a change in program behavior. The proposed approach has been evaluated regarding its ability to capture slices of code implementing a distinct functionality, its ability to resolve existing design flaws, its impact on the cohesion of the decomposed and extracted methods, and its ability to preserve program behavior. Moreover, precision and recall have been computed employing the refactoring opportunities found by independent evaluators in software that they developed as a golden set.",1,"Identification of extract method refactoring opportunities for the decomposition of methods. The extraction of a code fragment into a separate method is one of the most widely performed refactoring activities, since it allows the decomposition of large and complex methods and can be used in combination with other code transformations for fixing a variety of design problems. Despite the significance of Extract Method refactoring towards code quality improvement, there is limited support for the identification of code fragments with distinct functionality that could be extracted into new methods. The goal of our approach is to automatically identify Extract Method refactoring opportunities which are related with the complete computation of a given variable (complete computation slice) and the statements affecting the state of a given object (object state slice). Moreover, a set of rules regarding the preservation of existing dependences is proposed that exclude refactoring opportunities corresponding to slices whose extraction could possibly cause a change in program behavior. The proposed approach has been evaluated regarding its ability to capture slices of code implementing a distinct functionality, its ability to resolve existing design flaws, its impact on the cohesion of the decomposed and extracted methods, and its ability to preserve program behavior. Moreover, precision and recall have been computed employing the refactoring opportunities found by independent evaluators in software that they developed as a golden set.",0,2,2,1,3,0
90,Transformations everywhere,,"This special issue is devoted to ``program transformation'' in the sense of tool-supported adaptation of software systems. Software engineering and software re-engineering rely on such transformations, which are automated in, for example, tools for refactoring, migration, program specialisation, compiler optimisation, database re-engineering, software configuration, business-rule extraction, aspect weaving, aspect mining, architectural modifications, and model-driven approaches. This special issue bundles ten state-of-the-art contributions, while covering the broad area of program transformation in a complementary, almost survey-like manner. Three papers relate to refactoring---to the composition problem, to reasoning about correctness, and to the details of challenging refactoring samples. Two papers survey successful transformation systems, namely the Tempo system for program specialisation, and the FermaT system for software migration. One paper develops concepts for run-time system transformations.. Finally, four papers communicate idioms or concepts for transformation systems: higher-order and dynamic traversals, the use of flow analysis for driving transformations, validated compiler transformations, and the cause--effect patterns in partial evaluation. This introduction to the special issue briefly describes the articles included, and connects them to general concerns in research on program transformation. In addition, a list of research challenges is compiled, which will perhaps be useful in the further exploration of the area of program transformation.",0,"Transformations everywhere. This special issue is devoted to ``program transformation'' in the sense of tool-supported adaptation of software systems. Software engineering and software re-engineering rely on such transformations, which are automated in, for example, tools for refactoring, migration, program specialisation, compiler optimisation, database re-engineering, software configuration, business-rule extraction, aspect weaving, aspect mining, architectural modifications, and model-driven approaches. This special issue bundles ten state-of-the-art contributions, while covering the broad area of program transformation in a complementary, almost survey-like manner. Three papers relate to refactoring---to the composition problem, to reasoning about correctness, and to the details of challenging refactoring samples. Two papers survey successful transformation systems, namely the Tempo system for program specialisation, and the FermaT system for software migration. One paper develops concepts for run-time system transformations.. Finally, four papers communicate idioms or concepts for transformation systems: higher-order and dynamic traversals, the use of flow analysis for driving transformations, validated compiler transformations, and the cause--effect patterns in partial evaluation. This introduction to the special issue briefly describes the articles included, and connects them to general concerns in research on program transformation. In addition, a list of research challenges is compiled, which will perhaps be useful in the further exploration of the area of program transformation.",1,3,2,0,0,2
91,SPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clones,"Near-miss clones, Amorphous procedure extraction, Refactoring","Cloned code, also known as duplicated code, is among the bad ``code smells''. Procedure extraction can be used to remove clones and to make a software system more maintainable. While the existing procedure extraction techniques can handle automatic extraction of exact clones effectively, they fail to do so for near-miss clones, which are the code fragments that are similar but not the same. To address this gap, we developed SPAPE, a novel semantic-preserving amorphous procedure extraction method to extract near-miss clones. SPAPE relaxes the constraint of having the same syntax and uses the structural semantic information. We evaluated the performance, effectiveness, and benefits of SPAPE. Our results show that SPAPE can extract more near-miss clones than the best applicable method for ten open-source-software products in an efficient and effective fashion. We conclude that SPAPE can be a useful contribution to the toolsets of software managers and developers, and it can help them improve code structure and reduce software maintenance and overall project costs.",0,"SPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clones. Cloned code, also known as duplicated code, is among the bad ``code smells''. Procedure extraction can be used to remove clones and to make a software system more maintainable. While the existing procedure extraction techniques can handle automatic extraction of exact clones effectively, they fail to do so for near-miss clones, which are the code fragments that are similar but not the same. To address this gap, we developed SPAPE, a novel semantic-preserving amorphous procedure extraction method to extract near-miss clones. SPAPE relaxes the constraint of having the same syntax and uses the structural semantic information. We evaluated the performance, effectiveness, and benefits of SPAPE. Our results show that SPAPE can extract more near-miss clones than the best applicable method for ten open-source-software products in an efficient and effective fashion. We conclude that SPAPE can be a useful contribution to the toolsets of software managers and developers, and it can help them improve code structure and reduce software maintenance and overall project costs.",0,2,2,0,2,0
92,Chapter 5 - A Practical Guide to Analyzing IDE Usage Data,"Usage Data, Developer Activity, Refactoring, Productivity","Integrated development environments such as Eclipse and Visual Studio provide tools and capabilities to perform tasks such as navigating among classes and methods, continuous compilation, code refactoring, automated testing, and integrated debugging, all designed to increase productivity. Instrumenting the integrated development environment to collect usage data provides a more fine-grained understanding of developers' work than was previously possible. Usage data supports analysis of how developers spend their time, what activities might benefit from greater tool support, where developers have difficulty comprehending code, and whether they are following specific practices such as test-driven development. With usage data, we expect to uncover more nuggets of how developers create mental models, how they investigate code, how they perform mini trial-and-error experiments, and what might drive productivity improvements for everyone.",0,"Chapter 5 - A Practical Guide to Analyzing IDE Usage Data. Integrated development environments such as Eclipse and Visual Studio provide tools and capabilities to perform tasks such as navigating among classes and methods, continuous compilation, code refactoring, automated testing, and integrated debugging, all designed to increase productivity. Instrumenting the integrated development environment to collect usage data provides a more fine-grained understanding of developers' work than was previously possible. Usage data supports analysis of how developers spend their time, what activities might benefit from greater tool support, where developers have difficulty comprehending code, and whether they are following specific practices such as test-driven development. With usage data, we expect to uncover more nuggets of how developers create mental models, how they investigate code, how they perform mini trial-and-error experiments, and what might drive productivity improvements for everyone.",0,3,0,0,2,0
93,Transforming Application Compositions with XSLTs,,"Architectural Description Languages (ADLs) allow developers to describe the architecture of a software system but provide only limited support for their execution. In this paper, we present ACL/1, an XML based Application Composition Language, that describes the architectural view of a software system while at the same time enabling the execution of the software application. As we will show, our approach allows us to use XSLTs to transform the architectural view of the application into source code and it may be used to refactor the underlying software application.",0,"Transforming Application Compositions with XSLTs. Architectural Description Languages (ADLs) allow developers to describe the architecture of a software system but provide only limited support for their execution. In this paper, we present ACL/1, an XML based Application Composition Language, that describes the architectural view of a software system while at the same time enabling the execution of the software application. As we will show, our approach allows us to use XSLTs to transform the architectural view of the application into source code and it may be used to refactor the underlying software application.",1,1,0,0,1,1
94,Source-code queries with graph databases---with application to programming language usage and evolution,"Programming language evolution, Source-code queries and DSLs, Graph databases","Program querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA. The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held---with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses. We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or data-flow levels. We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail.",0,"Source-code queries with graph databases---with application to programming language usage and evolution. Program querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA. The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held---with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses. We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or data-flow levels. We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail.",1,2,0,0,1,0
95,An experimental investigation on the innate relationship between quality and refactoring,"Refactoring, Code smells, Empirical study","Previous studies have investigated the reasons behind refactoring operations performed by developers, and proposed methods and tools to recommend refactorings based on quality metric profiles, or on the presence of poor design and implementation choices, i.e., code smells. Nevertheless, the existing literature lacks observations about the relations between metrics/code smells and refactoring activities performed by developers. In other words, the characteristics of code components increasing/decreasing their chances of being object of refactoring operations are still unknown. This paper aims at bridging this gap. Specifically, we mined the evolution history of three Java open source projects to investigate whether refactoring activities occur on code components for which certain indicators---such as quality metrics or the presence of smells as detected by tools---suggest there might be need for refactoring operations. Results indicate that, more often than not, quality metrics do not show a clear relationship with refactoring. In other words, refactoring operations are generally focused on code components for which quality metrics do not suggest there might be need for refactoring operations. Finally, 42% of refactoring operations are performed on code entities affected by code smells. However, only 7% of the performed operations actually remove the code smells from the affected class.",0,"An experimental investigation on the innate relationship between quality and refactoring. Previous studies have investigated the reasons behind refactoring operations performed by developers, and proposed methods and tools to recommend refactorings based on quality metric profiles, or on the presence of poor design and implementation choices, i.e., code smells. Nevertheless, the existing literature lacks observations about the relations between metrics/code smells and refactoring activities performed by developers. In other words, the characteristics of code components increasing/decreasing their chances of being object of refactoring operations are still unknown. This paper aims at bridging this gap. Specifically, we mined the evolution history of three Java open source projects to investigate whether refactoring activities occur on code components for which certain indicators---such as quality metrics or the presence of smells as detected by tools---suggest there might be need for refactoring operations. Results indicate that, more often than not, quality metrics do not show a clear relationship with refactoring. In other words, refactoring operations are generally focused on code components for which quality metrics do not suggest there might be need for refactoring operations. Finally, 42% of refactoring operations are performed on code entities affected by code smells. However, only 7% of the performed operations actually remove the code smells from the affected class.",0,2,2,1,3,0
96,Improving conceptual data models through iterative development,"Database profiling, Evolvability, Object database, Agile development, Conceptual models, Model quality, Semantic verification","Agile methods promote iterative development with short cycles, where user feedback from the previous iteration is used to refactor and improve the current version. To facilitate agile development of information systems, this paper offers three contributions. First, we introduce the concept of evolvability as a model quality characteristic. Evolvability refers to the expected implications of future model refactorings, both in terms of complexity of the required database evolution algorithm and in terms of the expected volume of data to evolve. Second, we propose extending the agile development cycle by using database profiling information to suggest adaptations to the conceptual model to improve performance. For every software release, the database profiler identifies and analyses navigational access patterns, and proposes model optimisations based on data characteristics, access patterns and a cost--benefit model. Based on an experimental evaluation of the profiler we discuss why the quality of conceptual models can generally benefit from profiling and how performance measurements convey semantic information. Third, we discuss the flow of semantic information when developing and using information systems. Beyond these contributions, we also make a case for using object databases in agile development environments. However, most of the presented concepts are also applicable to other database paradigms.",0,"Improving conceptual data models through iterative development. Agile methods promote iterative development with short cycles, where user feedback from the previous iteration is used to refactor and improve the current version. To facilitate agile development of information systems, this paper offers three contributions. First, we introduce the concept of evolvability as a model quality characteristic. Evolvability refers to the expected implications of future model refactorings, both in terms of complexity of the required database evolution algorithm and in terms of the expected volume of data to evolve. Second, we propose extending the agile development cycle by using database profiling information to suggest adaptations to the conceptual model to improve performance. For every software release, the database profiler identifies and analyses navigational access patterns, and proposes model optimisations based on data characteristics, access patterns and a cost--benefit model. Based on an experimental evaluation of the profiler we discuss why the quality of conceptual models can generally benefit from profiling and how performance measurements convey semantic information. Third, we discuss the flow of semantic information when developing and using information systems. Beyond these contributions, we also make a case for using object databases in agile development environments. However, most of the presented concepts are also applicable to other database paradigms.",0,3,2,1,2,4
97,Identification and application of Extract Class refactorings in object-oriented systems,"Refactoring, Software reengineering, Object-oriented programming, Clustering","Refactoring is recognized as an essential practice in the context of evolutionary and agile software development. Recognizing the importance of the practice, modern IDEs provide some support for low-level refactorings. A notable exception in the list of supported refactorings is the ``Extract Class'' refactoring, which is conceived to simplify large, complex, unwieldy and less cohesive classes. In this work, we describe a method and a tool, implemented as an Eclipse plugin, designed to fulfill exactly this need. Our method involves three steps: (a) recognition of Extract Class opportunities, (b) ranking of the identified opportunities in terms of the improvement each one is anticipated to bring about to the system design, and (c) fully automated application of the refactoring chosen by the developer. The first step relies on an agglomerative clustering algorithm, which identifies cohesive sets of class members within the system classes. The second step relies on the Entity Placement metric as a measure of design quality. Through a set of experiments we have shown that the tool is able to identify and extract new classes that developers recognize as ``coherent concepts'' and improve the design quality of the underlying system.",0,"Identification and application of Extract Class refactorings in object-oriented systems. Refactoring is recognized as an essential practice in the context of evolutionary and agile software development. Recognizing the importance of the practice, modern IDEs provide some support for low-level refactorings. A notable exception in the list of supported refactorings is the ``Extract Class'' refactoring, which is conceived to simplify large, complex, unwieldy and less cohesive classes. In this work, we describe a method and a tool, implemented as an Eclipse plugin, designed to fulfill exactly this need. Our method involves three steps: (a) recognition of Extract Class opportunities, (b) ranking of the identified opportunities in terms of the improvement each one is anticipated to bring about to the system design, and (c) fully automated application of the refactoring chosen by the developer. The first step relies on an agglomerative clustering algorithm, which identifies cohesive sets of class members within the system classes. The second step relies on the Entity Placement metric as a measure of design quality. Through a set of experiments we have shown that the tool is able to identify and extract new classes that developers recognize as ``coherent concepts'' and improve the design quality of the underlying system.",0,3,2,1,3,4
98,Leveraging UML Profiles to Generate Plugins From Visual Model Transformations,"Refactoring, Model Transformation, SDM, JMI","Model transformation is a fundamental technology in the MDA. Therefore, model transformations should be treated as first class entities, that is, models. One could use the metamodel of SDM, a graph based object transformation language, as the metamodel of such transformation models. However, there are two problems associated with this. First, SDM has a non-standardized metamodel, meaning a specific tool (Fujaba) would be needed to write transformation specifications. Secondly, due to assumptions of the code generator, the transformations could only be deployed on the Fujaba tool itself. In this paper, we describe how these issues have been overcome through the development of a template based code generator that translates instances of a UML profile for SDM to complete model transformation code that complies to the JMI standard. We have validated this approach by specifying a simple visual refactoring in one UML tool and deploying the generated plugin on another UML tool.",0,"Leveraging UML Profiles to Generate Plugins From Visual Model Transformations. Model transformation is a fundamental technology in the MDA. Therefore, model transformations should be treated as first class entities, that is, models. One could use the metamodel of SDM, a graph based object transformation language, as the metamodel of such transformation models. However, there are two problems associated with this. First, SDM has a non-standardized metamodel, meaning a specific tool (Fujaba) would be needed to write transformation specifications. Secondly, due to assumptions of the code generator, the transformations could only be deployed on the Fujaba tool itself. In this paper, we describe how these issues have been overcome through the development of a template based code generator that translates instances of a UML profile for SDM to complete model transformation code that complies to the JMI standard. We have validated this approach by specifying a simple visual refactoring in one UML tool and deploying the generated plugin on another UML tool.",1,1,0,0,1,1
99,A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring,"Software product line, Evolution, Reengineering, Legacy system, Refactoring","Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.",0,"A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring. Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.",0,0,0,0,0,2
100,Refactoring Alloy Specifications,"Formal Methods, Model Transformations, Refactoring, Model Checking","This paper proposes modeling laws for Alloy, a formal object-oriented modeling language. These laws are important not only to define the axiomatic semantics of Alloy but also to guide and formalize popular software development practices. In particular, these laws can be used to formaly refactor specifications. As an example, we formally refactor a specification for Java types.",0,"Refactoring Alloy Specifications. This paper proposes modeling laws for Alloy, a formal object-oriented modeling language. These laws are important not only to define the axiomatic semantics of Alloy but also to guide and formalize popular software development practices. In particular, these laws can be used to formaly refactor specifications. As an example, we formally refactor a specification for Java types.",1,1,1,1,3,4
101,Accelerated application development: The ORNL Titan experience,"High performance computing, Accelerated computing, GPU graphics processing units, Science applications, Code refactoring, Software optimization","The use of computational accelerators such as NVIDIA GPUs and Intel Xeon Phi processors is now widespread in the high performance computing community, with many applications delivering impressive performance gains. However, programming these systems for high performance, performance portability and software maintainability has been a challenge. In this paper we discuss experiences porting applications to the Titan system. Titan, which began planning in 2009 and was deployed for general use in 2013, was the first multi-petaflop system based on accelerator hardware. To ready applications for accelerated computing, a preparedness effort was undertaken prior to delivery of Titan. In this paper we report experiences and lessons learned from this process and describe how users are currently making use of computational accelerators on Titan.",0,"Accelerated application development: The ORNL Titan experience. The use of computational accelerators such as NVIDIA GPUs and Intel Xeon Phi processors is now widespread in the high performance computing community, with many applications delivering impressive performance gains. However, programming these systems for high performance, performance portability and software maintainability has been a challenge. In this paper we discuss experiences porting applications to the Titan system. Titan, which began planning in 2009 and was deployed for general use in 2013, was the first multi-petaflop system based on accelerator hardware. To ready applications for accelerated computing, a preparedness effort was undertaken prior to delivery of Titan. In this paper we report experiences and lessons learned from this process and describe how users are currently making use of computational accelerators on Titan.",0,3,1,0,0,1
102,Successful extreme programming: Fidelity to the methodology or good teamworking?,"Software development, Extreme programming, Agile methods, Teamwork, Cooperation, Performance","Context
Developing a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team's success? Perhaps the more specific software engineering team factors, for example the agile development method's collective ownership and code management, are decisive.
Objective
To assess the contribution of agile methodology, agile-specific team methods, and general team factors in the performance of software teams.
Method
We studied 40 small-scale software development teams which used Extreme Programming (XP). We measured (1) the teams' adherence to XP methods, (2) their use of XP-specific team practices, and (3) standard team attributes, as well as the quality of the project's outcomes. We used Williams et al.'s (2004a) [33] Shodan measures of XP methods, and regression analysis.
Results
All three types of variables are associated with the project's performance. Teamworking is important but it is the XP-specific team factor (continuous integration, coding standards, and collective code ownership) that is significant. Only customer planning (release planning/planning game, customer access, short releases, and stand-up meeting) is positively related to performance. A negative relationship between foundations (automated unit tests, customer acceptance tests, test-first design, pair programming, and refactoring) is found and is moderated by craftsmanship (sustainable pace, simple design, and metaphor/system of names). Of the general team factors only cooperation is related to performance. Cooperation mediates the relationship between the XP-specific team factor and performance.
Conclusion
Client and team foci of the XP method are its critical active ingredients.",0,"Successful extreme programming: Fidelity to the methodology or good teamworking?. Context
Developing a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team's success? Perhaps the more specific software engineering team factors, for example the agile development method's collective ownership and code management, are decisive.
Objective
To assess the contribution of agile methodology, agile-specific team methods, and general team factors in the performance of software teams.
Method
We studied 40 small-scale software development teams which used Extreme Programming (XP). We measured (1) the teams' adherence to XP methods, (2) their use of XP-specific team practices, and (3) standard team attributes, as well as the quality of the project's outcomes. We used Williams et al.'s (2004a) [33] Shodan measures of XP methods, and regression analysis.
Results
All three types of variables are associated with the project's performance. Teamworking is important but it is the XP-specific team factor (continuous integration, coding standards, and collective code ownership) that is significant. Only customer planning (release planning/planning game, customer access, short releases, and stand-up meeting) is positively related to performance. A negative relationship between foundations (automated unit tests, customer acceptance tests, test-first design, pair programming, and refactoring) is found and is moderated by craftsmanship (sustainable pace, simple design, and metaphor/system of names). Of the general team factors only cooperation is related to performance. Cooperation mediates the relationship between the XP-specific team factor and performance.
Conclusion
Client and team foci of the XP method are its critical active ingredients.",0,0,0,0,0,3
103,A language-independent software renovation framework,"Refactoring, Software renovation, Clustering, Genetic algorithms, Hill climbing","One of the undesired effects of software evolution is the proliferation of unused components, which are not used by any application. As a consequence, the size of binaries and libraries tends to grow and system maintainability tends to decrease. At the same time, a major trend of today's software market is the porting of applications on hand-held devices or, in general, on devices which have a limited amount of available resources. Refactoring and, in particular, the miniaturization of libraries and applications are therefore necessary. We propose a Software Renovation Framework (SRF) and a toolkit covering several aspects of software renovation, such as removing unused objects and code clones, and refactoring existing libraries into smaller more cohesive ones. Refactoring has been implemented in the SRF using a hybrid approach based on hierarchical clustering, on genetic algorithms and hill climbing, also taking into account the developers' feedback. The SRF aims to monitor software system quality in terms of the identified affecting factors, and to perform renovation activities when necessary. Most of the framework activities are language-independent, do not require any kind of source code parsing, and rely on object module analysis. The SRF has been applied to GRASS, which is a large open source Geographical Information System of about one million LOCs in size. It has significantly improved the software organization, has reduced by about 50% the average number of objects linked by each application, and has consequently also reduced the applications' memory requirements.",0,"A language-independent software renovation framework. One of the undesired effects of software evolution is the proliferation of unused components, which are not used by any application. As a consequence, the size of binaries and libraries tends to grow and system maintainability tends to decrease. At the same time, a major trend of today's software market is the porting of applications on hand-held devices or, in general, on devices which have a limited amount of available resources. Refactoring and, in particular, the miniaturization of libraries and applications are therefore necessary. We propose a Software Renovation Framework (SRF) and a toolkit covering several aspects of software renovation, such as removing unused objects and code clones, and refactoring existing libraries into smaller more cohesive ones. Refactoring has been implemented in the SRF using a hybrid approach based on hierarchical clustering, on genetic algorithms and hill climbing, also taking into account the developers' feedback. The SRF aims to monitor software system quality in terms of the identified affecting factors, and to perform renovation activities when necessary. Most of the framework activities are language-independent, do not require any kind of source code parsing, and rely on object module analysis. The SRF has been applied to GRASS, which is a large open source Geographical Information System of about one million LOCs in size. It has significantly improved the software organization, has reduced by about 50% the average number of objects linked by each application, and has consequently also reduced the applications' memory requirements.",0,3,2,0,0,2
104,9 - Case Study: A Final Review,,"Publisher Summary
This chapter focuses on how refactoring enhances the object-oriented development. The chapter emphasizes the importance of controlled development through relatively small iterations. Refactoring is to make changes to the internal structure of software so that it is easier to understand and modify without changes in its observable behavior. Each refactoring should make a relatively small change. It should be accompanied with extensive testing to guarantee the same behavior from the software. Redistribution of classes in stereotyped packages clarifies their role and eases the maintenance burden. Code duplication is a major cause for refactoring. Exposure to design patterns can help identify useful refactoring. One of the iteration discussed is aimed at reducing the perceived complexity of the system. One important reason for refactoring a design is to remove duplicate code. Apart from making the code size larger than it need be, duplicate code is a barrier to effective maintenance. The main problem is that any changes must also be duplicated. There is a risk of not being aware that duplicate changes are required or making different changes to achieve the same effect. Ideally a change should be made only in one place. The aim of this iteration is to remove duplicate code.",0,"9 - Case Study: A Final Review. Publisher Summary
This chapter focuses on how refactoring enhances the object-oriented development. The chapter emphasizes the importance of controlled development through relatively small iterations. Refactoring is to make changes to the internal structure of software so that it is easier to understand and modify without changes in its observable behavior. Each refactoring should make a relatively small change. It should be accompanied with extensive testing to guarantee the same behavior from the software. Redistribution of classes in stereotyped packages clarifies their role and eases the maintenance burden. Code duplication is a major cause for refactoring. Exposure to design patterns can help identify useful refactoring. One of the iteration discussed is aimed at reducing the perceived complexity of the system. One important reason for refactoring a design is to remove duplicate code. Apart from making the code size larger than it need be, duplicate code is a barrier to effective maintenance. The main problem is that any changes must also be duplicated. There is a risk of not being aware that duplicate changes are required or making different changes to achieve the same effect. Ideally a change should be made only in one place. The aim of this iteration is to remove duplicate code.",0,3,2,1,3,2
105,Automated refactoring to the Null Object design pattern,"Refactoring, Design patterns, Null Object, Optional fields, Null checks","Context
Null-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object.
Objective
This work proposes a novel method for automated refactoring to Null Object that eliminates null-checking conditionals associated with optional class fields, i.e., fields that are not initialized in all class instantiations and, thus, their usage needs to be guarded in order to avoid null dereferences.
Method
We introduce an algorithm for automated discovery of refactoring opportunities to Null Object. Moreover, we specify the source code transformation procedure and an extensive set of refactoring preconditions for safely refactoring an optional field and its associated null-checking conditionals to the Null Object design pattern. The method is implemented as an Eclipse plug-in and is evaluated on a set of open source Java projects.
Results
Several refactoring candidates are discovered in the projects used in the evaluation and their refactoring lead to improvement of the cyclomatic complexity of the affected classes. The successful execution of the projects' test suites, on their refactored versions, provides empirical evidence on the soundness of the proposed source code transformation. Runtime performance results highlight the potential for applying our method to a wide range of project sizes.
Conclusion
Our method automates the elimination of null-checking conditionals through refactoring to the Null Object design pattern. It contributes to improvement of the cyclomatic complexity of classes with optional fields. The runtime processing overhead of applying our method is limited and allows its integration to the programmer's routine code analysis activities.",0,"Automated refactoring to the Null Object design pattern. Context
Null-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object.
Objective
This work proposes a novel method for automated refactoring to Null Object that eliminates null-checking conditionals associated with optional class fields, i.e., fields that are not initialized in all class instantiations and, thus, their usage needs to be guarded in order to avoid null dereferences.
Method
We introduce an algorithm for automated discovery of refactoring opportunities to Null Object. Moreover, we specify the source code transformation procedure and an extensive set of refactoring preconditions for safely refactoring an optional field and its associated null-checking conditionals to the Null Object design pattern. The method is implemented as an Eclipse plug-in and is evaluated on a set of open source Java projects.
Results
Several refactoring candidates are discovered in the projects used in the evaluation and their refactoring lead to improvement of the cyclomatic complexity of the affected classes. The successful execution of the projects' test suites, on their refactored versions, provides empirical evidence on the soundness of the proposed source code transformation. Runtime performance results highlight the potential for applying our method to a wide range of project sizes.
Conclusion
Our method automates the elimination of null-checking conditionals through refactoring to the Null Object design pattern. It contributes to improvement of the cyclomatic complexity of classes with optional fields. The runtime processing overhead of applying our method is limited and allows its integration to the programmer's routine code analysis activities.",0,0,2,1,3,4
106,Chapter Four - Multiobjective Optimization for Software Refactoring and Evolution,,"Many studies reported that software maintenance, traditionally defined as any modification made on a software system after its delivery, consumes up to 90% of the total cost of a typical software project. Adding new functionalities, correcting bugs, and modifying the code to improve its quality are major parts of those costs. To ease these maintenance activities, one of the most used techniques is the refactoring which improves design structure while preserving the external behavior. In general, refactoring is performed through two main steps: (1) detection of code fragments corresponding to design defects that need to be improved/fixed and (2) identification of refactoring solutions to achieve this goal. Our research project targets the automation of these two refactoring steps. Concretely, we consider the detection step as a search-based process to find the suitable detection rules for each type of design defect, by means of a genetic algorithm. To guide the rule-derivation process, real examples of design defects are used. For the refactoring identification step, a multiobjective search-based approach is also used. The process aims at finding the optimal sequence of refactoring operations that improve the software quality by minimizing the number of detected defects. In addition, we explore other objectives to optimize: the effort needed to apply refactorings, semantic preservation, and the similarity with good refactorings applied in the past to similar contexts. Hence, the effort corresponds to the code modification/adaptation score needed to apply the suggested refactoring solutions. On the other hand, the semantic preservation insures that the refactored program is semantically equivalent to the original one, and that it models correctly the domain semantics. Indeed, we use knowledge from historical code changes to propose new refactoring solutions in similar contexts to improve the automation of refactoring.",0,"Chapter Four - Multiobjective Optimization for Software Refactoring and Evolution. Many studies reported that software maintenance, traditionally defined as any modification made on a software system after its delivery, consumes up to 90% of the total cost of a typical software project. Adding new functionalities, correcting bugs, and modifying the code to improve its quality are major parts of those costs. To ease these maintenance activities, one of the most used techniques is the refactoring which improves design structure while preserving the external behavior. In general, refactoring is performed through two main steps: (1) detection of code fragments corresponding to design defects that need to be improved/fixed and (2) identification of refactoring solutions to achieve this goal. Our research project targets the automation of these two refactoring steps. Concretely, we consider the detection step as a search-based process to find the suitable detection rules for each type of design defect, by means of a genetic algorithm. To guide the rule-derivation process, real examples of design defects are used. For the refactoring identification step, a multiobjective search-based approach is also used. The process aims at finding the optimal sequence of refactoring operations that improve the software quality by minimizing the number of detected defects. In addition, we explore other objectives to optimize: the effort needed to apply refactorings, semantic preservation, and the similarity with good refactorings applied in the past to similar contexts. Hence, the effort corresponds to the code modification/adaptation score needed to apply the suggested refactoring solutions. On the other hand, the semantic preservation insures that the refactored program is semantically equivalent to the original one, and that it models correctly the domain semantics. Indeed, we use knowledge from historical code changes to propose new refactoring solutions in similar contexts to improve the automation of refactoring.",0,2,2,1,3,2
107,On the refactoring of activity labels in business process models,"Activity label refactoring, Business process modeling, Model quality","Large corporations increasingly utilize business process models for documenting and redesigning their operations. The extent of such modeling initiatives with several hundred models and dozens of often hardly trained modelers calls for automated quality assurance. While formal properties of control flow can easily be checked by existing tools, there is a notable gap for checking the quality of the textual content of models, in particular, its activity labels. In this paper, we address the problem of activity label quality in business process models. We designed a technique for the recognition of labeling styles, and the automatic refactoring of labels with quality issues. More specifically, we developed a parsing algorithm that is able to deal with the shortness of activity labels, which integrates natural language tools like WordNet and the Stanford Parser. Using three business process model collections from practice with differing labeling style distributions, we demonstrate the applicability of our technique. In comparison to a straightforward application of standard natural language tools, our technique provides much more stable results. As an outcome, the technique shifts the boundary of process model quality issues that can be checked automatically from syntactic to semantic aspects.",0,"On the refactoring of activity labels in business process models. Large corporations increasingly utilize business process models for documenting and redesigning their operations. The extent of such modeling initiatives with several hundred models and dozens of often hardly trained modelers calls for automated quality assurance. While formal properties of control flow can easily be checked by existing tools, there is a notable gap for checking the quality of the textual content of models, in particular, its activity labels. In this paper, we address the problem of activity label quality in business process models. We designed a technique for the recognition of labeling styles, and the automatic refactoring of labels with quality issues. More specifically, we developed a parsing algorithm that is able to deal with the shortness of activity labels, which integrates natural language tools like WordNet and the Stanford Parser. Using three business process model collections from practice with differing labeling style distributions, we demonstrate the applicability of our technique. In comparison to a straightforward application of standard natural language tools, our technique provides much more stable results. As an outcome, the technique shifts the boundary of process model quality issues that can be checked automatically from syntactic to semantic aspects.",0,3,2,1,2,0
108,Toward automated refactoring of crosscutting concerns into aspects,"Software evolution, Separation of concerns, Aspect refactoring","Aspect-oriented programing (AOP) improves the separation of concerns by encapsulating crosscutting concerns into aspects. Thus, aspect-oriented programing aims to better support the evolution of systems. Along this line, we have defined a process that assists the developer to refactor an object-oriented system into an aspect-oriented one. In this paper we propose the use of association rules and Markov models to improve the assistance in accomplishing some of the tasks of this process. Specifically, we use these techniques to help the developer in the task of encapsulating a fragment of aspectizable code into an aspect. This includes the choice of a fragment of aspectizable code to be encapsulated, the selection of a suitable aspect refactoring, and the analysis and application of additional restructurings when necessary. Our case study of the refactoring of a J2EE system shows that the use of the process reduces the intervention of the developer during the refactoring.",0,"Toward automated refactoring of crosscutting concerns into aspects. Aspect-oriented programing (AOP) improves the separation of concerns by encapsulating crosscutting concerns into aspects. Thus, aspect-oriented programing aims to better support the evolution of systems. Along this line, we have defined a process that assists the developer to refactor an object-oriented system into an aspect-oriented one. In this paper we propose the use of association rules and Markov models to improve the assistance in accomplishing some of the tasks of this process. Specifically, we use these techniques to help the developer in the task of encapsulating a fragment of aspectizable code into an aspect. This includes the choice of a fragment of aspectizable code to be encapsulated, the selection of a suitable aspect refactoring, and the analysis and application of additional restructurings when necessary. Our case study of the refactoring of a J2EE system shows that the use of the process reduces the intervention of the developer during the refactoring.",0,1,2,1,3,4
109,An efficient approach to identify multiple and independent Move Method refactoring candidates,"Refactoring identification process, Refactoring selection, Multiple simultaneous refactorings, Maintainability improvement, Refactoring effect dependency","Context
Application of a refactoring operation creates a new set of dependency in the revised design as well as a new set of further refactoring candidates. In the studies of stepwise refactoring recommendation approaches, applying one refactoring at a time has been used, but is inefficient because the identification of the best candidate in each iteration of refactoring identification process is computation-intensive. Therefore, it is desirable to accurately identify multiple and independent candidates to enhance efficiency of refactoring process.
Objective
We propose an automated approach to identify multiple refactorings that can be applied simultaneously to maximize the maintainability improvement of software. Our approach can attain the same degree of maintainability enhancement as the method of the refactoring identification of the single best one, but in fewer iterations (lower computation cost).
Method
The concept of maximal independent set (MIS) enables us to identify multiple refactoring operations that can be applied simultaneously. Each MIS contains a group of refactoring candidates that neither affect (i.e., enable or disable) nor influence maintainability on each other. Refactoring effect delta table quantifies the degree of maintainability improvement each elementary candidate. For each iteration of the refactoring identification process, multiple refactorings that best improve maintainability are selected among sets of refactoring candidates (MISs).
Results
We demonstrate the effectiveness and efficiency of the proposed approach by simulating the refactoring operations on several large-scale open source projects such as jEdit, Columba, and JGit. The results show that our proposed approach can improve maintainability by the same degree or to a better extent than the competing method, choosing one refactoring candidate at a time, in a significantly smaller number of iterations. Thus, applying multiple refactorings at a time is both effective and efficient.
Conclusion
Our proposed approach helps improve the maintainability as well as the productivity of refactoring identification.",1,"An efficient approach to identify multiple and independent Move Method refactoring candidates. Context
Application of a refactoring operation creates a new set of dependency in the revised design as well as a new set of further refactoring candidates. In the studies of stepwise refactoring recommendation approaches, applying one refactoring at a time has been used, but is inefficient because the identification of the best candidate in each iteration of refactoring identification process is computation-intensive. Therefore, it is desirable to accurately identify multiple and independent candidates to enhance efficiency of refactoring process.
Objective
We propose an automated approach to identify multiple refactorings that can be applied simultaneously to maximize the maintainability improvement of software. Our approach can attain the same degree of maintainability enhancement as the method of the refactoring identification of the single best one, but in fewer iterations (lower computation cost).
Method
The concept of maximal independent set (MIS) enables us to identify multiple refactoring operations that can be applied simultaneously. Each MIS contains a group of refactoring candidates that neither affect (i.e., enable or disable) nor influence maintainability on each other. Refactoring effect delta table quantifies the degree of maintainability improvement each elementary candidate. For each iteration of the refactoring identification process, multiple refactorings that best improve maintainability are selected among sets of refactoring candidates (MISs).
Results
We demonstrate the effectiveness and efficiency of the proposed approach by simulating the refactoring operations on several large-scale open source projects such as jEdit, Columba, and JGit. The results show that our proposed approach can improve maintainability by the same degree or to a better extent than the competing method, choosing one refactoring candidate at a time, in a significantly smaller number of iterations. Thus, applying multiple refactorings at a time is both effective and efficient.
Conclusion
Our proposed approach helps improve the maintainability as well as the productivity of refactoring identification.",0,0,2,2,3,0
110,A reference architecture for organizing the internal structure of metadata-based frameworks,"Framework, Metadata, Metadata-based framework, Software architecture, Reference architecture, Pattern language","Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes.",0,"A reference architecture for organizing the internal structure of metadata-based frameworks. Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes.",0,3,2,0,2,1
111,Metrics for measuring complexity and completeness for social goal models,"Goal-oriented requirements models, , Software metrics, Model assessment","Goal-oriented Requirements Engineering approaches have become popular in the Requirements Engineering community as they provide expressive modelling languages for requirements elicitation and analysis. However, as a common challenge, such approaches are still struggling when it comes to managing the accidental complexity of their models. Furthermore, those models might be incomplete, resulting in insufficient information for proper understanding and implementation. In this paper, we provide a set of metrics, which are formally specified and have tool support, to measure and analyse complexity and completeness of goal models, in particular social goal models (e.g. i). Concerning complexity, the aim is to identify refactoring opportunities to improve the modularity of those models, and consequently reduce their accidental complexity. With respect to completeness, the goal is to automatically detect model incompleteness. We evaluate these metrics by applying them to a set of well-known system models from industry and academia. Our results suggest refactoring opportunities in the evaluated models, and provide a timely feedback mechanism for requirements engineers on how close they are to completing their models.",0,"Metrics for measuring complexity and completeness for social goal models. Goal-oriented Requirements Engineering approaches have become popular in the Requirements Engineering community as they provide expressive modelling languages for requirements elicitation and analysis. However, as a common challenge, such approaches are still struggling when it comes to managing the accidental complexity of their models. Furthermore, those models might be incomplete, resulting in insufficient information for proper understanding and implementation. In this paper, we provide a set of metrics, which are formally specified and have tool support, to measure and analyse complexity and completeness of goal models, in particular social goal models (e.g. i). Concerning complexity, the aim is to identify refactoring opportunities to improve the modularity of those models, and consequently reduce their accidental complexity. With respect to completeness, the goal is to automatically detect model incompleteness. We evaluate these metrics by applying them to a set of well-known system models from industry and academia. Our results suggest refactoring opportunities in the evaluated models, and provide a timely feedback mechanism for requirements engineers on how close they are to completing their models.",0,3,2,0,2,1
112,Xmipp 3.0: An improved software suite for image processing in electron microscopy,"Electron microscopy, Single particles analysis, Image processing, Software package","Xmipp is a specialized software package for image processing in electron microscopy, and that is mainly focused on 3D reconstruction of macromolecules through single-particles analysis. In this article we present Xmipp 3.0, a major release which introduces several improvements and new developments over the previous version. A central improvement is the concept of a project that stores the entire processing workflow from data import to final results. It is now possible to monitor, reproduce and restart all computing tasks as well as graphically explore the complete set of interrelated tasks associated to a given project. Other graphical tools have also been improved such as data visualization, particle picking and parameter ``wizards'' that allow the visual selection of some key parameters. Many standard image formats are transparently supported for input/output from all programs. Additionally, results have been standardized, facilitating the interoperation between different Xmipp programs. Finally, as a result of a large code refactoring, the underlying C++ libraries are better suited for future developments and all code has been optimized. Xmipp is an open-source package that is freely available for download from: http://xmipp..cnb.csic.es.",0,"Xmipp 3.0: An improved software suite for image processing in electron microscopy. Xmipp is a specialized software package for image processing in electron microscopy, and that is mainly focused on 3D reconstruction of macromolecules through single-particles analysis. In this article we present Xmipp 3.0, a major release which introduces several improvements and new developments over the previous version. A central improvement is the concept of a project that stores the entire processing workflow from data import to final results. It is now possible to monitor, reproduce and restart all computing tasks as well as graphically explore the complete set of interrelated tasks associated to a given project. Other graphical tools have also been improved such as data visualization, particle picking and parameter ``wizards'' that allow the visual selection of some key parameters. Many standard image formats are transparently supported for input/output from all programs. Additionally, results have been standardized, facilitating the interoperation between different Xmipp programs. Finally, as a result of a large code refactoring, the underlying C++ libraries are better suited for future developments and all code has been optimized. Xmipp is an open-source package that is freely available for download from: http://xmipp..cnb.csic.es.",1,3,2,0,1,0
113,Ring: A unifying meta-model and infrastructure for Smalltalk source code analysis tools,"Source code meta-model, Versioning, Refactoring, Monticello, Smalltalk","Source code management systems record different versions of code. Tool support can then compute deltas between versions. To ease version history analysis we need adequate models to represent source code entities.. Now naturally the questions of their definition, the abstractions they use, and the APIs of such models are raised, especially in the context of a reflective system which already offers a model of its own structure. We believe that this problem is due to the lack of a powerful code meta-model as well as an infrastructure. In Smalltalk, often several source code meta-models coexist: the Smalltalk reflective API coexists with the one of the Refactoring engine or distributed versioning system such as Monticello or Store. While having specific meta-models is an adequate engineered solution, it multiplies meta-models and it requires more maintenance efforts (e.g., duplication of tests, transformation between models), and more importantly hinders navigation tool reuse when meta-models do not offer polymorphic APIs. As a first step to provide an infrastructure to support history analysis, this article presents Ring, a unifying source code meta-model that can be used to support several activities and proposes a unified and layered approach to be the foundation for building an infrastructure for version and stream of change analyses. We re-implemented three tools based on Ring to show that it can be used as the underlying meta-model for remote and off-image browsing, scoping refactoring, and visualizing and analyzing changes. As a future work and based on Ring we will build a new generation of history analysis tools.",0,"Ring: A unifying meta-model and infrastructure for Smalltalk source code analysis tools. Source code management systems record different versions of code. Tool support can then compute deltas between versions. To ease version history analysis we need adequate models to represent source code entities.. Now naturally the questions of their definition, the abstractions they use, and the APIs of such models are raised, especially in the context of a reflective system which already offers a model of its own structure. We believe that this problem is due to the lack of a powerful code meta-model as well as an infrastructure. In Smalltalk, often several source code meta-models coexist: the Smalltalk reflective API coexists with the one of the Refactoring engine or distributed versioning system such as Monticello or Store. While having specific meta-models is an adequate engineered solution, it multiplies meta-models and it requires more maintenance efforts (e.g., duplication of tests, transformation between models), and more importantly hinders navigation tool reuse when meta-models do not offer polymorphic APIs. As a first step to provide an infrastructure to support history analysis, this article presents Ring, a unifying source code meta-model that can be used to support several activities and proposes a unified and layered approach to be the foundation for building an infrastructure for version and stream of change analyses. We re-implemented three tools based on Ring to show that it can be used as the underlying meta-model for remote and off-image browsing, scoping refactoring, and visualizing and analyzing changes. As a future work and based on Ring we will build a new generation of history analysis tools.",1,2,2,1,1,1
114,Automated refactoring to the Strategy design pattern,"Refactoring, Design patterns, Strategy pattern, Polymorphism, Total replacement of conditional logic","Context
The automated identification of code fragments characterized by common design flaws (or ``code smells'') that can be handled through refactoring, fosters refactoring activities, especially in large code bases where multiple developers are engaged without a detailed view on the whole system. Automated refactoring to design patterns enables significant contributions to design quality even from developers with little experience on the use of the required patterns.
Objective
This work targets the automated identification of refactoring opportunities to the Strategy design pattern and the elimination through polymorphism of respective ``code smells'' that are related to extensive use of complex conditional statements.
Method
An algorithm is introduced for the automated identification of refactoring opportunities to the Strategy design pattern. Suggested refactorings comprise conditional statements that are characterized by analogies to the Strategy design pattern, in terms of the purpose and selection mode of strategies.. Moreover, this work specifies the procedure for refactoring to Strategy the identified conditional statements. For special cases of these statements, a technique is proposed for total replacement of conditional logic with method calls of appropriate concrete Strategy instances. The identification algorithm and the refactoring procedure are implemented and integrated in the JDeodorant Eclipse plug-in. The method is evaluated on a set of Java projects, in terms of quality of the suggested refactorings and run-time efficiency. The relevance of the identified refactoring opportunities is verified by expert software engineers.
Results
The identification algorithm recalled, from the projects used during evaluation, many of the refactoring candidates that were identified by the expert software engineers. Its execution time on projects of varying size confirmed the run-time efficiency of this method.
Conclusion
The proposed method for automated refactoring to Strategy contributes to simplification of conditional statements. Moreover, it enhances system extensibility through the Strategy design pattern.",0,"Automated refactoring to the Strategy design pattern. Context
The automated identification of code fragments characterized by common design flaws (or ``code smells'') that can be handled through refactoring, fosters refactoring activities, especially in large code bases where multiple developers are engaged without a detailed view on the whole system. Automated refactoring to design patterns enables significant contributions to design quality even from developers with little experience on the use of the required patterns.
Objective
This work targets the automated identification of refactoring opportunities to the Strategy design pattern and the elimination through polymorphism of respective ``code smells'' that are related to extensive use of complex conditional statements.
Method
An algorithm is introduced for the automated identification of refactoring opportunities to the Strategy design pattern. Suggested refactorings comprise conditional statements that are characterized by analogies to the Strategy design pattern, in terms of the purpose and selection mode of strategies.. Moreover, this work specifies the procedure for refactoring to Strategy the identified conditional statements. For special cases of these statements, a technique is proposed for total replacement of conditional logic with method calls of appropriate concrete Strategy instances. The identification algorithm and the refactoring procedure are implemented and integrated in the JDeodorant Eclipse plug-in. The method is evaluated on a set of Java projects, in terms of quality of the suggested refactorings and run-time efficiency. The relevance of the identified refactoring opportunities is verified by expert software engineers.
Results
The identification algorithm recalled, from the projects used during evaluation, many of the refactoring candidates that were identified by the expert software engineers. Its execution time on projects of varying size confirmed the run-time efficiency of this method.
Conclusion
The proposed method for automated refactoring to Strategy contributes to simplification of conditional statements. Moreover, it enhances system extensibility through the Strategy design pattern.",0,0,2,2,3,0
115,Delving source code with formal concept analysis,"Source-code mining, Formal concept analysis, Software classification","Getting an initial understanding of the structure of a software system, whether it is for software maintenance, evolution or reengineering purposes, is a nontrivial task. We propose a lightweight approach to delve a system's source code automatically and efficiently for relevant concepts of interest: what concerns are addressed in the code, what patterns, coding idioms and conventions have been adopted, and where and how are they implemented. We use formal concept analysis to do the actual source-code mining, and then filter, classify and combine the results to present them in a format that is more convenient to a software engineer. We applied a prototype tool that implements this approach to several small to medium-sized Smalltalk applications. For each of these, the tool uncovered several design pattern instances, coding and naming conventions, refactoring opportunities and important domain concepts. Although the tool and approach can still be improved in many ways, the tool does already provides useful results when trying to get an initial understanding of a system. The obtained results also illustrate the relevance and feasibility of using formal concept analysis as an efficient technique for source-code mining.",0,"Delving source code with formal concept analysis. Getting an initial understanding of the structure of a software system, whether it is for software maintenance, evolution or reengineering purposes, is a nontrivial task. We propose a lightweight approach to delve a system's source code automatically and efficiently for relevant concepts of interest: what concerns are addressed in the code, what patterns, coding idioms and conventions have been adopted, and where and how are they implemented. We use formal concept analysis to do the actual source-code mining, and then filter, classify and combine the results to present them in a format that is more convenient to a software engineer. We applied a prototype tool that implements this approach to several small to medium-sized Smalltalk applications. For each of these, the tool uncovered several design pattern instances, coding and naming conventions, refactoring opportunities and important domain concepts. Although the tool and approach can still be improved in many ways, the tool does already provides useful results when trying to get an initial understanding of a system. The obtained results also illustrate the relevance and feasibility of using formal concept analysis as an efficient technique for source-code mining.",0,3,2,1,1,1
116,Refactoring and representation independence for class hierarchies,"Refactoring, Program transformation, Class inheritance, Representation independence, Semantics, Verification","Refactoring transformations are important for productivity and quality in software evolution. Modular reasoning about semantics preserving transformations is difficult even in typed class-based languages because transformations can change the internal representations for multiple interdependent classes and because encapsulation can be violated by pointers to mutable objects. In this paper, an existing theory of representation independence for a single class, based on a simple notion of ownership confinement, is generalized to a hierarchy of classes and used to prove refactoring rules that embody transformations of complete class trees. This allows us to formalize refactorings that inherently involve class inheritance, such as Pull Up or Push Down Field; moreover, this makes it possible to generalize refactorings previously restricted to change of data representation of private attributes (like Extract Class and Encapsulate Field) to address data refinement of protected attributes, dealing with the impact that the corresponding transformations may cause in the subclasses. The utility of the proposed rules is shown in a relatively extensive case study. Shortcomings of the theory are described as a challenge to other approaches to heap encapsulation and relational reasoning for classes.",0,"Refactoring and representation independence for class hierarchies. Refactoring transformations are important for productivity and quality in software evolution. Modular reasoning about semantics preserving transformations is difficult even in typed class-based languages because transformations can change the internal representations for multiple interdependent classes and because encapsulation can be violated by pointers to mutable objects. In this paper, an existing theory of representation independence for a single class, based on a simple notion of ownership confinement, is generalized to a hierarchy of classes and used to prove refactoring rules that embody transformations of complete class trees. This allows us to formalize refactorings that inherently involve class inheritance, such as Pull Up or Push Down Field; moreover, this makes it possible to generalize refactorings previously restricted to change of data representation of private attributes (like Extract Class and Encapsulate Field) to address data refinement of protected attributes, dealing with the impact that the corresponding transformations may cause in the subclasses. The utility of the proposed rules is shown in a relatively extensive case study. Shortcomings of the theory are described as a challenge to other approaches to heap encapsulation and relational reasoning for classes.",1,1,0,0,1,4
117,A categorical framework for the transformation of object-oriented systems: Models and data,"Refactoring, Evolution, Transformation, Migration","Refactoring of information systems is hard, for two reasons. On the one hand, large databases exist which have to be adjusted. On the other hand, many programs access those data. Data and programs all have to be migrated in a consistent manner such that their semantics does not change. This paper addresses the data part of the problem and introduces a model for object-oriented structures, describing the schema level with classes, associations, and inheritance as well as the instance level with objects and links. Positive Horn formulas based on predicates are used to formulate constraints to be obeyed by the schema and instance level, in order to reflect object-oriented structures. Homomorphisms are used for the typing of the instance level as well as for the description of refactorings which specify the addition, folding, and unfolding of schema elements. A categorial framework is presented which allows us to derive instance migrations from schema transformations in such a way that instances of the old schema are automatically migrated into instances of the new schema. The natural use of the pullback functor for unfolding is followed by an initial semantics approach: Instance migration is completed with the help of a co-adjoint functor on arrow categories.",0,"A categorical framework for the transformation of object-oriented systems: Models and data. Refactoring of information systems is hard, for two reasons. On the one hand, large databases exist which have to be adjusted. On the other hand, many programs access those data. Data and programs all have to be migrated in a consistent manner such that their semantics does not change. This paper addresses the data part of the problem and introduces a model for object-oriented structures, describing the schema level with classes, associations, and inheritance as well as the instance level with objects and links. Positive Horn formulas based on predicates are used to formulate constraints to be obeyed by the schema and instance level, in order to reflect object-oriented structures. Homomorphisms are used for the typing of the instance level as well as for the description of refactorings which specify the addition, folding, and unfolding of schema elements. A categorial framework is presented which allows us to derive instance migrations from schema transformations in such a way that instances of the old schema are automatically migrated into instances of the new schema. The natural use of the pullback functor for unfolding is followed by an initial semantics approach: Instance migration is completed with the help of a co-adjoint functor on arrow categories.",1,1,0,0,1,4
118,Adaptive Detection of Design Flaws,"Design flaw, code smell, object-oriented design, software quality, refactoring, program analysis, machine learning","Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid ``bad'' --- incomprehensible, hardly expandable and changeable --- program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study.",0,"Adaptive Detection of Design Flaws. Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid ``bad'' --- incomprehensible, hardly expandable and changeable --- program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study.",0,3,2,1,0,3
119,Refactoring-aware versioning in Eclipse,"Refactoring, Versioning, Merge, Eclipse, Software Configuration Management","To fully support refactorings in a team development environment we have implemented a refactoring-aware repository provider as an extension plug-in to the Java Development Tools in Eclipse. The versioning system treats refactorings as first-class changes described as semantic ac- tions rather than the set of resulting changes scattered over the source tree. We also introduce refactoring-aware merge, which merges refactorings as well as traditional changes utilizing the se- mantics of the refactorings to detect and resolve merge conflicts. It also ensures that the semantic meaning of a refactoring is preserved after the merge.",0,"Refactoring-aware versioning in Eclipse. To fully support refactorings in a team development environment we have implemented a refactoring-aware repository provider as an extension plug-in to the Java Development Tools in Eclipse. The versioning system treats refactorings as first-class changes described as semantic ac- tions rather than the set of resulting changes scattered over the source tree. We also introduce refactoring-aware merge, which merges refactorings as well as traditional changes utilizing the se- mantics of the refactorings to detect and resolve merge conflicts. It also ensures that the semantic meaning of a refactoring is preserved after the merge.",0,1,0,0,3,4
120,Client-based cohesion metrics for Java programs,"Metrics, Cohesion, Refactoring, Design patterns, Java","One purpose of software metrics is to measure the quality of programs. The results can be for example used to predict maintenance costs or improve code quality. An emerging view is that if software metrics are going to be used to improve quality, they must help in finding code that should be refactored. Often refactoring or applying a design pattern is related to the role of the class to be refactored. In client-based metrics, a project gives the class a context. These metrics measure how a class is used by other classes in the context. We present a new client-based metric LCIC (Lack of Coherence in Clients), which analyses if the class being measured has a coherent set of roles in the program. Interfaces represent the roles of classes. If a class does not have a coherent set of roles, it should be refactored, or a new interface should be defined for the class. We have implemented a tool for measuring the metric LCIC for Java projects in the Eclipse environment. We calculated LCIC values for classes of several open source projects. We compare these results with results of other related metrics, and inspect the measured classes to find out what kind of refactorings are needed. We also analyse the relation of different design patterns and refactorings to our metric. Our experiments reveal the usefulness of client-based metrics to improve the quality of code.",0,"Client-based cohesion metrics for Java programs. One purpose of software metrics is to measure the quality of programs. The results can be for example used to predict maintenance costs or improve code quality. An emerging view is that if software metrics are going to be used to improve quality, they must help in finding code that should be refactored. Often refactoring or applying a design pattern is related to the role of the class to be refactored. In client-based metrics, a project gives the class a context. These metrics measure how a class is used by other classes in the context. We present a new client-based metric LCIC (Lack of Coherence in Clients), which analyses if the class being measured has a coherent set of roles in the program. Interfaces represent the roles of classes. If a class does not have a coherent set of roles, it should be refactored, or a new interface should be defined for the class. We have implemented a tool for measuring the metric LCIC for Java projects in the Eclipse environment. We calculated LCIC values for classes of several open source projects. We compare these results with results of other related metrics, and inspect the measured classes to find out what kind of refactorings are needed. We also analyse the relation of different design patterns and refactorings to our metric. Our experiments reveal the usefulness of client-based metrics to improve the quality of code.",0,2,2,1,2,3
121,n-Tiered Test Automation Architecture for Agile Software Systems,"Software Engineering, Testing, Test Automation, Architecture, Agile Systems","This paper introduces a multi-tiered test automation architecture to optimize test automation in an Agile software development environment while increasing both the test coverage and depth of each tier. Test Automation is the act of converting manual test cases into automated scripts that can be executed autonomously. In general, testing accounts for roughly 60% of the overall development budget and approximately 50% of that is attributed to regression testing. In recent years software organizations have begun migrating to Agile software development practices and automated testing in hopes of reducing the cost, lengthy regression cycles, and time to market. Traditionally, test automation is conducted on stable, non-changing applications. In an Agile environment where the code constantly changes, automated test cases become obsolete and must constantly be refactored in order to provide meaningful feedback about the system's quality. In most instances the cost of maintenance of automated test code completely overshadows the entire automation effort and negates any possible Return on Investment (RoI). An n-Tiered Test Automation Architecture seeks to retain the RoI by abstracting the automation project into separate distinct tiers; Presentation, Business, Data, and Services. These abstractions allow automated testing to continue providing feedback despite the constant revision of the system. A case study was conducted using this method and the observations showed that the automation architecture was resilient to change while increasing the test coverage, the depth of testing, and the overall quality of the application under test.",0,"n-Tiered Test Automation Architecture for Agile Software Systems. This paper introduces a multi-tiered test automation architecture to optimize test automation in an Agile software development environment while increasing both the test coverage and depth of each tier. Test Automation is the act of converting manual test cases into automated scripts that can be executed autonomously. In general, testing accounts for roughly 60% of the overall development budget and approximately 50% of that is attributed to regression testing. In recent years software organizations have begun migrating to Agile software development practices and automated testing in hopes of reducing the cost, lengthy regression cycles, and time to market. Traditionally, test automation is conducted on stable, non-changing applications. In an Agile environment where the code constantly changes, automated test cases become obsolete and must constantly be refactored in order to provide meaningful feedback about the system's quality. In most instances the cost of maintenance of automated test code completely overshadows the entire automation effort and negates any possible Return on Investment (RoI). An n-Tiered Test Automation Architecture seeks to retain the RoI by abstracting the automation project into separate distinct tiers; Presentation, Business, Data, and Services. These abstractions allow automated testing to continue providing feedback despite the constant revision of the system. A case study was conducted using this method and the observations showed that the automation architecture was resilient to change while increasing the test coverage, the depth of testing, and the overall quality of the application under test.",0,2,0,0,0,2
122,Modelchecking Correctness of Refactorings - Some Experiments,"Refactoring, Object Z, Model Checking, SAL","Refactorings are changes made to programs, models or specifications with the intention of improving their structure and thus making them clearer, more readable and re-usable. Refactorings are required to be behaviour-preserving in that the external behaviour of the program/model/specification remains unchanged. In this paper we show how a simple type of refactorings on object-oriented specifications (written in Object-Z) can be formally shown to be behaviour-preserving using a modelchecker (SAL). The class of refactorings treated covers those operating on a single method only.",0,"Modelchecking Correctness of Refactorings - Some Experiments. Refactorings are changes made to programs, models or specifications with the intention of improving their structure and thus making them clearer, more readable and re-usable. Refactorings are required to be behaviour-preserving in that the external behaviour of the program/model/specification remains unchanged. In this paper we show how a simple type of refactorings on object-oriented specifications (written in Object-Z) can be formally shown to be behaviour-preserving using a modelchecker (SAL). The class of refactorings treated covers those operating on a single method only.",1,1,2,1,3,4
123,Identifying refactoring opportunities in object-oriented code: A systematic literature review,"Refactoring activity, Refactoring opportunity, Systematic literature review","Context
Identifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities.
Objective
This paper provides a systematic literature review of existing studies identifying opportunities for code refactoring activities.
Method
We performed an automatic search of the relevant digital libraries for potentially relevant studies published through the end of 2013, performed pilot and author-based searches, and selected 47 primary studies (PSs) based on inclusion and exclusion criteria. The PSs were analyzed based on a number of criteria, including the refactoring activities, the approaches to refactoring opportunity identification, the empirical evaluation approaches, and the data sets used.
Results
The results indicate that research in the area of identifying refactoring opportunities is highly active. Most of the studies have been performed by academic researchers using nonindustrial data sets. Extract Class and Move Method were found to be the most frequently considered refactoring activities. The results show that researchers use six primary existing approaches to identify refactoring opportunities and six approaches to empirically evaluate the identification techniques. Most of the systems used in the evaluation process were open-source, which helps to make the studies repeatable. However, a relatively high percentage of the data sets used in the empirical evaluations were small, which limits the generality of the results.
Conclusions
It would be beneficial to perform further studies that consider more refactoring activities, involve researchers from industry, and use large-scale and industrial-based systems.",1,"Identifying refactoring opportunities in object-oriented code: A systematic literature review. Context
Identifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities.
Objective
This paper provides a systematic literature review of existing studies identifying opportunities for code refactoring activities.
Method
We performed an automatic search of the relevant digital libraries for potentially relevant studies published through the end of 2013, performed pilot and author-based searches, and selected 47 primary studies (PSs) based on inclusion and exclusion criteria. The PSs were analyzed based on a number of criteria, including the refactoring activities, the approaches to refactoring opportunity identification, the empirical evaluation approaches, and the data sets used.
Results
The results indicate that research in the area of identifying refactoring opportunities is highly active. Most of the studies have been performed by academic researchers using nonindustrial data sets. Extract Class and Move Method were found to be the most frequently considered refactoring activities. The results show that researchers use six primary existing approaches to identify refactoring opportunities and six approaches to empirically evaluate the identification techniques. Most of the systems used in the evaluation process were open-source, which helps to make the studies repeatable. However, a relatively high percentage of the data sets used in the empirical evaluations were small, which limits the generality of the results.
Conclusions
It would be beneficial to perform further studies that consider more refactoring activities, involve researchers from industry, and use large-scale and industrial-based systems.",0,2,1,2,2,0
124,"International Conference on Computational Science, ICCS 2011 Gleipnir: A Memory Analysis Tool","Program Profiling, Memory Access Traces, Cache Memories, Data and Code Refactoring","This paper describes a program profiling and analysis tool called Gleipnir. Gleipnir collects memory access traces and associates each access with a specific program internal structure such as a thread, a function, a data structure or a scalar variable. The data provided by Gleipnir can be used to analyze how program variables and associated memory accesses map to L-1 as well as higher level cache memories. This information can be used to investigate techniques to refactor data or code to improve memory access performance. It is our hypothesis that optimizing cache performance at all levels is very important to both single-core and multi-core processors.. In this paper we will describe the Gleipnir tool and some examples of its use in optimizing memory performance. The overall goal of our research is to develop techniques usable by application developers, compilers, and runtime systems to improve the performance of their applications.",0,"International Conference on Computational Science, ICCS 2011 Gleipnir: A Memory Analysis Tool. This paper describes a program profiling and analysis tool called Gleipnir. Gleipnir collects memory access traces and associates each access with a specific program internal structure such as a thread, a function, a data structure or a scalar variable. The data provided by Gleipnir can be used to analyze how program variables and associated memory accesses map to L-1 as well as higher level cache memories. This information can be used to investigate techniques to refactor data or code to improve memory access performance. It is our hypothesis that optimizing cache performance at all levels is very important to both single-core and multi-core processors.. In this paper we will describe the Gleipnir tool and some examples of its use in optimizing memory performance. The overall goal of our research is to develop techniques usable by application developers, compilers, and runtime systems to improve the performance of their applications.",1,3,2,0,2,0
125,Chapter 4 - C/C++ Developers' Toolkit (CDT),,"Publisher Summary
This chapter introduces the basic concepts and operation of the C/C11 Development Tools, CDT. It examines many of the views in the C/C11 Perspective in detail, to see how they contribute to the development process. The CDT Editor has a number of features to make coding life easier. These fall under the general heading of Content Assist. The basic idea of Content Assist is to reduce the number of keystrokes by predicting what is likely to typed based on the current context, scope, and prefix. After building the project, the Debug perspective is looked in detail, with particular emphasis on the many capabilities of breakpoints. Other views in the Debug perspective are reviewed to see how they help you gain insight into what's happening in the program. Finally, the chapter explores a couple of nifty features of Eclipse that can make life easier for developers. Projects can refer to, and can be dependent on, other projects. When changes are made to the referenced project, the dependent project will be rebuilt. Refactoring offers a way to automatically rename a symbol throughout a project or a set of related projects. Eclipse supports an automated approach to refactoring that makes sure that changes are propagated properly throughout a project. The nature of refactoring support is somewhat language-dependent. Eclipse offers extensive refactoring support for Java involving operations on classes, interfaces, and methods.",0,"Chapter 4 - C/C++ Developers' Toolkit (CDT). Publisher Summary
This chapter introduces the basic concepts and operation of the C/C11 Development Tools, CDT. It examines many of the views in the C/C11 Perspective in detail, to see how they contribute to the development process. The CDT Editor has a number of features to make coding life easier. These fall under the general heading of Content Assist. The basic idea of Content Assist is to reduce the number of keystrokes by predicting what is likely to typed based on the current context, scope, and prefix. After building the project, the Debug perspective is looked in detail, with particular emphasis on the many capabilities of breakpoints. Other views in the Debug perspective are reviewed to see how they help you gain insight into what's happening in the program. Finally, the chapter explores a couple of nifty features of Eclipse that can make life easier for developers. Projects can refer to, and can be dependent on, other projects. When changes are made to the referenced project, the dependent project will be rebuilt. Refactoring offers a way to automatically rename a symbol throughout a project or a set of related projects. Eclipse supports an automated approach to refactoring that makes sure that changes are propagated properly throughout a project. The nature of refactoring support is somewhat language-dependent. Eclipse offers extensive refactoring support for Java involving operations on classes, interfaces, and methods.",1,3,0,0,0,0
126,Detecting Structural Refactoring Conflicts Using Critical Pair Analysis,"refactoring, restructuring, graph transformation, critical pair analysis, evolution conflicts, parallel changes","Refactorings are program transformations that improve the software structure while preserving the external behaviour. In spite of this very useful property, refactorings can still give rise to structural conflicts when parallel evolutions to the same software are made by different developers. This paper explores this problem of structural evolution conflicts in a formal way by using graph transformation and critical pair analysis. Based on experiments carried out in the graph transformation tool AGG, we show how this formalism can be exploited to detect and resolve refactoring conflicts.",0,"Detecting Structural Refactoring Conflicts Using Critical Pair Analysis. Refactorings are program transformations that improve the software structure while preserving the external behaviour. In spite of this very useful property, refactorings can still give rise to structural conflicts when parallel evolutions to the same software are made by different developers. This paper explores this problem of structural evolution conflicts in a formal way by using graph transformation and critical pair analysis. Based on experiments carried out in the graph transformation tool AGG, we show how this formalism can be exploited to detect and resolve refactoring conflicts.",1,1,0,1,3,2
127,Increasing clone maintenance support by unifying clone detection and refactoring activities,"Maintenance, Code clones, Refactoring","Context
Clone detection tools provide an automated mechanism to discover clones in source code. On the other side, refactoring capabilities within integrated development environments provide the necessary functionality to assist programmers in refactoring. However, we have observed a gap between the processes of clone detection and refactoring.
Objective
In this paper, we describe our work on unifying the code clone maintenance process by bridging the gap between clone detection and refactoring.
Method
Through an Eclipse plug-in called CeDAR (Clone Detection, Analysis, and Refactoring), we forward clone detection results to the refactoring engine in Eclipse. In this case, the refactoring engine is supplied with information about the detected clones to which it can then determine those clones that can be refactored. We describe the extensions to Eclipse's refactoring engine to allow clones with additional similarity properties to be refactored.
Results
Our evaluation of open source artifacts shows that this process yields considerable increases in the instances of clone groups that may be suggested to the programmer for refactoring within Eclipse.
Conclusion
By unifying the processes of clone detection and refactoring, in addition to providing extensions to the refactoring engine of an IDE, the strengths of both processes (i.e., more significant detection capabilities and an established framework for refactoring) can be garnered.",1,"Increasing clone maintenance support by unifying clone detection and refactoring activities. Context
Clone detection tools provide an automated mechanism to discover clones in source code. On the other side, refactoring capabilities within integrated development environments provide the necessary functionality to assist programmers in refactoring. However, we have observed a gap between the processes of clone detection and refactoring.
Objective
In this paper, we describe our work on unifying the code clone maintenance process by bridging the gap between clone detection and refactoring.
Method
Through an Eclipse plug-in called CeDAR (Clone Detection, Analysis, and Refactoring), we forward clone detection results to the refactoring engine in Eclipse. In this case, the refactoring engine is supplied with information about the detected clones to which it can then determine those clones that can be refactored. We describe the extensions to Eclipse's refactoring engine to allow clones with additional similarity properties to be refactored.
Results
Our evaluation of open source artifacts shows that this process yields considerable increases in the instances of clone groups that may be suggested to the programmer for refactoring within Eclipse.
Conclusion
By unifying the processes of clone detection and refactoring, in addition to providing extensions to the refactoring engine of an IDE, the strengths of both processes (i.e., more significant detection capabilities and an established framework for refactoring) can be garnered.",0,2,2,2,3,0
128,Object-oriented transformations for extracting aspects,"Aspect-oriented programming, Refactoring, Software evolution, Program transformation","In the migration of object-oriented systems towards the aspect technology, after locating fragments of code presenting a crosscutting behavior and before extracting such code to aspects, transformations may be needed in the base program. Such transformations aim to associate crosscutting code to points of the base program that can be captured using the pointcut descriptor model of aspect-oriented languages. In this paper, we present a catalog of object-oriented transformations and demonstrate the importance of such transformations by reporting on a case study involving four systems that have been aspectized using AspectJ.",0,"Object-oriented transformations for extracting aspects. In the migration of object-oriented systems towards the aspect technology, after locating fragments of code presenting a crosscutting behavior and before extracting such code to aspects, transformations may be needed in the base program. Such transformations aim to associate crosscutting code to points of the base program that can be captured using the pointcut descriptor model of aspect-oriented languages. In this paper, we present a catalog of object-oriented transformations and demonstrate the importance of such transformations by reporting on a case study involving four systems that have been aspectized using AspectJ.",1,1,2,1,1,4
129,Refactoring large process model repositories,"Process-aware information system, Process model quality, Process model smell, Process model refactoring","Abstract
With the increasing adoption of process-aware information systems, large process model repositories have emerged. Typically, the models in such repositories are re-aligned to real-world events and demands through adaptation on a day-to-day basis. This bears the risk of introducing model redundancies and of unnecessarily increasing model complexity. If no continuous investment is made in keeping process models simple, changes will become more difficult and error-prone over time. Although refactoring techniques are widely used in software engineering to address similar problems, so far, no comparable state-of-the-art has evolved in the business process management domain. Process designers either have to refactor process models by hand or are simply unable to apply respective techniques at all. This paper proposes a catalogue of process model ``smells'' for identifying refactoring opportunities. In addition, it introduces a set of behavior-preserving techniques for refactoring large process repositories. The proposed refactorings enable process designers to effectively deal with model complexity by making process models better understandable and easier to maintain. The refactorings have been evaluated using large process repositories from the healthcare and automotive domain. To demonstrate the feasibility of the refactoring techniques, a proof-of-concept prototype has been implemented.",0,"Refactoring large process model repositories. Abstract
With the increasing adoption of process-aware information systems, large process model repositories have emerged. Typically, the models in such repositories are re-aligned to real-world events and demands through adaptation on a day-to-day basis. This bears the risk of introducing model redundancies and of unnecessarily increasing model complexity. If no continuous investment is made in keeping process models simple, changes will become more difficult and error-prone over time. Although refactoring techniques are widely used in software engineering to address similar problems, so far, no comparable state-of-the-art has evolved in the business process management domain. Process designers either have to refactor process models by hand or are simply unable to apply respective techniques at all. This paper proposes a catalogue of process model ``smells'' for identifying refactoring opportunities. In addition, it introduces a set of behavior-preserving techniques for refactoring large process repositories. The proposed refactorings enable process designers to effectively deal with model complexity by making process models better understandable and easier to maintain. The refactorings have been evaluated using large process repositories from the healthcare and automotive domain. To demonstrate the feasibility of the refactoring techniques, a proof-of-concept prototype has been implemented.",0,2,1,1,3,1
130,An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution,"Object-oriented design, Bad smells, Software metrics, Design evolution, Open source software, Empirical study","Bad smells are used as a means to identify problematic classes in object-oriented systems for refactoring. The belief that the bad smells are linked with problematic classes is largely based on previous metric research results. Although there is a plethora of empirical studies linking software metrics to errors and error proneness of classes in object-oriented systems, the link between the bad smells and class error probability in the evolution of object-oriented systems after the systems are released has not been explored. There has been no empirical evidence linking the bad smells with class error probability so far. This paper presents the results from an empirical study that investigated the relationship between the bad smells and class error probability in three error-severity levels in an industrial-strength open source system. Our research, which was conducted in the context of the post-release system evolution process, showed that some bad smells were positively associated with the class error probability in the three error-severity levels. This finding supports the use of bad smells as a systematic method to identify and refactor problematic classes in this specific context.",0,"An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution. Bad smells are used as a means to identify problematic classes in object-oriented systems for refactoring. The belief that the bad smells are linked with problematic classes is largely based on previous metric research results. Although there is a plethora of empirical studies linking software metrics to errors and error proneness of classes in object-oriented systems, the link between the bad smells and class error probability in the evolution of object-oriented systems after the systems are released has not been explored. There has been no empirical evidence linking the bad smells with class error probability so far. This paper presents the results from an empirical study that investigated the relationship between the bad smells and class error probability in three error-severity levels in an industrial-strength open source system. Our research, which was conducted in the context of the post-release system evolution process, showed that some bad smells were positively associated with the class error probability in the three error-severity levels. This finding supports the use of bad smells as a systematic method to identify and refactor problematic classes in this specific context.",0,2,0,0,2,0
131,Considerations on the Development of a Refactoring-based Navigation Model for On-line Transaction Systems,"UWE, Navigation Model, Class Diagram, Model Quality, On-line System","The purpose of our present research was to diminish the users `dissatisfaction related to navigational limitations of on-line stores. The outcome may well be used also by the firms which implement transactions and commercial web applications. Our case study was carried out on the platform of a virtual store, for whose navigation model we used a class diagram, via UWE (UML-based Web Engineering), which needed be modified for the navigation sustainability. There were two main sequences in the model development. In the first instance, we chose the classes which could be accessed directly and, in this fashion, we would acquire the class navigation diagram. The second instance consisted in diagram's extension with access structures and system menu, thus resulting a navigation structure diagram used for easily gaining each point of the application. At this point, we utilized refactoring in an attempt to offer a quality model for both developers and users.",0,"Considerations on the Development of a Refactoring-based Navigation Model for On-line Transaction Systems. The purpose of our present research was to diminish the users `dissatisfaction related to navigational limitations of on-line stores. The outcome may well be used also by the firms which implement transactions and commercial web applications. Our case study was carried out on the platform of a virtual store, for whose navigation model we used a class diagram, via UWE (UML-based Web Engineering), which needed be modified for the navigation sustainability. There were two main sequences in the model development. In the first instance, we chose the classes which could be accessed directly and, in this fashion, we would acquire the class navigation diagram. The second instance consisted in diagram's extension with access structures and system menu, thus resulting a navigation structure diagram used for easily gaining each point of the application. At this point, we utilized refactoring in an attempt to offer a quality model for both developers and users.",0,3,0,0,2,1
132,An expert system for determining candidate software classes for refactoring,"Refactoring, Software metrics, Naive Bayes, Refactor prediction","In the lifetime of a software product, development costs are only the tip of the iceberg. Nearly 90% of the cost is maintenance due to error correction, adaptation and mainly enhancements. As Lehman and Belady [Lehman, M. M., & Belady, L. A. (1985). Program evolution: Processes of software change. Academic Press Professional.] state that software will become increasingly unstructured as it is changed. One way to overcome this problem is refactoring. Refactoring is an approach which reduces the software complexity by incrementally improving internal software quality. Our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity. We propose a machine learning based model to predict classes to be refactored. We use Weighted Na{""\i}ve Bayes with InfoGain heuristic as the learner and we conducted experiments with metric data that we collected from the largest GSM operator in Turkey. Our results showed that we can predict 82% of the classes that need refactoring with 13% of manual inspection effort on the average.",0,"An expert system for determining candidate software classes for refactoring. In the lifetime of a software product, development costs are only the tip of the iceberg. Nearly 90% of the cost is maintenance due to error correction, adaptation and mainly enhancements. As Lehman and Belady [Lehman, M. M., & Belady, L. A. (1985). Program evolution: Processes of software change. Academic Press Professional.] state that software will become increasingly unstructured as it is changed. One way to overcome this problem is refactoring. Refactoring is an approach which reduces the software complexity by incrementally improving internal software quality. Our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity. We propose a machine learning based model to predict classes to be refactored. We use Weighted Na{""\i}ve Bayes with InfoGain heuristic as the learner and we conducted experiments with metric data that we collected from the largest GSM operator in Turkey. Our results showed that we can predict 82% of the classes that need refactoring with 13% of manual inspection effort on the average.",0,3,2,0,0,2
133,Toward a new aspect-mining approach for multi-agent systems,"Multi-agent systems, Aspect mining, Aspect-oriented programming","Many aspect mining techniques have been proposed for object-oriented systems. Unfortunately, aspect mining for multi-agent systems is an unexplored research area. The inherent specificities of multi-agent systems (such as autonomy, pro-activity, reactivity, and adaptability) make it difficult to understand, reuse and maintain their code. We propose, in this paper, a (semi-automatic) hybrid aspect mining approach for agent-oriented code. The technique is based on both static and dynamic analyzes. The main motivations of this work are (1) identifying cross-cutting concerns in existing agent-oriented code, and (2) making them explicitly available to software engineers involved in the evolution of agent-oriented code in order to facilitate its refactoring and, consequently, to improve its understandability, reusability and maintainability. The proposed approach is supported by a software tool, called MAMIT (MAS Aspect-MIning Tool), that we developed. The approach and the associated tool are illustrated using a concrete case study.",0,"Toward a new aspect-mining approach for multi-agent systems. Many aspect mining techniques have been proposed for object-oriented systems. Unfortunately, aspect mining for multi-agent systems is an unexplored research area. The inherent specificities of multi-agent systems (such as autonomy, pro-activity, reactivity, and adaptability) make it difficult to understand, reuse and maintain their code. We propose, in this paper, a (semi-automatic) hybrid aspect mining approach for agent-oriented code. The technique is based on both static and dynamic analyzes. The main motivations of this work are (1) identifying cross-cutting concerns in existing agent-oriented code, and (2) making them explicitly available to software engineers involved in the evolution of agent-oriented code in order to facilitate its refactoring and, consequently, to improve its understandability, reusability and maintainability. The proposed approach is supported by a software tool, called MAMIT (MAS Aspect-MIning Tool), that we developed. The approach and the associated tool are illustrated using a concrete case study.",0,3,1,1,1,1
134,Building an expert system to assist system refactorization,"Interface agents, Expert systems, Aspect-oriented software development, Aspect refactoring","The separation of concerns is an important issue in the building of maintenable systems. Aspect oriented programming (AOP) is a software paradigm that allows the encapsulation of those concerns that crosscut a system and can not be modularized using current paradigms such as object-oriented programming. In this way, AOP increases the software modularization and reduces the impact when changes are made in the system. In order to take advantage of the benefits of AOP, the legacy OO systems should be migrated.. To migrate object-oriented systems to aspect-oriented ones, specific refactorings for aspects should be used. This is a complex and tedious task for the developer because he/she needs to know how the refactorings should be applied and under what context. Therefore, it is desirable to have tools that help him/her through the process. In this article, we present an expert software agent, named RefactoringRecommender, that assists the developer during a refactorization of a system. The agent uses a Markovian algorithm with the goal of predicting the needed restructurings.",0,"Building an expert system to assist system refactorization. The separation of concerns is an important issue in the building of maintenable systems. Aspect oriented programming (AOP) is a software paradigm that allows the encapsulation of those concerns that crosscut a system and can not be modularized using current paradigms such as object-oriented programming. In this way, AOP increases the software modularization and reduces the impact when changes are made in the system. In order to take advantage of the benefits of AOP, the legacy OO systems should be migrated.. To migrate object-oriented systems to aspect-oriented ones, specific refactorings for aspects should be used. This is a complex and tedious task for the developer because he/she needs to know how the refactorings should be applied and under what context. Therefore, it is desirable to have tools that help him/her through the process. In this article, we present an expert software agent, named RefactoringRecommender, that assists the developer during a refactorization of a system. The agent uses a Markovian algorithm with the goal of predicting the needed restructurings.",0,3,0,0,3,4
135,Chapter 6 - A Decision-Support System Approach to Economics-Driven Modularity Evaluation,"Software Modularity, Software Architecture, Technical Debt, Refactoring, Real options, Complexity Metrics, Effort Measures","Modularity debt is the most difficult kind of technical debt to quantify and manage. Modularity decay, thus modularity debt, causes huge losses over time in terms of reduced ability to provide new functionality and fix bugs, operational failures, and even canceled projects. As modularity debt accumulates over time, software system managers are often faced with a challenging task of deciding when and whether to refactor, for example, choosing to improve modularity or not. While the costs of refactoring are significant and immediate, their benefits are largely invisible, intangible, and long term. Existing research lacks effective methods to quantify the costs and benefits of refactoring to support refactoring decision making. In this chapter, we present a decision-support system (DSS) approach to the modularity debt management. Using such a system, managers would be able to play out various ``what-if'' scenarios to make informed decisions regarding refactoring. Our DSS approach is built on a scientific foundation for explicitly manifesting the economic implications of software refactoring activities so that the costs and benefits of such activities can be understood, analyzed, and predicted. We discuss our contributions and current progress in developing the building blocks and the underpinning framework, an integrated economics-driven modularization evaluation framework, for the modularity debt management decision-support system (MDM-DSS).",0,"Chapter 6 - A Decision-Support System Approach to Economics-Driven Modularity Evaluation. Modularity debt is the most difficult kind of technical debt to quantify and manage. Modularity decay, thus modularity debt, causes huge losses over time in terms of reduced ability to provide new functionality and fix bugs, operational failures, and even canceled projects. As modularity debt accumulates over time, software system managers are often faced with a challenging task of deciding when and whether to refactor, for example, choosing to improve modularity or not. While the costs of refactoring are significant and immediate, their benefits are largely invisible, intangible, and long term. Existing research lacks effective methods to quantify the costs and benefits of refactoring to support refactoring decision making. In this chapter, we present a decision-support system (DSS) approach to the modularity debt management. Using such a system, managers would be able to play out various ``what-if'' scenarios to make informed decisions regarding refactoring. Our DSS approach is built on a scientific foundation for explicitly manifesting the economic implications of software refactoring activities so that the costs and benefits of such activities can be understood, analyzed, and predicted. We discuss our contributions and current progress in developing the building blocks and the underpinning framework, an integrated economics-driven modularization evaluation framework, for the modularity debt management decision-support system (MDM-DSS).",0,2,0,0,0,3
136,Coordinated Distributed Diagram Transformation for Software Evolution1 1Partially supported by the EC under Research and Training Network SeGraVis.,,"We present an approach to maintaining consistency between code and specification during refactoring, where a specification comprises several UML diagrams of different types. Code is represented as a flowgraph, and the flowgraph and UML diagrams constitute different views of a software system. A refactoring is modelled as a set of distributed graph transformations, organized into transformation units.",0,"Coordinated Distributed Diagram Transformation for Software Evolution1 1Partially supported by the EC under Research and Training Network SeGraVis.. We present an approach to maintaining consistency between code and specification during refactoring, where a specification comprises several UML diagrams of different types. Code is represented as a flowgraph, and the flowgraph and UML diagrams constitute different views of a software system. A refactoring is modelled as a set of distributed graph transformations, organized into transformation units.",1,1,0,0,1,4
137,A proof system for adaptable class hierarchies,"Software evolution, Object orientation, Verification, Proof systems, Class updates, Dynamic code modification","The code base of a software system undergoes changes during its life time. For object-oriented languages, classes are adapted, e.g., to meet new requirements, customize the software to specific user functionalities, or refactor the code to reduce its complexity. However, the adaptation of class hierarchies makes reasoning about program behavior challenging; even classes in the middle of a class hierarchy can be modified. This paper develops a proof system for analyzing the effect of operations to adapt classes, in the context of method overriding and late bound method calls. The proof system is incremental in the sense that reverification is avoided for methods that are not explicitly changed by adaptations. Furthermore, the possible adaptations are not unduly restricted; i.e., flexibility is retained without compromising on reasoning control. To achieve this balance, we extend the mechanism of lazy behavioral subtyping, originally proposed for reasoning about inheritance when subclasses are added to a class hierarchy, to deal with the more general situation of adaptable class hierarchies and changing specifications. The reasoning system distinguishes guaranteed method behavior from requirements toward methods, and achieves incremental reasoning by tracking guarantees and requirements in adaptable class hierarchies.. We show soundness of the proposed proof system.",0,"A proof system for adaptable class hierarchies. The code base of a software system undergoes changes during its life time. For object-oriented languages, classes are adapted, e.g., to meet new requirements, customize the software to specific user functionalities, or refactor the code to reduce its complexity. However, the adaptation of class hierarchies makes reasoning about program behavior challenging; even classes in the middle of a class hierarchy can be modified. This paper develops a proof system for analyzing the effect of operations to adapt classes, in the context of method overriding and late bound method calls. The proof system is incremental in the sense that reverification is avoided for methods that are not explicitly changed by adaptations. Furthermore, the possible adaptations are not unduly restricted; i.e., flexibility is retained without compromising on reasoning control. To achieve this balance, we extend the mechanism of lazy behavioral subtyping, originally proposed for reasoning about inheritance when subclasses are added to a class hierarchy, to deal with the more general situation of adaptable class hierarchies and changing specifications. The reasoning system distinguishes guaranteed method behavior from requirements toward methods, and achieves incremental reasoning by tracking guarantees and requirements in adaptable class hierarchies.. We show soundness of the proposed proof system.",0,1,0,0,1,4
138,Refactoring of Execution Control Charts in Basic Function Blocks of the IEC 61499 Standard,"refactoring, IEC 61499, graph transformation, function block, discrete control",This paper deals with refactoring of execution control charts of IEC 61499 basic function blocks as a means to improve the engineering support potential of the standard in development of industrial control applications. The main purpose of the refactoring is removal of arcs without event inputs and getting rid of potential deadlock states. The ECC refactoring is implemented as a set of graph transformation rules. A prototype has been implemented using the AGG software tool.,0,Refactoring of Execution Control Charts in Basic Function Blocks of the IEC 61499 Standard. This paper deals with refactoring of execution control charts of IEC 61499 basic function blocks as a means to improve the engineering support potential of the standard in development of industrial control applications. The main purpose of the refactoring is removal of arcs without event inputs and getting rid of potential deadlock states. The ECC refactoring is implemented as a set of graph transformation rules. A prototype has been implemented using the AGG software tool.,1,1,2,0,3,0
139,Comparing approaches to analyze refactoring activity on software repositories,"Refactoring, Repository, Manual analysis, Automated analysis","Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers' productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation.",0,"Comparing approaches to analyze refactoring activity on software repositories. Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers' productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation.",0,2,2,1,3,2
140,Identifying refactoring opportunities in process model repositories,"Business Process Model, Refactoring, Repository","Context
In order to ensure high quality of a process model repository, refactoring operations can be applied to correct anti-patterns, such as overlap of process models, inconsistent labeling of activities and overly complex models. However, if a process model collection is created and maintained by different people over a longer period of time, manual detection of such refactoring opportunities becomes difficult, simply due to the number of processes in the repository. Consequently, there is a need for techniques to detect refactoring opportunities automatically.
Objective
This paper proposes a technique for automatically detecting refactoring opportunities.
Method
We developed the technique based on metrics that can be used to measure the consistency of activity labels as well as the extent to which processes overlap and the type of overlap that they have. We evaluated it, by applying it to two large process model repositories.
Results
The evaluation shows that the technique can be used to pinpoint the approximate location of three types of refactoring opportunities with high precision and recall and of one type of refactoring opportunity with high recall, but low precision.
Conclusion
We conclude that the technique presented in this paper can be used in practice to automatically detect a number of anti-patterns that can be corrected by refactoring.",0,"Identifying refactoring opportunities in process model repositories. Context
In order to ensure high quality of a process model repository, refactoring operations can be applied to correct anti-patterns, such as overlap of process models, inconsistent labeling of activities and overly complex models. However, if a process model collection is created and maintained by different people over a longer period of time, manual detection of such refactoring opportunities becomes difficult, simply due to the number of processes in the repository. Consequently, there is a need for techniques to detect refactoring opportunities automatically.
Objective
This paper proposes a technique for automatically detecting refactoring opportunities.
Method
We developed the technique based on metrics that can be used to measure the consistency of activity labels as well as the extent to which processes overlap and the type of overlap that they have. We evaluated it, by applying it to two large process model repositories.
Results
The evaluation shows that the technique can be used to pinpoint the approximate location of three types of refactoring opportunities with high precision and recall and of one type of refactoring opportunity with high recall, but low precision.
Conclusion
We conclude that the technique presented in this paper can be used in practice to automatically detect a number of anti-patterns that can be corrected by refactoring.",0,2,1,2,2,0
141,Simplifying process model abstraction: Techniques for generating model names,"Business process model, Model name, Process model repository, Refactoring, Model abstraction","The increased adoption of business process management approaches, tools, and practices has led organizations to accumulate large collections of business process models. These collections can easily include from a hundred to a thousand models, especially in the context of multinational corporations or as a result of organizational mergers and acquisitions. A concrete problem is thus how to maintain these large repositories in such a way that their complexity does not hamper their practical usefulness as a means to describe and communicate business operations. This paper proposes a technique to automatically infer suitable names for business process models and fragments thereof. This technique is useful for model abstraction scenarios, as for instance when user-specific views of a repository are required, or as part of a refactoring initiative aimed to simplify the repository's complexity. The technique is grounded in an adaptation of the theory of meaning to the realm of business process models. We implemented the technique in a prototype tool and conducted an extensive evaluation using three process model collections from practice and a case study involving process modelers with different experience.",0,"Simplifying process model abstraction: Techniques for generating model names. The increased adoption of business process management approaches, tools, and practices has led organizations to accumulate large collections of business process models. These collections can easily include from a hundred to a thousand models, especially in the context of multinational corporations or as a result of organizational mergers and acquisitions. A concrete problem is thus how to maintain these large repositories in such a way that their complexity does not hamper their practical usefulness as a means to describe and communicate business operations. This paper proposes a technique to automatically infer suitable names for business process models and fragments thereof. This technique is useful for model abstraction scenarios, as for instance when user-specific views of a repository are required, or as part of a refactoring initiative aimed to simplify the repository's complexity. The technique is grounded in an adaptation of the theory of meaning to the realm of business process models. We implemented the technique in a prototype tool and conducted an extensive evaluation using three process model collections from practice and a case study involving process modelers with different experience.",0,3,2,0,2,1
142,A Static Semantics for Alloy and its Impact in Refactorings,"refactoring, type system, theorem proving, object models","Refactorings are usually proposed in an ad hoc way because it is difficult to prove that they are sound with respect to a formal semantics, not guaranteeing the absence of type errors or semantic changes. Consequently, developers using refactoring tools must rely on compilation and tests to ensure type-correctness and semantics preservation, respectively, which may not be satisfactory to critical software development. In this paper, we formalize a static semantics for Alloy, which is a formal object-oriented modeling language, and encode it in Prototype Verification System (PVS). The static semantics' formalization can be useful for specifying and proving that transformations in general (not only refactorings) do not introduce type errors, for instance, as we show here.",0,"A Static Semantics for Alloy and its Impact in Refactorings. Refactorings are usually proposed in an ad hoc way because it is difficult to prove that they are sound with respect to a formal semantics, not guaranteeing the absence of type errors or semantic changes. Consequently, developers using refactoring tools must rely on compilation and tests to ensure type-correctness and semantics preservation, respectively, which may not be satisfactory to critical software development. In this paper, we formalize a static semantics for Alloy, which is a formal object-oriented modeling language, and encode it in Prototype Verification System (PVS). The static semantics' formalization can be useful for specifying and proving that transformations in general (not only refactorings) do not introduce type errors, for instance, as we show here.",1,1,0,0,3,4
143,CScout: A refactoring browser for C,"C, Browser, Refactoring, Preprocessor","Despite its maturity and popularity, the C programming language still lacks tool support for reliably performing even simple refactoring, browsing, or analysis operations. This is primarily due to identifier scope complications introduced by the C preprocessor. The CScout refactoring browser analyses complete program families by tagging the original identifiers with their precise location and classifying them into equivalence classes orthogonal to the C language's namespace and scope extents. A web-based user interface provides programmers with an intuitive source code analysis and navigation front-end, while an sql-based back-end allows more complex source code analysis and manipulation. CScout has been successfully applied to many medium and large-sized proprietary and open-source projects identifying thousands of modest refactoring opportunities.",0,"CScout: A refactoring browser for C. Despite its maturity and popularity, the C programming language still lacks tool support for reliably performing even simple refactoring, browsing, or analysis operations. This is primarily due to identifier scope complications introduced by the C preprocessor. The CScout refactoring browser analyses complete program families by tagging the original identifiers with their precise location and classifying them into equivalence classes orthogonal to the C language's namespace and scope extents. A web-based user interface provides programmers with an intuitive source code analysis and navigation front-end, while an sql-based back-end allows more complex source code analysis and manipulation. CScout has been successfully applied to many medium and large-sized proprietary and open-source projects identifying thousands of modest refactoring opportunities.",1,1,0,0,1,0
144,A unit test approach for database schema evolution,"Database schema evolution, Database testing, Unit testing","Context
The constant changes in today's business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution.
Objective
This work examines the challenges for database schema evolution and explores the possibility of using various testing strategies to assist with schema evolution. Specifically, the work proposes a novel unit test approach for the application code that accesses databases with the objective of proactively evaluating the code against the altered database.
Method
The approach was validated through the implementation of a testing framework in conjunction with a sample application and a relatively simple database schema. Although the database schema in this study was simple, it was nevertheless able to demonstrate the advantages of the proposed approach.
Results
After changes in the database schema, the proposed approach found all SELECT statements as well as the majority of other statements requiring modifications in the application code. Due to its efficiency with SELECT statements, the proposed approach is expected to be more successful with database warehouse applications where SELECT statements are dominant.
Conclusion
The unit test approach that accesses databases has proven to be successful in evaluating the application code against the evolved database. In particular, the approach is simple and straightforward to implement, which makes it easily adoptable in practice.",0,"A unit test approach for database schema evolution. Context
The constant changes in today's business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution.
Objective
This work examines the challenges for database schema evolution and explores the possibility of using various testing strategies to assist with schema evolution. Specifically, the work proposes a novel unit test approach for the application code that accesses databases with the objective of proactively evaluating the code against the altered database.
Method
The approach was validated through the implementation of a testing framework in conjunction with a sample application and a relatively simple database schema. Although the database schema in this study was simple, it was nevertheless able to demonstrate the advantages of the proposed approach.
Results
After changes in the database schema, the proposed approach found all SELECT statements as well as the majority of other statements requiring modifications in the application code. Due to its efficiency with SELECT statements, the proposed approach is expected to be more successful with database warehouse applications where SELECT statements are dominant.
Conclusion
The unit test approach that accesses databases has proven to be successful in evaluating the application code against the evolved database. In particular, the approach is simple and straightforward to implement, which makes it easily adoptable in practice.",0,2,1,2,2,4
145,Search-based refactoring for software maintenance,"Search-based software engineering, Automated design improvement, Refactoring","The high cost of software maintenance could be reduced by automatically improving the design of object-oriented programs without altering their behaviour. We have constructed a software tool capable of refactoring object-oriented programs to conform more closely to a given design quality model, by formulating the task as a search problem in the space of alternative designs. This novel approach is validated by two case studies, where programs are automatically refactored to increase flexibility, reusability and understandability as defined by a contemporary quality model. Both local and simulated annealing searches were found to be effective in this task..",0,"Search-based refactoring for software maintenance. The high cost of software maintenance could be reduced by automatically improving the design of object-oriented programs without altering their behaviour. We have constructed a software tool capable of refactoring object-oriented programs to conform more closely to a given design quality model, by formulating the task as a search problem in the space of alternative designs. This novel approach is validated by two case studies, where programs are automatically refactored to increase flexibility, reusability and understandability as defined by a contemporary quality model. Both local and simulated annealing searches were found to be effective in this task..",0,3,1,1,3,2
146,A study of cyclic dependencies on defect profile of software components,"Dependency cycle, Defects, Defect-prone components","Background
Empirical evidence shows that dependency cycles among software components are pervasive in real-life software systems, although such cycles are known to be detrimental to software quality attributes such as understandability, testability, reusability, build-ability and maintainability.
Research goals
Can the use of extended object-oriented metrics make us better understand the relationships among cyclic related components and their defect-proneness?
Approach
First, we extend such metrics to mine and classify software components into two groups -- the cyclic and the non-cyclic ones. Next, we have performed an empirical study of six software applications. Using standard statistical tests on four different hypotheses, we have determined the significance of the defect profiles of both groups.
Results
Our results show that most defects and defective components are concentrated in cyclic-dependent components, either directly or indirectly.
Discussion and conclusion
These results have important implications for software maintenance and system testing. By identifying the most defect-prone set in a software system, it is possible to effectively allocate testing resources in a cost efficient manner. Based on these results, we demonstrate how additional structural properties could be collected to understand component's defect proneness and aid decision process in refactoring defect-prone cyclic related components.",0,"A study of cyclic dependencies on defect profile of software components. Background
Empirical evidence shows that dependency cycles among software components are pervasive in real-life software systems, although such cycles are known to be detrimental to software quality attributes such as understandability, testability, reusability, build-ability and maintainability.
Research goals
Can the use of extended object-oriented metrics make us better understand the relationships among cyclic related components and their defect-proneness?
Approach
First, we extend such metrics to mine and classify software components into two groups -- the cyclic and the non-cyclic ones. Next, we have performed an empirical study of six software applications. Using standard statistical tests on four different hypotheses, we have determined the significance of the defect profiles of both groups.
Results
Our results show that most defects and defective components are concentrated in cyclic-dependent components, either directly or indirectly.
Discussion and conclusion
These results have important implications for software maintenance and system testing. By identifying the most defect-prone set in a software system, it is possible to effectively allocate testing resources in a cost efficient manner. Based on these results, we demonstrate how additional structural properties could be collected to understand component's defect proneness and aid decision process in refactoring defect-prone cyclic related components.",0,2,2,0,0,3
147,The effect of refactoring on change and fault-proneness in commercial C# software,"Refactoring, Change-proneness, Fault analysis, Evolution","Refactoring is a process for improving the internal characteristics and design of software while preserving its external behaviour. Refactoring has been suggested as a positive influence on the long-term quality and maintainability of software and, as a result, we might expect benefits of a lower future change or fault propensity by refactoring software. Conversely, many studies show a correlation between change and future faults; so application of a refactoring may in itself increase future fault propensity, negating any benefit of the refactoring. In this paper, we determine whether the refactoring process reaps future maintenance benefits and, as a consequence, results in software with a lower propensity for both faults and change. We studied a large, commercial software system over a twelve-month period and identified a set of refactored classes during the middle four months of the study; a bespoke tool was used to detect occurrences of fifteen types of refactoring. We then examined the fault- and change-proneness of the same set of refactored classes in the four months prior to, during, and after the period of refactoring to determine if change or fault activity was reduced either during or after the period of refactoring studied. We also compared these trends with remaining classes in the system that had not been refactored over the same periods. Results revealed that refactored classes experienced a lower change-proneness in the period after refactoring and were significantly less fault-prone during and after the period of refactoring, even when accounting for the effects of change. The study therefore presents concrete evidence of the benefits of refactoring in these two senses.",0,"The effect of refactoring on change and fault-proneness in commercial C# software. Refactoring is a process for improving the internal characteristics and design of software while preserving its external behaviour. Refactoring has been suggested as a positive influence on the long-term quality and maintainability of software and, as a result, we might expect benefits of a lower future change or fault propensity by refactoring software. Conversely, many studies show a correlation between change and future faults; so application of a refactoring may in itself increase future fault propensity, negating any benefit of the refactoring. In this paper, we determine whether the refactoring process reaps future maintenance benefits and, as a consequence, results in software with a lower propensity for both faults and change. We studied a large, commercial software system over a twelve-month period and identified a set of refactored classes during the middle four months of the study; a bespoke tool was used to detect occurrences of fifteen types of refactoring. We then examined the fault- and change-proneness of the same set of refactored classes in the four months prior to, during, and after the period of refactoring to determine if change or fault activity was reduced either during or after the period of refactoring studied. We also compared these trends with remaining classes in the system that had not been refactored over the same periods. Results revealed that refactored classes experienced a lower change-proneness in the period after refactoring and were significantly less fault-prone during and after the period of refactoring, even when accounting for the effects of change. The study therefore presents concrete evidence of the benefits of refactoring in these two senses.",0,2,0,0,3,2
148,Ontological anti-patterns: empirically uncovered error-prone structures in ontology-driven conceptual models,"Ontology-driven conceptual modeling, Ontological anti-patterns, OntoUML, UFO","The construction of large-scale reference conceptual models is a complex engineering activity. To develop high-quality models, a modeler must have the support of expressive engineering tools such as theoretically well-founded modeling languages and methodologies, patterns and anti-patterns and automated supporting environments. This paper proposes a set of Ontological Anti-Patterns for Ontology-Driven Conceptual Modeling. These anti-patterns capture error-prone modeling decisions that can result in the creation of models that fail to exclude unintended model instances (representing unintended state of affairs) or forbid intended ones (representing intended states of affairs). The anti-patterns presented here have been empirically elicited through an approach of conceptual models validation via visual simulation. The paper also presents a series of refactoring plans for rectifying the models in which these anti-patterns occur. In addition, we present here a computational tool that is able to: automatically identify these anti-patterns in user's models, guide users in assessing their consequences, and generate corrections to these models by the automatic inclusion of OCL constraints implementing the proposed refactoring plans. Finally, the paper also presents an empirical study for assessing the harmfulness of each of the uncovered anti-patterns (i.e., the likelihood that its occurrence in a model entails unintended consequences) as well as the effectiveness of the proposed refactoring plans.",0,"Ontological anti-patterns: empirically uncovered error-prone structures in ontology-driven conceptual models. The construction of large-scale reference conceptual models is a complex engineering activity. To develop high-quality models, a modeler must have the support of expressive engineering tools such as theoretically well-founded modeling languages and methodologies, patterns and anti-patterns and automated supporting environments. This paper proposes a set of Ontological Anti-Patterns for Ontology-Driven Conceptual Modeling. These anti-patterns capture error-prone modeling decisions that can result in the creation of models that fail to exclude unintended model instances (representing unintended state of affairs) or forbid intended ones (representing intended states of affairs). The anti-patterns presented here have been empirically elicited through an approach of conceptual models validation via visual simulation. The paper also presents a series of refactoring plans for rectifying the models in which these anti-patterns occur. In addition, we present here a computational tool that is able to: automatically identify these anti-patterns in user's models, guide users in assessing their consequences, and generate corrections to these models by the automatic inclusion of OCL constraints implementing the proposed refactoring plans. Finally, the paper also presents an empirical study for assessing the harmfulness of each of the uncovered anti-patterns (i.e., the likelihood that its occurrence in a model entails unintended consequences) as well as the effectiveness of the proposed refactoring plans.",0,2,2,1,2,1
149,Software refactoring at the function level using new Adaptive K-Nearest Neighbor algorithm,"Software refactoring, Clustering, Cohesion, Code restructuring, Function level cohesion, Software quality","Improving the quality of software is a vital target of software engineering. Constantly evolving requirements force software developers to enhance, modify, or adapt software. Thus, increasing internal complexity, maintenance effort, and ultimately cost. In trying to balance between the needs to change software, maintain high quality, and keep the maintenance effort and cost low, refactoring comes up as a solution. Refactoring aims to improve a number of quality factors, among which is understandability. Enhancing understandability of ill-structured software decreases the maintenance effort and cost. To improve understandability, designers need to maximize cohesion and minimize coupling, which becomes more difficult to achieve as software evolves and internal complexity increases. In this paper, we propose a new Adaptive K-Nearest Neighbor (A-KNN) algorithm to perform clustering with different attribute weights. The technique is used to assist software developers in refactoring at the function/method level. This is achieved by identifying ill-structured software entities and providing suggestions to improve cohesion of such entities. We also compare the proposed technique with three function-level clustering techniques Single Linkage algorithm (SLINK), Complete Linkage algorithm (CLINK) and Weighted Pair-Group Method using Arithmetic averages (WPGMA). A-KNN showed competitive performance with the other three algorithms and required less computational complexity.",0,"Software refactoring at the function level using new Adaptive K-Nearest Neighbor algorithm. Improving the quality of software is a vital target of software engineering. Constantly evolving requirements force software developers to enhance, modify, or adapt software. Thus, increasing internal complexity, maintenance effort, and ultimately cost. In trying to balance between the needs to change software, maintain high quality, and keep the maintenance effort and cost low, refactoring comes up as a solution. Refactoring aims to improve a number of quality factors, among which is understandability. Enhancing understandability of ill-structured software decreases the maintenance effort and cost. To improve understandability, designers need to maximize cohesion and minimize coupling, which becomes more difficult to achieve as software evolves and internal complexity increases. In this paper, we propose a new Adaptive K-Nearest Neighbor (A-KNN) algorithm to perform clustering with different attribute weights. The technique is used to assist software developers in refactoring at the function/method level. This is achieved by identifying ill-structured software entities and providing suggestions to improve cohesion of such entities. We also compare the proposed technique with three function-level clustering techniques Single Linkage algorithm (SLINK), Complete Linkage algorithm (CLINK) and Weighted Pair-Group Method using Arithmetic averages (WPGMA). A-KNN showed competitive performance with the other three algorithms and required less computational complexity.",0,2,2,1,3,2
150,Software rejuvenation via a multi-agent approach,"Software rejuvenation, Refactoring, Multi-agent systems","Usually, development teams devote a huge amount of time and effort on maintaining existing software. Since many of these maintenance tasks are not planned, the software tends to degrade over time, causing side effects mainly on its non-functional requirements. This paper proposes the use of a multi-agent system in order to perform perfective maintenance tasks in a software product through refactorings. The software developer chooses the quality attribute that the agents should improve and the agents are able to autonomously search the code for opportunities to apply perfective maintenance, apply the perfective maintenance, and evaluate if the source code quality has been improved. Its main contributions are: (i) the refactorings are autonomously done by software agents during the idle development time; (ii) all changes are stored in isolated branches in order to facilitate the communication with the developers; (iii) the refactorings are applied only when the program semantics is preserved; (iv) the agents are able to learn the more suitable sequence of refactorings to improve a specific quality attribute; and (v) the approach can be extended with other metrics and refactorings. This paper also presents a set of experimental studies that provide evidences of the benefits of our approach for software rejuvenation.",0,"Software rejuvenation via a multi-agent approach. Usually, development teams devote a huge amount of time and effort on maintaining existing software. Since many of these maintenance tasks are not planned, the software tends to degrade over time, causing side effects mainly on its non-functional requirements. This paper proposes the use of a multi-agent system in order to perform perfective maintenance tasks in a software product through refactorings. The software developer chooses the quality attribute that the agents should improve and the agents are able to autonomously search the code for opportunities to apply perfective maintenance, apply the perfective maintenance, and evaluate if the source code quality has been improved. Its main contributions are: (i) the refactorings are autonomously done by software agents during the idle development time; (ii) all changes are stored in isolated branches in order to facilitate the communication with the developers; (iii) the refactorings are applied only when the program semantics is preserved; (iv) the agents are able to learn the more suitable sequence of refactorings to improve a specific quality attribute; and (v) the approach can be extended with other metrics and refactorings. This paper also presents a set of experimental studies that provide evidences of the benefits of our approach for software rejuvenation.",0,2,2,0,0,2
151,Chapter 3 - Aspects of Software Valuation,"Business Decision Making, Economic Analysis, Cash Flow, Software Capitalization","Basing software technical decisions on economics has not been generally recognized as important in the software industry. However, most software is being developed and maintained in a business context, so the business consequences of technical choices must be considered. A commercial software development project, a low-cost payroll processing application targeted at small businesses, is used to introduce the basic elements of economic analysis for software projects. Development of the software in an Automated Test Equipment (ATE) system is then used to illustrate the same elements for internally developed software. Next the approach is generalized using a decision to invest in refactoring an existing Customer Relationship Management (CRM) code base as an example. Finally, the decision to capitalize software development costs is analyzed to illustrate that it is not as good an idea as some people think it is. Using the techniques described in this chapter, software professionals will be able to evaluate the business consequences of their technical alternatives and, thus, be able to make appropriate business decisions about their software projects.",0,"Chapter 3 - Aspects of Software Valuation. Basing software technical decisions on economics has not been generally recognized as important in the software industry. However, most software is being developed and maintained in a business context, so the business consequences of technical choices must be considered. A commercial software development project, a low-cost payroll processing application targeted at small businesses, is used to introduce the basic elements of economic analysis for software projects. Development of the software in an Automated Test Equipment (ATE) system is then used to illustrate the same elements for internally developed software. Next the approach is generalized using a decision to invest in refactoring an existing Customer Relationship Management (CRM) code base as an example. Finally, the decision to capitalize software development costs is analyzed to illustrate that it is not as good an idea as some people think it is. Using the techniques described in this chapter, software professionals will be able to evaluate the business consequences of their technical alternatives and, thus, be able to make appropriate business decisions about their software projects.",0,3,0,0,0,3
152,Observations on the assured evolution of concurrent Java programs,"Java, Concurrency, Program assurance, Static analysis, Program transformation, Refactoring, Program evolution","Evolving and refactoring concurrent Java software can be error-prone, resulting in race conditions and other concurrency difficulties. We suggest that there are two principal causes: concurrency design intent is often not explicit in code and, additionally, consistency of intent and code cannot easily be established through either testing or inspection. We explore several aspects of this issue in this paper. First, we describe a tool-assisted approach to modeling and assurance for concurrent programs. Second, we give an account of recent case study experience on larger-scale production Java systems. Third, we suggest an approach to scalable co-evolution of code and models that is designed to support working programmers without special training or incentives. Fourth, we propose some concurrency-related refactorings that, with suitable analysis and tool support, can potentially offer assurances of soundness.",0,"Observations on the assured evolution of concurrent Java programs. Evolving and refactoring concurrent Java software can be error-prone, resulting in race conditions and other concurrency difficulties. We suggest that there are two principal causes: concurrency design intent is often not explicit in code and, additionally, consistency of intent and code cannot easily be established through either testing or inspection. We explore several aspects of this issue in this paper. First, we describe a tool-assisted approach to modeling and assurance for concurrent programs. Second, we give an account of recent case study experience on larger-scale production Java systems. Third, we suggest an approach to scalable co-evolution of code and models that is designed to support working programmers without special training or incentives. Fourth, we propose some concurrency-related refactorings that, with suitable analysis and tool support, can potentially offer assurances of soundness.",1,3,2,1,3,2
153,Recovering test-to-code traceability using slicing and textual analysis,"Test-to-code traceability, Dynamic slicing, Information retrieval","Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature.",0,"Recovering test-to-code traceability using slicing and textual analysis. Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature.",0,2,2,0,2,0
154,21 - Design System Architecture,,"Publisher Summary
This chapter focuses on the design system architecture used in the development of trading and investment system. Software and hardware design transforms the requirements into the technologically feasible solutions and selecting optimal architectures. Design must be documented either with team-specific notations, or more formally with the Unified Modeling Language (UML) to view use cases, object models, and other components. In the Stage 3 of development process, the software engineers are focuses their attention solely on technology design. In this step, the development team translates requirements into feasible, high-level design alternatives, which reduce development time, software errors, and bugs, as well as minimize the need for additional resources for refactoring and fixes later on. Most fully functioning trading/investment systems make use of a multitier architecture. The multitier designs promote modular software development with well-defined interfaces. The main advantage to multitier design is that it allows developers to replace or refactor any individual tier as needed without affecting the other tiers. Multitier architectures also facilitate software design since each tier is built, tested, and run on its own separate platform. This allows the development team to program different tiers in different languages such as the graphical user interface language for the top tier; C/C++ for the middle tier; and SQL for much of the database tier. Design starts with domain analysis, which models the interactions of a trader, real-time and historical data, and the system software in use cases. The use of the ATAM method for evaluation and the creation of a software architecture document as a primary deliverable for Gate 3 are recommended.",0,"21 - Design System Architecture. Publisher Summary
This chapter focuses on the design system architecture used in the development of trading and investment system. Software and hardware design transforms the requirements into the technologically feasible solutions and selecting optimal architectures. Design must be documented either with team-specific notations, or more formally with the Unified Modeling Language (UML) to view use cases, object models, and other components. In the Stage 3 of development process, the software engineers are focuses their attention solely on technology design. In this step, the development team translates requirements into feasible, high-level design alternatives, which reduce development time, software errors, and bugs, as well as minimize the need for additional resources for refactoring and fixes later on. Most fully functioning trading/investment systems make use of a multitier architecture. The multitier designs promote modular software development with well-defined interfaces. The main advantage to multitier design is that it allows developers to replace or refactor any individual tier as needed without affecting the other tiers. Multitier architectures also facilitate software design since each tier is built, tested, and run on its own separate platform. This allows the development team to program different tiers in different languages such as the graphical user interface language for the top tier; C/C++ for the middle tier; and SQL for much of the database tier. Design starts with domain analysis, which models the interactions of a trader, real-time and historical data, and the system software in use cases. The use of the ATAM method for evaluation and the creation of a software architecture document as a primary deliverable for Gate 3 are recommended.",0,3,2,0,0,1
155,AutoRefactoring: A platform to build refactoring agents,"Autonomous agents, Software refactoring, Smells detection, Smells correction, Code quality","Software maintenance may degrade the software quality. One of the primary ways to reduce undesired effects of maintenance is refactoring, which is a technique to improve software code quality without changing its observable behavior. To safely apply a refactoring, several issues must be considered: (i) identify the code parts that should be improved; (ii) determine the changes that must be applied to the code in order to improve its; (iii) evaluate the corrections impacts on code quality; and (iv) check that the observable behavior of the software will be preserved after applying the corrections. Given the amount of issues to consider, refactoring by hand has been assumed to be an expensive and error-prone task. Therefore, in this paper, we propose an agent-based platform that enables to implement an agent able to autonomously deal with the above mentioned refactoring issues. To evaluate our approach, we performed an empirical study on code smells detection and correction, code quality improvement and preservation of the software observable behavior. To answer our research questions, we analyze 5 releases of Java open source projects, ranging from 166 to 711 classes.",1,"AutoRefactoring: A platform to build refactoring agents. Software maintenance may degrade the software quality. One of the primary ways to reduce undesired effects of maintenance is refactoring, which is a technique to improve software code quality without changing its observable behavior. To safely apply a refactoring, several issues must be considered: (i) identify the code parts that should be improved; (ii) determine the changes that must be applied to the code in order to improve its; (iii) evaluate the corrections impacts on code quality; and (iv) check that the observable behavior of the software will be preserved after applying the corrections. Given the amount of issues to consider, refactoring by hand has been assumed to be an expensive and error-prone task. Therefore, in this paper, we propose an agent-based platform that enables to implement an agent able to autonomously deal with the above mentioned refactoring issues. To evaluate our approach, we performed an empirical study on code smells detection and correction, code quality improvement and preservation of the software observable behavior. To answer our research questions, we analyze 5 releases of Java open source projects, ranging from 166 to 711 classes.",0,2,2,1,3,2
156,A Hierarchical Program Representation for Refactoring,,"Currently there is a lot of interest in graph representations of software systems, as they provide a natural and flexible means to describe complex structures. The various visual sublanguages of the UML are perhaps the most obvious example of this. In [11] a graph representation of object-oriented programs was presented that enables one to describe refactoring operations (behaviour-preserving changes in the structure of a program) in a formal, concise way by graph rewriting productions. In general, however, a refactoring makes changes to a small part of a program, so the graph representation should only contain the information needed to carry out that refactoring. All other details are redundant and make the graph unnecessarily large for good visualization. A possible solution consists in using a hierarchical representation. Such a representation of object-oriented programs is presented in this paper. It is based on node-rewriting graph productions: each refinement step corresponds to a production. The construction is illustrated by applying it to a small Java simulation of a Local Area Network..",0,"A Hierarchical Program Representation for Refactoring. Currently there is a lot of interest in graph representations of software systems, as they provide a natural and flexible means to describe complex structures. The various visual sublanguages of the UML are perhaps the most obvious example of this. In [11] a graph representation of object-oriented programs was presented that enables one to describe refactoring operations (behaviour-preserving changes in the structure of a program) in a formal, concise way by graph rewriting productions. In general, however, a refactoring makes changes to a small part of a program, so the graph representation should only contain the information needed to carry out that refactoring. All other details are redundant and make the graph unnecessarily large for good visualization. A possible solution consists in using a hierarchical representation. Such a representation of object-oriented programs is presented in this paper. It is based on node-rewriting graph productions: each refinement step corresponds to a production. The construction is illustrated by applying it to a small Java simulation of a Local Area Network..",1,1,0,0,1,4
157,A test driven approach for aspectualizing legacy software using mock systems,"Mock systems, Aspect-oriented programming, Legacy systems, Refactoring, Testing","Aspect-based refactoring, called aspectualization, involves moving program code that implements cross-cutting concerns into aspects. Such refactoring can improve the maintainability of legacy systems. Long compilation and weave times, and the lack of an appropriate testing methodology are two challenges to the aspectualization of large legacy systems. We propose an iterative test driven approach for creating and introducing aspects. The approach uses mock systems that enable aspect developers to quickly experiment with different pointcuts and advice, and reduce the compile and weave times. The approach also uses weave analysis, regression testing, and code coverage analysis to test the aspects. We developed several tools for unit and integration testing. We demonstrate the test driven approach in the context of large industrial C++ systems, and we provide guidelines for mock system creation.",0,"A test driven approach for aspectualizing legacy software using mock systems. Aspect-based refactoring, called aspectualization, involves moving program code that implements cross-cutting concerns into aspects. Such refactoring can improve the maintainability of legacy systems. Long compilation and weave times, and the lack of an appropriate testing methodology are two challenges to the aspectualization of large legacy systems. We propose an iterative test driven approach for creating and introducing aspects. The approach uses mock systems that enable aspect developers to quickly experiment with different pointcuts and advice, and reduce the compile and weave times. The approach also uses weave analysis, regression testing, and code coverage analysis to test the aspects. We developed several tools for unit and integration testing. We demonstrate the test driven approach in the context of large industrial C++ systems, and we provide guidelines for mock system creation.",0,3,1,1,3,4
158,Chapter 5 - Action Research Can Swing the Balance in Experimental Software Engineering,"Action Research, Refactoring, study, Software Engineering Theory, Scientific Knowledge Management, Experimental Software Engineering","In general, professionals still ignore scientific evidence in place of expert opinions in most of their decision making. For this reason, it is still common to see the adoption of new software technologies in the field without any scientific basis or well-grounded criteria, but on the opinions of experts. Experimental Software Engineering is of paramount importance to provide the foundations to understand the limits and applicability of software technologies. The need to better observe and understand the practice of Software Engineering leads us to look for alternative experimental approaches to support our studies. Different research strategies can be used to explore different Software Engineering practices. Action Research can be seen as one alternative to intensify the conducting of important experimental studies with results of great value while investigating the Software Engineering practices in depth. In this chapter, a discussion on the use of Action Research in Software Engineering is presented. As indicated by a technical literature survey, along the years a growing tendency for addressing different research topics in Software Engineering through Action Research studies has been seen. This behavior can indicate the great potential of its applicability in our scientific field. Despite their clear benefits and diversity of application, the initial findings also revealed that the rigor and control of such studies should improve in Software Engineering. Aiming at better explaining the application of Action Research, an experimental study (in vivo) on the investigation of the subjective decisions of software developers, concerned with the refactoring of source code to improve source code quality in a distributed software development context is depicted.. A Software Engineering theory regarding refactoring and some guidance on how to accomplish an Action Research study in Software Engineering supplement the discussions in this chapter.",0,"Chapter 5 - Action Research Can Swing the Balance in Experimental Software Engineering. In general, professionals still ignore scientific evidence in place of expert opinions in most of their decision making. For this reason, it is still common to see the adoption of new software technologies in the field without any scientific basis or well-grounded criteria, but on the opinions of experts. Experimental Software Engineering is of paramount importance to provide the foundations to understand the limits and applicability of software technologies. The need to better observe and understand the practice of Software Engineering leads us to look for alternative experimental approaches to support our studies. Different research strategies can be used to explore different Software Engineering practices. Action Research can be seen as one alternative to intensify the conducting of important experimental studies with results of great value while investigating the Software Engineering practices in depth. In this chapter, a discussion on the use of Action Research in Software Engineering is presented. As indicated by a technical literature survey, along the years a growing tendency for addressing different research topics in Software Engineering through Action Research studies has been seen. This behavior can indicate the great potential of its applicability in our scientific field. Despite their clear benefits and diversity of application, the initial findings also revealed that the rigor and control of such studies should improve in Software Engineering. Aiming at better explaining the application of Action Research, an experimental study (in vivo) on the investigation of the subjective decisions of software developers, concerned with the refactoring of source code to improve source code quality in a distributed software development context is depicted.. A Software Engineering theory regarding refactoring and some guidance on how to accomplish an Action Research study in Software Engineering supplement the discussions in this chapter.",0,3,0,0,0,3
159,An empirical study of relationships among extreme programming engineering activities,"Extreme programming, Design evolution, Extreme programming engineering activities, Empirical study","Extreme programming (XP) is an agile software process that promotes early and quick production of working code. In this paper, we investigated the relationship among three XP engineering activities: new design, refactoring, and error fix. We found that the more the new design performed to the system the less refactoring and error fix were performed. However, the refactoring and error fix efforts did not seem to be related. We also found that the error fix effort is related to number of days spent on each story, while new design is not. The relationship between the refactoring effort and number of days spent on each story was not conclusive.",0,"An empirical study of relationships among extreme programming engineering activities. Extreme programming (XP) is an agile software process that promotes early and quick production of working code. In this paper, we investigated the relationship among three XP engineering activities: new design, refactoring, and error fix. We found that the more the new design performed to the system the less refactoring and error fix were performed. However, the refactoring and error fix efforts did not seem to be related. We also found that the error fix effort is related to number of days spent on each story, while new design is not. The relationship between the refactoring effort and number of days spent on each story was not conclusive.",0,2,2,0,3,2
160,A posteriori operation detection in evolving software models,"Model evolution, Model refactoring, Model comparison","As every software artifact, also software models are subject to continuous evolution. The operations applied between two successive versions of a model are crucial for understanding its evolution. Generic approaches for detecting operations a posteriori identify atomic operations, but neglect composite operations, such as refactorings, which leads to cluttered difference reports. To tackle this limitation, we present an orthogonal extension of existing atomic operation detection approaches for detecting also composite operations. Our approach searches for occurrences of composite operations within a set of detected atomic operations in a post-processing manner. One major benefit is the reuse of specifications available for executing composite operations also for detecting applications of them. We evaluate the accuracy of the approach in a real-world case study and investigate the scalability of our implementation in an experiment.",0,"A posteriori operation detection in evolving software models. As every software artifact, also software models are subject to continuous evolution. The operations applied between two successive versions of a model are crucial for understanding its evolution. Generic approaches for detecting operations a posteriori identify atomic operations, but neglect composite operations, such as refactorings, which leads to cluttered difference reports. To tackle this limitation, we present an orthogonal extension of existing atomic operation detection approaches for detecting also composite operations. Our approach searches for occurrences of composite operations within a set of detected atomic operations in a post-processing manner. One major benefit is the reuse of specifications available for executing composite operations also for detecting applications of them. We evaluate the accuracy of the approach in a real-world case study and investigate the scalability of our implementation in an experiment.",1,1,2,1,2,0
161,Evaluation of model transformation approaches for model refactoring,"Model transformation, Measurement, Quality characteristics, Model restructuring","This paper provides a systematic evaluation framework for comparing model transformation approaches, based upon the ISO/IEC 9126-1 quality characteristics for software systems. We apply this framework to compare five transformation approaches (QVT-R, ATL, Kermeta, UML-RSDS and GrGen.NET) on a complex model refactoring case study: the amalgamation of apparent attribute clones in a class diagram. The case study highlights the problems with the specification and design of the refactoring category of model transformations, and provides a challenging example by which model transformation languages and approaches can be compared. We take into account a wide range of evaluation criteria aspects such as correctness, efficiency, flexibility, interoperability, re-usability and robustness, which have not been comprehensively covered by other comparative surveys of transformation approaches. The results show clear distinctions between the capabilities and suitabilities of different approaches to address the refactoring form of transformation problem.",0,"Evaluation of model transformation approaches for model refactoring. This paper provides a systematic evaluation framework for comparing model transformation approaches, based upon the ISO/IEC 9126-1 quality characteristics for software systems. We apply this framework to compare five transformation approaches (QVT-R, ATL, Kermeta, UML-RSDS and GrGen.NET) on a complex model refactoring case study: the amalgamation of apparent attribute clones in a class diagram. The case study highlights the problems with the specification and design of the refactoring category of model transformations, and provides a challenging example by which model transformation languages and approaches can be compared. We take into account a wide range of evaluation criteria aspects such as correctness, efficiency, flexibility, interoperability, re-usability and robustness, which have not been comprehensively covered by other comparative surveys of transformation approaches. The results show clear distinctions between the capabilities and suitabilities of different approaches to address the refactoring form of transformation problem.",1,3,2,1,2,1
162,Improving refactoring tools in Smalltalk using static type inference,"Smalltalk, Type inference, Static type inference, Refactoring","Refactoring is a crucial activity in agile software development. As a consequence, automated tools are expected to support refactoring, both for reducing the developer's effort as well as for avoiding errors due to manual changes. In this context, the chosen programming language has a major impact on the level of support that an automated refactoring tool can offer. One important aspect of a programming language concerning the automation of refactoring is the type system. While a static type system, present in languages such as Java, provides information about dependencies in the program, the dynamic type system of the Smalltalk programming language offers little information that can be used by automated refactoring tools. This paper focuses on the challenges in the context of refactoring raised by the dynamic type system of Smalltalk. It highlights the problems caused by the absence of static type information and proposes the use of static code analysis for performing type inference to gather information about the dependencies in the program's source code. It explains the mechanism of the static code analysis using sample code and presents a prototype of an enhanced refactoring tool, which uses the structural information extracted through static code analysis. Empirical samples build the base for evaluating the effectiveness of the approach.",0,"Improving refactoring tools in Smalltalk using static type inference. Refactoring is a crucial activity in agile software development. As a consequence, automated tools are expected to support refactoring, both for reducing the developer's effort as well as for avoiding errors due to manual changes. In this context, the chosen programming language has a major impact on the level of support that an automated refactoring tool can offer. One important aspect of a programming language concerning the automation of refactoring is the type system. While a static type system, present in languages such as Java, provides information about dependencies in the program, the dynamic type system of the Smalltalk programming language offers little information that can be used by automated refactoring tools. This paper focuses on the challenges in the context of refactoring raised by the dynamic type system of Smalltalk. It highlights the problems caused by the absence of static type information and proposes the use of static code analysis for performing type inference to gather information about the dependencies in the program's source code. It explains the mechanism of the static code analysis using sample code and presents a prototype of an enhanced refactoring tool, which uses the structural information extracted through static code analysis. Empirical samples build the base for evaluating the effectiveness of the approach.",1,3,2,1,3,4
163,Exploring implicit parallelism in class diagrams,"Class diagram analysis, Parallel software design, Object oriented software refactoring","Abstract
As multicore processors are becoming more wide-spread, leveraging of parallelism is once again becoming an important concern during the software development process. Substantial refactoring is required to parallelize legacy sequential software in order to exploit the advantages offered by parallel processing. In this study, guidelines are offered to aid in parallelizing object-oriented programs by analyzing their designs as represented in UML class diagrams. We define often occurring patterns of class-dependencies and demonstrate their characteristics in class diagrams by investigating their properties. We present example instances exhibiting the usage of these patterns in class diagrams. Through analyzing the runtime aspects of these instances, we have identified how they impact the parallelization of object oriented software. Taking these lessons into account when refactoring existing object-oriented software can significantly reduce time and effort required. We have evaluated our method by applying it to three popular design patterns and a real-world case study.",0,"Exploring implicit parallelism in class diagrams. Abstract
As multicore processors are becoming more wide-spread, leveraging of parallelism is once again becoming an important concern during the software development process. Substantial refactoring is required to parallelize legacy sequential software in order to exploit the advantages offered by parallel processing. In this study, guidelines are offered to aid in parallelizing object-oriented programs by analyzing their designs as represented in UML class diagrams. We define often occurring patterns of class-dependencies and demonstrate their characteristics in class diagrams by investigating their properties. We present example instances exhibiting the usage of these patterns in class diagrams. Through analyzing the runtime aspects of these instances, we have identified how they impact the parallelization of object oriented software. Taking these lessons into account when refactoring existing object-oriented software can significantly reduce time and effort required. We have evaluated our method by applying it to three popular design patterns and a real-world case study.",0,3,2,0,2,1
164,A technique for automatic component extraction from object-oriented programs by refactoring,"Component-based development (CBD), Refactoring, Object-oriented programming, Software reuse, Software component, JavaBeans","Component-based software development (CBD) is based on building software systems from previously-existing software components. In CBD, reuse of common parts in component form can reduce the development cost of new systems, and reduce the maintenance cost associated with the support of these systems. However, existing programs have usually been built using another paradigm, such as the object-oriented (OO) paradigm. OO programs cannot be reused rapidly or effectively in the CBD paradigm even if they contain reusable functions. In this paper, we propose a technique for extracting components from existing OO programs by our new refactoring ``Extract Component''. Our technique of refactoring can identify and extract reusable components composed of classes from OO programs, and modify the surrounding parts of extracted components in original programs. We have developed a system that performs our refactoring automatically and extracts JavaBeans components from Java programs. As a result of evaluation experiments, it is found that our system is useful for extracting reusable components along with usage examples from Java programs.",0,"A technique for automatic component extraction from object-oriented programs by refactoring. Component-based software development (CBD) is based on building software systems from previously-existing software components. In CBD, reuse of common parts in component form can reduce the development cost of new systems, and reduce the maintenance cost associated with the support of these systems. However, existing programs have usually been built using another paradigm, such as the object-oriented (OO) paradigm. OO programs cannot be reused rapidly or effectively in the CBD paradigm even if they contain reusable functions. In this paper, we propose a technique for extracting components from existing OO programs by our new refactoring ``Extract Component''. Our technique of refactoring can identify and extract reusable components composed of classes from OO programs, and modify the surrounding parts of extracted components in original programs. We have developed a system that performs our refactoring automatically and extracts JavaBeans components from Java programs. As a result of evaluation experiments, it is found that our system is useful for extracting reusable components along with usage examples from Java programs.",0,1,1,1,1,4
165,A concept and implementation of higher-level XML transformation languages,"Software generation and transformation, Higher-level transformation operators, Operator hierarchy concept, Transformation language, XML, XSLT","In the software development (e.g. with product lines or refactoring) transformations play an increasing role. To ease and automate these transformations, we propose a solution based on the operator hierarchy concept. It allows to define transformation operator hierarchies containing different levels of transformation operators. The operators capture reusable knowledge units. The concrete realization of such a higher-level transformation language construction is demonstrated by an application of the XML operator hierarchy concept to the transformation language XSLT. XSLT serves as an example which is employed to provide the elementary transformation operators. On top of these elementary operators the layered concept allows the definition of new higher-level operators, e.g. domain-independent and domain-specific ones. In an application example the construction of the higher-level language XML2DSV is presented. This is a stand-alone domain-specific transformation language, which can be used to create delimiter-separated values (DSV) files from XML documents, on the base of XSLT. We developed XTC (XML Transformation Coordinator) to automate the multi-level transformation process.",0,"A concept and implementation of higher-level XML transformation languages. In the software development (e.g. with product lines or refactoring) transformations play an increasing role. To ease and automate these transformations, we propose a solution based on the operator hierarchy concept. It allows to define transformation operator hierarchies containing different levels of transformation operators. The operators capture reusable knowledge units. The concrete realization of such a higher-level transformation language construction is demonstrated by an application of the XML operator hierarchy concept to the transformation language XSLT. XSLT serves as an example which is employed to provide the elementary transformation operators. On top of these elementary operators the layered concept allows the definition of new higher-level operators, e.g. domain-independent and domain-specific ones. In an application example the construction of the higher-level language XML2DSV is presented. This is a stand-alone domain-specific transformation language, which can be used to create delimiter-separated values (DSV) files from XML documents, on the base of XSLT. We developed XTC (XML Transformation Coordinator) to automate the multi-level transformation process.",1,1,0,0,1,4
166,An approach to test-driven development of conceptual schemas,"Conceptual modeling, Testing, TDD, Requirements validation, UML/OCL","Test-Driven Development (TDD) is an extreme programming development method in which a software system is developed in short iterations. In this paper we present the Test-Driven Conceptual Modeling (TDCM) method, which is an application of TDD for conceptual modeling, and we show how to develop a conceptual schema using it. In TDCM, a system's conceptual schema is incrementally obtained by performing three kinds of tasks: (1) Write a test the system should pass; (2) Change the schema to pass the test; and (3) Refactor the schema to improve its qualities. We also describe an integration approach of TDCM into a broad set of software development methodologies, including the Unified Process development methodology, the MDD-based approaches, the storytest-driven agile methods and the goal and scenario-oriented requirements engineering methods. We deal with schemas written in UML/OCL, but the TDCM method could be adapted to the development of schemas in other languages.",0,"An approach to test-driven development of conceptual schemas. Test-Driven Development (TDD) is an extreme programming development method in which a software system is developed in short iterations. In this paper we present the Test-Driven Conceptual Modeling (TDCM) method, which is an application of TDD for conceptual modeling, and we show how to develop a conceptual schema using it. In TDCM, a system's conceptual schema is incrementally obtained by performing three kinds of tasks: (1) Write a test the system should pass; (2) Change the schema to pass the test; and (3) Refactor the schema to improve its qualities. We also describe an integration approach of TDCM into a broad set of software development methodologies, including the Unified Process development methodology, the MDD-based approaches, the storytest-driven agile methods and the goal and scenario-oriented requirements engineering methods. We deal with schemas written in UML/OCL, but the TDCM method could be adapted to the development of schemas in other languages.",0,1,2,0,1,1
167,Static composition of refactorings,"Refactoring, Conditional program transformation, OR-sequence, AND-sequence, Composition, Derivation of precondition, Transformation description","The number of possible refactorings is unlimited, so no tool vendor will ever be able to provide custom refactorings for all specific user needs. Therefore, we propose a new kind of refactoring tools, which allow users to create, edit and compose required refactorings just like any other documents. The heart of such a refactoring editor is the ability to compose larger refactorings from existing ones. Computing the precondition of the composite refactoring from the preconditions of the composed refactorings is non-trivial since earlier transformations influence the truth of preconditions of later ones. The ability to calculate these effects without referring to a particular program to which the refactorings should be applied is called program-independent composition. It is the prerequisite for creating composite refactorings that are reusable on arbitrary programs. The main contribution of this paper is a formal model for automatic, program-independent composition of conditional program transformations. We show that conditional transformations, including refactorings, can be composed from a limited set of basic operations. Program-independent derivation of a precondition for the composite is based on the notion of ``transformation description'', which can be seen as a simplified, yet equally powerful, variant of Roberts' ``postconditions'' (Practical analysis for refactoring, Ph.D. Thesis (1999)). Our approach simplifies the implementation of refactoring tools---only the basic operations and the ability for composition must be hard coded in a tool. As a proof of concept, we sketch a transformation framework that implements our approach (jConditioner) and, based on the framework, an experimental refactoring tool (ConTraCT) that includes the editing capabilities that motivated our work.",0,"Static composition of refactorings. The number of possible refactorings is unlimited, so no tool vendor will ever be able to provide custom refactorings for all specific user needs. Therefore, we propose a new kind of refactoring tools, which allow users to create, edit and compose required refactorings just like any other documents. The heart of such a refactoring editor is the ability to compose larger refactorings from existing ones. Computing the precondition of the composite refactoring from the preconditions of the composed refactorings is non-trivial since earlier transformations influence the truth of preconditions of later ones. The ability to calculate these effects without referring to a particular program to which the refactorings should be applied is called program-independent composition. It is the prerequisite for creating composite refactorings that are reusable on arbitrary programs. The main contribution of this paper is a formal model for automatic, program-independent composition of conditional program transformations. We show that conditional transformations, including refactorings, can be composed from a limited set of basic operations. Program-independent derivation of a precondition for the composite is based on the notion of ``transformation description'', which can be seen as a simplified, yet equally powerful, variant of Roberts' ``postconditions'' (Practical analysis for refactoring, Ph.D. Thesis (1999)). Our approach simplifies the implementation of refactoring tools---only the basic operations and the ability for composition must be hard coded in a tool. As a proof of concept, we sketch a transformation framework that implements our approach (jConditioner) and, based on the framework, an experimental refactoring tool (ConTraCT) that includes the editing capabilities that motivated our work.",1,1,2,1,3,4
168,Slicing for architectural analysis,"Program analysis, Architectural recovery, Coordination","Current software development often relies on non-trivial coordination logic for combining autonomous services, eventually running on different platforms. As a rule, however, such a coordination layer is strongly woven within the application at source code level. Therefore, its precise identification becomes a major methodological (and technical) problem and a challenge to any program understanding or refactoring process. The approach introduced in this paper resorts to slicing techniques to extract coordination data from source code. Such data are captured in a specific dependency graph structure from which a coordination model can be recovered either in the form of an Orc specification or as a collection of code fragments corresponding to the identification of typical coordination patterns in the system. Tool support is also discussed.",0,"Slicing for architectural analysis. Current software development often relies on non-trivial coordination logic for combining autonomous services, eventually running on different platforms. As a rule, however, such a coordination layer is strongly woven within the application at source code level. Therefore, its precise identification becomes a major methodological (and technical) problem and a challenge to any program understanding or refactoring process. The approach introduced in this paper resorts to slicing techniques to extract coordination data from source code. Such data are captured in a specific dependency graph structure from which a coordination model can be recovered either in the form of an Orc specification or as a collection of code fragments corresponding to the identification of typical coordination patterns in the system. Tool support is also discussed.",1,3,1,1,1,1
169,Identifying Extract Class refactoring opportunities using structural and semantic cohesion measures,"Extract Class refactoring, Cohesion, Coupling, Graph theory, MaxFlow-MinCut, Empirical studies","Abstract
Approaches for improving class cohesion identify refactoring opportunities using metrics that capture structural relationships between the methods of a class, e.g., attribute references. Semantic metrics, e.g., C3 metric, have also been proposed to measure class cohesion, as they seem to complement structural metrics. However, until now semantic relationships between methods have not been used to identify refactoring opportunities. In this paper we propose an Extract Class refactoring method based on graph theory that exploits structural and semantic relationships between methods. The empirical evaluation of the proposed approach highlighted the benefits provided by the combination of semantic and structural measures and the potential usefulness of the proposed method as a feature for software development environments.",1,"Identifying Extract Class refactoring opportunities using structural and semantic cohesion measures. Abstract
Approaches for improving class cohesion identify refactoring opportunities using metrics that capture structural relationships between the methods of a class, e.g., attribute references. Semantic metrics, e.g., C3 metric, have also been proposed to measure class cohesion, as they seem to complement structural metrics. However, until now semantic relationships between methods have not been used to identify refactoring opportunities. In this paper we propose an Extract Class refactoring method based on graph theory that exploits structural and semantic relationships between methods. The empirical evaluation of the proposed approach highlighted the benefits provided by the combination of semantic and structural measures and the potential usefulness of the proposed method as a feature for software development environments.",0,1,2,1,2,0
170,Performance comparison of query-based techniques for anti-pattern detection,"Anti-patterns, Refactoring, Performance measurements, Columbus, EMF-IncQuery, OCL","Context
Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries.
Objective
Our paper investigates the costs and benefits of using the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by four different general-purpose model query techniques based on native Java code, OCL evaluation and (incremental) graph pattern matching.
Method
We provide in-depth comparison of these techniques on the source code of 28 Java projects using anti-pattern queries taken from refactoring operations in different usage profiles.
Results
Our results show that general purpose model queries can outperform hand-coded queries by 2--3 orders of magnitude, with the trade-off of an increased in memory consumption and model load time of up to an order of magnitude.
Conclusion
The measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios.",1,"Performance comparison of query-based techniques for anti-pattern detection. Context
Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries.
Objective
Our paper investigates the costs and benefits of using the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by four different general-purpose model query techniques based on native Java code, OCL evaluation and (incremental) graph pattern matching.
Method
We provide in-depth comparison of these techniques on the source code of 28 Java projects using anti-pattern queries taken from refactoring operations in different usage profiles.
Results
Our results show that general purpose model queries can outperform hand-coded queries by 2--3 orders of magnitude, with the trade-off of an increased in memory consumption and model load time of up to an order of magnitude.
Conclusion
The measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios.",1,2,1,2,2,1
171,Alloy as a Refactoring Checker?,"Alloy, refactoring, refinement, behaviour preservation, Z","Refactorings are systematic changes made to programs, models or specifications in order to improve their structure without changing the externally observable behaviour. We will examine how a constraint solver (the Alloy Analyzer) can be used to automatically check if refactorings, applied to a formal specification (written in Z), meet this requirement. Furthermore, we identify a class of refactorings for which the use of this tool is reasonable in general.",0,"Alloy as a Refactoring Checker?. Refactorings are systematic changes made to programs, models or specifications in order to improve their structure without changing the externally observable behaviour. We will examine how a constraint solver (the Alloy Analyzer) can be used to automatically check if refactorings, applied to a formal specification (written in Z), meet this requirement. Furthermore, we identify a class of refactorings for which the use of this tool is reasonable in general.",1,1,2,1,3,4
