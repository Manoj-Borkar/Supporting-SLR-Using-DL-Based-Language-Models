,title,keywords,abstract,label,Title_Abstract,cleaned,Preds_TFIDF,Preds_UW_BERT,Preds_W_BERT,Preds_paragraph_SBERT,Preds_UW_sentence_SBERT,Preds_W__sentence_SBERT
0,Emotion Explained,Brain; Consciousness; Decision-making; Emotion; Hunger; Pleasure; Reward; Sexual behaviour,"The book links the analysis of the brain mechanisms of emotion and motivation to the wider context of what emotions are, what their functions are, how emotions evolved, and the larger issue of why emotional and motivational feelings and consciousness might arise in a system organized like the brain. The topics in motivation covered are hunger, thirst, sexual behaviour, brain-stimulation reward, and addiction. The book proposes a theory of what emotions are, and an evolutionary, Darwinian, theory of the adaptive value of emotion, and then describes the brain mechanisms of emotion. The book examines how cognitive states can influence emotions, and in turn, how emotions can influence cognitive states. The book also examines emotion and decision-making, with links to the burgeoning field of neuroeconomics. The book describes the brain mechanisms that underlie both emotion and motivation in a scientific form that can be used by both students and scientists in the fields of neuroscience, psychology, cognitive neuroscience, biology, physiology, psychiatry, and medicine. {\copyright} Edmund T Rolls, 2005. All rights reserved.",0,"Emotion Explained. The book links the analysis of the brain mechanisms of emotion and motivation to the wider context of what emotions are, what their functions are, how emotions evolved, and the larger issue of why emotional and motivational feelings and consciousness might arise in a system organized like the brain. The topics in motivation covered are hunger, thirst, sexual behaviour, brain-stimulation reward, and addiction. The book proposes a theory of what emotions are, and an evolutionary, Darwinian, theory of the adaptive value of emotion, and then describes the brain mechanisms of emotion. The book examines how cognitive states can influence emotions, and in turn, how emotions can influence cognitive states. The book also examines emotion and decision-making, with links to the burgeoning field of neuroeconomics. The book describes the brain mechanisms that underlie both emotion and motivation in a scientific form that can be used by both students and scientists in the fields of neuroscience, psychology, cognitive neuroscience, biology, physiology, psychiatry, and medicine. {\copyright} Edmund T Rolls, 2005. All rights reserved.",emotion explained book links analysis brain mechanisms emotion motivation wider context emotions functions emotions evolved larger issue emotional motivational feelings consciousness might arise system organized like brain topics motivation covered hunger thirst sexual behaviour brain stimulation reward addiction book proposes theory emotions evolutionary darwinian theory adaptive value emotion describes brain mechanisms emotion book examines cognitive states influence emotions turn emotions influence cognitive states book also examines emotion decision making links burgeoning field neuroeconomics book describes brain mechanisms underlie emotion motivation scientific form used students scientists fields neuroscience psychology cognitive neuroscience biology physiology psychiatry medicine copyright edmund rolls 2005 rights reserved,2,0,1,2,1,3
1,Assessing the bug-prediction with re-usability based package organization for object oriented software systems,Fault-proneness prediction; Package reuse; Software quality,"Packages are re-usable components for faster and effective software maintenance. To promote the re-use in object-oriented systems and maintenance tasks easier, packages should be organized to depict compact design. Therefore, understanding and assessing package organization is primordial for maintenance tasks like Re-usability and Changeability. We believe that additional investigations of prevalent basic design principles such as defined by R.C. Martin are required to explore different aspects of package organization. In this study, we propose package-organization framework based on reachable components that measures re-usability index. Package re-usability index measures common effect of change taking place over dependent elements of a package in an object-oriented design paradigm. A detailed quality assessment on different versions of open source software systems is presented which evaluates capability of the proposed package re-usability index and other traditional package-level metrics to predict fault-proneness in software. The experimental study shows that proposed index captures different aspects of package-design which can be practically integrated with best practices of software development. Furthermore, the results provide insights on organization of feasible software design to counter potential faults appearing due to complex package dependencies. {\copyright} 2017 The Institute of Electronics, Information and Communication Engineers.",0,"Assessing the bug-prediction with re-usability based package organization for object oriented software systems. Packages are re-usable components for faster and effective software maintenance. To promote the re-use in object-oriented systems and maintenance tasks easier, packages should be organized to depict compact design. Therefore, understanding and assessing package organization is primordial for maintenance tasks like Re-usability and Changeability. We believe that additional investigations of prevalent basic design principles such as defined by R.C. Martin are required to explore different aspects of package organization. In this study, we propose package-organization framework based on reachable components that measures re-usability index. Package re-usability index measures common effect of change taking place over dependent elements of a package in an object-oriented design paradigm. A detailed quality assessment on different versions of open source software systems is presented which evaluates capability of the proposed package re-usability index and other traditional package-level metrics to predict fault-proneness in software. The experimental study shows that proposed index captures different aspects of package-design which can be practically integrated with best practices of software development. Furthermore, the results provide insights on organization of feasible software design to counter potential faults appearing due to complex package dependencies. {\copyright} 2017 The Institute of Electronics, Information and Communication Engineers.",assessing bug prediction usability based package organization object oriented software systems packages usable components faster effective software maintenance promote use object oriented systems maintenance tasks easier packages organized depict compact design therefore understanding assessing package organization primordial maintenance tasks like usability changeability believe additional investigations prevalent basic design principles defined r c martin required explore different aspects package organization study propose package organization framework based reachable components measures usability index package usability index measures common effect change taking place dependent elements package object oriented design paradigm detailed quality assessment different versions open source software systems presented evaluates capability proposed package usability index traditional package level metrics predict fault proneness software experimental study shows proposed index captures different aspects package design practically integrated best practices software development furthermore results provide insights organization feasible software design counter potential faults appearing due complex package dependencies copyright 2017 institute electronics information communication engineers,1,2,0,1,3,2
2,Prediction of change-prone classes using machine learning and statistical techniques,,"For software development, availability of resources is limited, thereby necessitating efficient and effective utilization of resources. This can be achieved through prediction of key attributes, which affect software quality such as fault proneness, change proneness, effort, maintainability, etc. The primary aim of this chapter is to investigate the relationship between object-oriented metrics and change proneness. Predicting the classes that are prone to changes can help in maintenance and testing. Developers can focus on the classes that are more change prone by appropriately allocating resources. This will help in reducing costs associated with software maintenance activities. The authors have constructed models to predict change proneness using various machine-learning methods and one statistical method. They have evaluated and compared the performance of these methods. The proposed models are validated using open source software, Frinika, and the results are evaluated using Receiver Operating Characteristic (ROC) analysis. The study shows that machine-learning methods are more efficient than regression techniques. Among the machine-learning methods, boosting technique (i.e. Logitboost) outperformed all the other models. Thus, the authors conclude that the developed models can be used to predict the change proneness of classes, leading to improved software quality. {\copyright} 2017 by IGI Global. All rights reserved.",0,"Prediction of change-prone classes using machine learning and statistical techniques. For software development, availability of resources is limited, thereby necessitating efficient and effective utilization of resources. This can be achieved through prediction of key attributes, which affect software quality such as fault proneness, change proneness, effort, maintainability, etc. The primary aim of this chapter is to investigate the relationship between object-oriented metrics and change proneness. Predicting the classes that are prone to changes can help in maintenance and testing. Developers can focus on the classes that are more change prone by appropriately allocating resources. This will help in reducing costs associated with software maintenance activities. The authors have constructed models to predict change proneness using various machine-learning methods and one statistical method. They have evaluated and compared the performance of these methods. The proposed models are validated using open source software, Frinika, and the results are evaluated using Receiver Operating Characteristic (ROC) analysis. The study shows that machine-learning methods are more efficient than regression techniques. Among the machine-learning methods, boosting technique (i.e. Logitboost) outperformed all the other models. Thus, the authors conclude that the developed models can be used to predict the change proneness of classes, leading to improved software quality. {\copyright} 2017 by IGI Global. All rights reserved.",prediction change prone classes using machine learning statistical techniques software development availability resources limited thereby necessitating efficient effective utilization resources achieved prediction key attributes affect software quality fault proneness change proneness effort maintainability etc primary aim chapter investigate relationship object oriented metrics change proneness predicting classes prone changes help maintenance testing developers focus classes change prone appropriately allocating resources help reducing costs associated software maintenance activities authors constructed models predict change proneness using various machine learning methods one statistical method evaluated compared performance methods proposed models validated using open source software frinika results evaluated using receiver operating characteristic roc analysis study shows machine learning methods efficient regression techniques among machine learning methods boosting technique e logitboost outperformed models thus authors conclude developed models used predict change proneness classes leading improved software quality copyright 2017 igi global rights reserved,1,2,0,1,0,2
3,Primate behavioral ecology: Fifth edition,,"This comprehensive introductory text integrates evolutionary, ecological, and demographic perspectives with new results from field studies and contemporary noninvasive molecular and hormonal techniques to understand how different primates behave and the significance of these insights for primate conservation. Each chapter is organized around the major research themes in the field, with Strier emphasizing the interplay between theory, observations, and conservation issues. Examples are drawn from the ""classic"" primate field studies as well as more recent studies on previously neglected species, illustrating the vast behavioral variation that exists across the primate order. Primate Behavioral Ecology 5th Edition also examines how anthropogenic activities are negatively impacting primate populations, including a thorough analysis of behavioural plasticity and its implications. This fully updated new edition incorporates exciting new discoveries and the most up-to-date approaches in the field to provide an invaluable overview of the field of primate behavioral ecology and its applications to primate conservation. It is considered to be a ""must read"" for all students interested in primates. {\copyright} 2017, 2011, 2007, 2003, 2000 K. B. Strier. All rights reserved.",0,"Primate behavioral ecology: Fifth edition. This comprehensive introductory text integrates evolutionary, ecological, and demographic perspectives with new results from field studies and contemporary noninvasive molecular and hormonal techniques to understand how different primates behave and the significance of these insights for primate conservation. Each chapter is organized around the major research themes in the field, with Strier emphasizing the interplay between theory, observations, and conservation issues. Examples are drawn from the ""classic"" primate field studies as well as more recent studies on previously neglected species, illustrating the vast behavioral variation that exists across the primate order. Primate Behavioral Ecology 5th Edition also examines how anthropogenic activities are negatively impacting primate populations, including a thorough analysis of behavioural plasticity and its implications. This fully updated new edition incorporates exciting new discoveries and the most up-to-date approaches in the field to provide an invaluable overview of the field of primate behavioral ecology and its applications to primate conservation. It is considered to be a ""must read"" for all students interested in primates. {\copyright} 2017, 2011, 2007, 2003, 2000 K. B. Strier. All rights reserved.",primate behavioral ecology fifth edition comprehensive introductory text integrates evolutionary ecological demographic perspectives new results field studies contemporary noninvasive molecular hormonal techniques understand different primates behave significance insights primate conservation chapter organized around major research themes field strier emphasizing interplay theory observations conservation issues examples drawn classic primate field studies well recent studies previously neglected species illustrating vast behavioral variation exists across primate order primate behavioral ecology 5th edition also examines anthropogenic activities negatively impacting primate populations including thorough analysis behavioural plasticity implications fully updated new edition incorporates exciting new discoveries date approaches field provide invaluable overview field primate behavioral ecology applications primate conservation considered must read students interested primates copyright 2017 2011 2007 2003 2000 k b strier rights reserved,2,0,1,2,1,3
4,Feature subset selection for instance filtering methods on cross-project defect prediction,Code metrics; Cross-project defect prediction; Feature selection; Instance filtering; Network metrics; Software quality assurance,"The defect prediction models can be a good tool on organizing the project's test resources. However, not all companies maintain an appropriate set of historical defect data. In this case, the companies can build an appropriate dataset from known external projects. This approach, called Cross-project Defect Prediction (CPDP), solves the lack of defect data, although introduces heterogeneity on data. This heterogeneity can compromises the performance of CPDP models. Recently, filtering methods were proposed in order to decrease the heterogeneity of data by selecting the most similar instances from the training dataset. This similarity between instances is calculated based on the available features of the dataset. On this study, we propose that using only the most relevant features on this calculation can result in more accurate filtered datasets and better prediction performances. We present an empirical evaluation of different methods used for selecting the most relevant features. We evaluate different configurations of four Feature Selection (FS) methods and two metric subsets. We used 36 versions of 11 open source projects on experiments. The results indicate that the defect prediction performance can be improved by using the evaluated approach. In addition, we investigated which methods present better performances. The results do not indicate a method with general better performances, i.e., the most appropriate method for a project can vary according to the project characteristics.",0,"Feature subset selection for instance filtering methods on cross-project defect prediction. The defect prediction models can be a good tool on organizing the project's test resources. However, not all companies maintain an appropriate set of historical defect data. In this case, the companies can build an appropriate dataset from known external projects. This approach, called Cross-project Defect Prediction (CPDP), solves the lack of defect data, although introduces heterogeneity on data. This heterogeneity can compromises the performance of CPDP models. Recently, filtering methods were proposed in order to decrease the heterogeneity of data by selecting the most similar instances from the training dataset. This similarity between instances is calculated based on the available features of the dataset. On this study, we propose that using only the most relevant features on this calculation can result in more accurate filtered datasets and better prediction performances. We present an empirical evaluation of different methods used for selecting the most relevant features. We evaluate different configurations of four Feature Selection (FS) methods and two metric subsets. We used 36 versions of 11 open source projects on experiments. The results indicate that the defect prediction performance can be improved by using the evaluated approach. In addition, we investigated which methods present better performances. The results do not indicate a method with general better performances, i.e., the most appropriate method for a project can vary according to the project characteristics.",feature subset selection instance filtering methods cross project defect prediction defect prediction models good tool organizing project test resources however companies maintain appropriate set historical defect data case companies build appropriate dataset known external projects approach called cross project defect prediction cpdp solves lack defect data although introduces heterogeneity data heterogeneity compromises performance cpdp models recently filtering methods proposed order decrease heterogeneity data selecting similar instances training dataset similarity instances calculated based available features dataset study propose using relevant features calculation result accurate filtered datasets better prediction performances present empirical evaluation different methods used selecting relevant features evaluate different configurations four feature selection fs methods two metric subsets used 36 versions 11 open source projects experiments results indicate defect prediction performance improved using evaluated approach addition investigated methods present better performances results indicate method general better performances e appropriate method project vary according project characteristics,1,1,0,1,0,2
5,Social Science Methods for Psychodynamic Inquiry: The Unconscious on the World Scene,,"This book explains, with case examples, a variety of social science research methods suitable for studying the unconscious components of such irrational social and political actions in world affairs. {\copyright} William R. Meyers 2015. All rights reserved.",0,"Social Science Methods for Psychodynamic Inquiry: The Unconscious on the World Scene. This book explains, with case examples, a variety of social science research methods suitable for studying the unconscious components of such irrational social and political actions in world affairs. {\copyright} William R. Meyers 2015. All rights reserved.",social science methods psychodynamic inquiry unconscious world scene book explains case examples variety social science research methods suitable studying unconscious components irrational social political actions world affairs copyright william r meyers 2015 rights reserved,2,0,0,2,1,3
6,The prediction of code clone quality based on bayesian network,Bayesian network; Code clone; Prediction; Quality model; Reconstruct,"This paper researched on the quality of code clone in the software, evaluated the code clone quality of the current versions. Then using Bayesian network to train the existing sample data to get the prediction model of code clone that is able to predict the quality. The prediction results are able to help developers decide which code clone should be reconstructed or efficiently reused. The experiment shows that the method can be used to predict the quality of code clone in software more accurately. {\copyright} 2016 SERSC.",0,"The prediction of code clone quality based on bayesian network. This paper researched on the quality of code clone in the software, evaluated the code clone quality of the current versions. Then using Bayesian network to train the existing sample data to get the prediction model of code clone that is able to predict the quality. The prediction results are able to help developers decide which code clone should be reconstructed or efficiently reused. The experiment shows that the method can be used to predict the quality of code clone in software more accurately. {\copyright} 2016 SERSC.",prediction code clone quality based bayesian network paper researched quality code clone software evaluated code clone quality current versions using bayesian network train existing sample data get prediction model code clone able predict quality prediction results able help developers decide code clone reconstructed efficiently reused experiment shows method used predict quality code clone software accurately copyright 2016 sersc,1,2,2,1,0,0
7,A novel framework for integrating data mining techniques to software development phases,Code optimization; Defect prediction and classification; Software development process; Software effort estimation; Software reuse,"In software development process, phases such as development effort estimation, code optimization, source code defect detection and software reuse are very important in order to improve the productivity and quality of the software. Software repository data produced in each phases have increased as component of software development process and new data analysis techniques have emerged in order to optimize the software development process. There is a gap between the software project management practices and the need of valuable data from software repository. To overcome this gap, a novel integrated framework is proposed, which integrates data mining techniques to extract valuable information from software repository and software metrics are used in different phases of software development process. Integrated framework can be used by software development project managers to improve quality of software and reduce time in predicting effort estimation, optimizing source code, defect detection and classification. {\copyright} Springer India 2016.",0,"A novel framework for integrating data mining techniques to software development phases. In software development process, phases such as development effort estimation, code optimization, source code defect detection and software reuse are very important in order to improve the productivity and quality of the software. Software repository data produced in each phases have increased as component of software development process and new data analysis techniques have emerged in order to optimize the software development process. There is a gap between the software project management practices and the need of valuable data from software repository. To overcome this gap, a novel integrated framework is proposed, which integrates data mining techniques to extract valuable information from software repository and software metrics are used in different phases of software development process. Integrated framework can be used by software development project managers to improve quality of software and reduce time in predicting effort estimation, optimizing source code, defect detection and classification. {\copyright} Springer India 2016.",novel framework integrating data mining techniques software development phases software development process phases development effort estimation code optimization source code defect detection software reuse important order improve productivity quality software software repository data produced phases increased component software development process new data analysis techniques emerged order optimize software development process gap software project management practices need valuable data software repository overcome gap novel integrated framework proposed integrates data mining techniques extract valuable information software repository software metrics used different phases software development process integrated framework used software development project managers improve quality software reduce time predicting effort estimation optimizing source code defect detection classification copyright springer india 2016,1,2,2,1,0,2
8,Measuring the utility of functional-based software using centroid-adjusted class labelling,Functional programming; Java lambda; Pattern classification; Software engineering; Software metrics; Software utility,"The functional programming paradigm involves stateless computation on immutable data constructs. While this paradigm's historical context dates back to the early twentieth century with lambda calculus and a formal study of computability and function definition, there has been a resurgence in functional programming, especially in the area of predictive analytics. New, purely functional, languages have recently emerged, and functional extensions have been added to several popular programming languages. It is sometimes difficult to estimate the overall utility and extensibility of functional programming software components. At the same time, many software metrics exist that attempt to quantify various qualitative attributes of software components. Here, we use a computational intelligence strategy that uses a set of software metrics to predict the qualitative utility of a software system's underlying components. Centroid-adjusted class labelling is a pattern classification preprocessing method that compensates for the possible imprecision of an established external reference test (gold standard) by adjusting, when necessary, design pattern class labels while maintaining the reference test's discriminatory power. The adjusted design labels incorporate within-class centroid information using robust measures of location and dispersion. This method is applied to a biomedical data analysis software system written in a functional programming style. It is shown that significant improvement to the discriminatory power of the classifier is obtained when using this preprocessing method. {\copyright} Springer International Publishing Switzerland 2016.",0,"Measuring the utility of functional-based software using centroid-adjusted class labelling. The functional programming paradigm involves stateless computation on immutable data constructs. While this paradigm's historical context dates back to the early twentieth century with lambda calculus and a formal study of computability and function definition, there has been a resurgence in functional programming, especially in the area of predictive analytics. New, purely functional, languages have recently emerged, and functional extensions have been added to several popular programming languages. It is sometimes difficult to estimate the overall utility and extensibility of functional programming software components. At the same time, many software metrics exist that attempt to quantify various qualitative attributes of software components. Here, we use a computational intelligence strategy that uses a set of software metrics to predict the qualitative utility of a software system's underlying components. Centroid-adjusted class labelling is a pattern classification preprocessing method that compensates for the possible imprecision of an established external reference test (gold standard) by adjusting, when necessary, design pattern class labels while maintaining the reference test's discriminatory power. The adjusted design labels incorporate within-class centroid information using robust measures of location and dispersion. This method is applied to a biomedical data analysis software system written in a functional programming style. It is shown that significant improvement to the discriminatory power of the classifier is obtained when using this preprocessing method. {\copyright} Springer International Publishing Switzerland 2016.",measuring utility functional based software using centroid adjusted class labelling functional programming paradigm involves stateless computation immutable data constructs paradigm historical context dates back early twentieth century lambda calculus formal study computability function definition resurgence functional programming especially area predictive analytics new purely functional languages recently emerged functional extensions added several popular programming languages sometimes difficult estimate overall utility extensibility functional programming software components time many software metrics exist attempt quantify various qualitative attributes software components use computational intelligence strategy uses set software metrics predict qualitative utility software system underlying components centroid adjusted class labelling pattern classification preprocessing method compensates possible imprecision established external reference test gold standard adjusting necessary design pattern class labels maintaining reference test discriminatory power adjusted design labels incorporate within class centroid information using robust measures location dispersion method applied biomedical data analysis software system written functional programming style shown significant improvement discriminatory power classifier obtained using preprocessing method copyright springer international publishing switzerland 2016,1,1,0,1,0,2
9,Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension,association mining; logical coupling; particle swarm optimization; program comprehension; source code visualization,"By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration. {\copyright} 2015 The Authors.",0,"Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension. By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration. {\copyright} 2015 The Authors.",visualizing time based weighted coupling using particle swarm optimization aid program comprehension knowing software coupling developers get better view software quality improve productivity development maintenance paper presents method visualize coupling network often complex using heuristic approach based particle swarming optimization node placed randomly assigned initial speed node coupled together attracted trying get closer reach particular distance distance determined coupling value two nodes closely related nodes move closer reaching short distance iteration node position dynamically updated based attraction repulsive force around thus gradually forming near best solution logical coupling graph coupling values measured mining association rule changes history software development project sometimes active updates happen within minutes sometimes becomes slow weekly even monthly updates time based weighted analysis method used accommodate time sensitive situations co change short duration weighted co changes happen longer duration copyright 2015 authors,1,2,0,1,0,2
10,Programming biology: Expanding the toolset for the engineering of transcription,,"Transcription is a complex and dynamic process representing the first step in gene expression that can be readily controlled through current tools in molecular biology. Elucidating and subsequently controlling transcriptional processes in various prokaryotic and eukaryotic organisms have been a key element in translational research, yielding a variety of new opportunities for scientists and engineers. This chapter aims to give an overview of how the fields of molecular and synthetic biology have contributed both historically and presently to the state of the art in transcriptional engineering. The described tools and techniques, as well as the emerging genetic circuit engineering discipline, open the door to new advances in the fields of medical and industrial biotechnology. {\copyright} Springer International Publishing Switzerland 2016.",0,"Programming biology: Expanding the toolset for the engineering of transcription. Transcription is a complex and dynamic process representing the first step in gene expression that can be readily controlled through current tools in molecular biology. Elucidating and subsequently controlling transcriptional processes in various prokaryotic and eukaryotic organisms have been a key element in translational research, yielding a variety of new opportunities for scientists and engineers. This chapter aims to give an overview of how the fields of molecular and synthetic biology have contributed both historically and presently to the state of the art in transcriptional engineering. The described tools and techniques, as well as the emerging genetic circuit engineering discipline, open the door to new advances in the fields of medical and industrial biotechnology. {\copyright} Springer International Publishing Switzerland 2016.",programming biology expanding toolset engineering transcription transcription complex dynamic process representing first step gene expression readily controlled current tools molecular biology elucidating subsequently controlling transcriptional processes various prokaryotic eukaryotic organisms key element translational research yielding variety new opportunities scientists engineers chapter aims give overview fields molecular synthetic biology contributed historically presently state art transcriptional engineering described tools techniques well emerging genetic circuit engineering discipline open door new advances fields medical industrial biotechnology copyright springer international publishing switzerland 2016,2,0,1,2,2,3
11,Prediction of changeability for object oriented classes and packages by mining change history,change history; changeability prediction; data mining; object oriented software component; software measurements,"As a software system evolves, the classes are changed due to some development or maintenance activity, which is inevitable in software life cycle. These class changes can produce ripple effect or can lead to subsequent changes to other classes in the same package. In this paper, the changeability predictors for the classes and packages are proposed based on their past change pattern i.e. change history. Association learning method has been applied for discovering change-coupling pattern between the classes. In present work, the framework for the computation of the proposed changeability predictors is demonstrated and results are evaluated for java application. The results show that, association mining based machine learning technique can be useful to classify the classes as per their change-readiness. For doing changes in future, the proposed changeability predictors will be helpful for development team to predict the changeability of the classes. {\copyright} 2014 IEEE.",0,"Prediction of changeability for object oriented classes and packages by mining change history. As a software system evolves, the classes are changed due to some development or maintenance activity, which is inevitable in software life cycle. These class changes can produce ripple effect or can lead to subsequent changes to other classes in the same package. In this paper, the changeability predictors for the classes and packages are proposed based on their past change pattern i.e. change history. Association learning method has been applied for discovering change-coupling pattern between the classes. In present work, the framework for the computation of the proposed changeability predictors is demonstrated and results are evaluated for java application. The results show that, association mining based machine learning technique can be useful to classify the classes as per their change-readiness. For doing changes in future, the proposed changeability predictors will be helpful for development team to predict the changeability of the classes. {\copyright} 2014 IEEE.",prediction changeability object oriented classes packages mining change history software system evolves classes changed due development maintenance activity inevitable software life cycle class changes produce ripple effect lead subsequent changes classes package paper changeability predictors classes packages proposed based past change pattern e change history association learning method applied discovering change coupling pattern classes present work framework computation proposed changeability predictors demonstrated results evaluated java application results show association mining based machine learning technique useful classify classes per change readiness changes future proposed changeability predictors helpful development team predict changeability classes copyright 2014 ieee,1,2,0,1,0,2
12,Analysing the style of textual labels in i* models,,"An important quality aspect for conceptual models (such as i* models) is the quality of textual labels. Naming conventions are aimed to make sure that labels are used in a consistent manner. We present a tool that checks automatically whether a textual label in an i* model adheres to a set of naming conventions. This does not only help to enforce the use of a consistent labelling style, it also helps to detect modelling errors such as goals in i* models that should be softgoals (or vice versa).",0,"Analysing the style of textual labels in i* models. An important quality aspect for conceptual models (such as i* models) is the quality of textual labels. Naming conventions are aimed to make sure that labels are used in a consistent manner. We present a tool that checks automatically whether a textual label in an i* model adheres to a set of naming conventions. This does not only help to enforce the use of a consistent labelling style, it also helps to detect modelling errors such as goals in i* models that should be softgoals (or vice versa).",analysing style textual labels models important quality aspect conceptual models models quality textual labels naming conventions aimed make sure labels used consistent manner present tool checks automatically whether textual label model adheres set naming conventions help enforce use consistent labelling style also helps detect modelling errors goals models softgoals vice versa,1,2,0,1,3,2
13,Design pattern recognition by using adaptive neuro fuzzy inference system,ANFIS; machine learning; pattern recognition; Software design patterns,"Software design patterns describe recurring design problems and provide the essence of best practice solutions. It is useful and important, for various software engineering tasks, to know which design pattern is implemented where in a software design. However, this information is often lost due to poor or absent documentation, and so accurate recognition tools are required. The problem is that design patterns, given their abstract and vague nature, have a level of resistance to be automatically and accurately recognized. Although this vagueness or fuzziness can be captured and modelled by the fuzzy inference system, it has not yet been applied to solve this problem. This paper fills this gap by proposing an approach for design pattern recognition based on Adaptive Neuro Fuzzy Inference System. Our approach consists of two phases: space reduction phase and design pattern recognition phase. Both phases are implemented by ANFIS. We evaluate the approach by an experiment conducted to recognize six design patterns in an open source application. The results show that the approach is viable and promising. {\copyright} 2013 IEEE.",0,"Design pattern recognition by using adaptive neuro fuzzy inference system. Software design patterns describe recurring design problems and provide the essence of best practice solutions. It is useful and important, for various software engineering tasks, to know which design pattern is implemented where in a software design. However, this information is often lost due to poor or absent documentation, and so accurate recognition tools are required. The problem is that design patterns, given their abstract and vague nature, have a level of resistance to be automatically and accurately recognized. Although this vagueness or fuzziness can be captured and modelled by the fuzzy inference system, it has not yet been applied to solve this problem. This paper fills this gap by proposing an approach for design pattern recognition based on Adaptive Neuro Fuzzy Inference System. Our approach consists of two phases: space reduction phase and design pattern recognition phase. Both phases are implemented by ANFIS. We evaluate the approach by an experiment conducted to recognize six design patterns in an open source application. The results show that the approach is viable and promising. {\copyright} 2013 IEEE.",design pattern recognition using adaptive neuro fuzzy inference system software design patterns describe recurring design problems provide essence best practice solutions useful important various software engineering tasks know design pattern implemented software design however information often lost due poor absent documentation accurate recognition tools required problem design patterns given abstract vague nature level resistance automatically accurately recognized although vagueness fuzziness captured modelled fuzzy inference system yet applied solve problem paper fills gap proposing approach design pattern recognition based adaptive neuro fuzzy inference system approach consists two phases space reduction phase design pattern recognition phase phases implemented anfis evaluate approach experiment conducted recognize six design patterns open source application results show approach viable promising copyright 2013 ieee,0,2,0,1,0,2
14,Feature-oriented software product lines: Concepts and implementation,,"While standardization has empowered the software industry to substantially scale software development and to provide affordable software to a broad market, it often does not address smaller market segments, nor the needs and wishes of individual customers. Software product lines reconcile mass production and standardization with mass customization in software engineering. Ideally, based on a set of reusable parts, a software manufacturer can generate a software product based on the requirements of its customer. The concept of features is central to achieving this level of automation, because features bridge the gap between the requirements the customer has and the functionality a product provides. Thus features are a central concept in all phases of product-line development. The authors take a developer's viewpoint, focus on the development, maintenance, and implementation of product-line variability, and especially concentrate on automated product derivation based on a user's feature selection. The book consists of three parts. Part I provides a general introduction to feature-oriented software product lines, describing the product-line approach and introducing the product-line development process with its two elements of domain and application engineering. The pivotal part II covers a wide variety of implementation techniques including design patterns, frameworks, components, feature-oriented programming, and aspect-oriented programming, as well as tool-based approaches including preprocessors, build systems, version-control systems, and virtual separation of concerns. Finally, part III is devoted to advanced topics related to feature-oriented product lines like refactoring, feature interaction, and analysis tools specific to product lines. In addition, an appendix lists various helpful tools for software product-line development, along with a description of how they relate to the topics covered in this book. To tie the book together, the authors use two running examples that are well documented in the product-line literature: data management for embedded systems, and variations of graph data structures. They start every chapter by explicitly stating the respective learning goals and finish it with a set of exercises; additional teaching material is also available online. All these features make the book ideally suited for teaching - both for academic classes and for professionals interested in self-study. {\copyright} Springer-Verlag Berlin Heidelberg 2013. All rights are reserved.",0,"Feature-oriented software product lines: Concepts and implementation. While standardization has empowered the software industry to substantially scale software development and to provide affordable software to a broad market, it often does not address smaller market segments, nor the needs and wishes of individual customers. Software product lines reconcile mass production and standardization with mass customization in software engineering. Ideally, based on a set of reusable parts, a software manufacturer can generate a software product based on the requirements of its customer. The concept of features is central to achieving this level of automation, because features bridge the gap between the requirements the customer has and the functionality a product provides. Thus features are a central concept in all phases of product-line development. The authors take a developer's viewpoint, focus on the development, maintenance, and implementation of product-line variability, and especially concentrate on automated product derivation based on a user's feature selection. The book consists of three parts. Part I provides a general introduction to feature-oriented software product lines, describing the product-line approach and introducing the product-line development process with its two elements of domain and application engineering. The pivotal part II covers a wide variety of implementation techniques including design patterns, frameworks, components, feature-oriented programming, and aspect-oriented programming, as well as tool-based approaches including preprocessors, build systems, version-control systems, and virtual separation of concerns. Finally, part III is devoted to advanced topics related to feature-oriented product lines like refactoring, feature interaction, and analysis tools specific to product lines. In addition, an appendix lists various helpful tools for software product-line development, along with a description of how they relate to the topics covered in this book. To tie the book together, the authors use two running examples that are well documented in the product-line literature: data management for embedded systems, and variations of graph data structures. They start every chapter by explicitly stating the respective learning goals and finish it with a set of exercises; additional teaching material is also available online. All these features make the book ideally suited for teaching - both for academic classes and for professionals interested in self-study. {\copyright} Springer-Verlag Berlin Heidelberg 2013. All rights are reserved.",feature oriented software product lines concepts implementation standardization empowered software industry substantially scale software development provide affordable software broad market often address smaller market segments needs wishes individual customers software product lines reconcile mass production standardization mass customization software engineering ideally based set reusable parts software manufacturer generate software product based requirements customer concept features central achieving level automation features bridge gap requirements customer functionality product provides thus features central concept phases product line development authors take developer viewpoint focus development maintenance implementation product line variability especially concentrate automated product derivation based user feature selection book consists three parts part provides general introduction feature oriented software product lines describing product line approach introducing product line development process two elements domain application engineering pivotal part ii covers wide variety implementation techniques including design patterns frameworks components feature oriented programming aspect oriented programming well tool based approaches including preprocessors build systems version control systems virtual separation concerns finally part iii devoted advanced topics related feature oriented product lines like refactoring feature interaction analysis tools specific product lines addition appendix lists various helpful tools software product line development along description relate topics covered book tie book together authors use two running examples well documented product line literature data management embedded systems variations graph data structures start every chapter explicitly stating respective learning goals finish set exercises additional teaching material also available online features make book ideally suited teaching academic classes professionals interested self study copyright springer verlag berlin heidelberg 2013 rights reserved,2,1,1,1,2,3
15,Unifying and refactoring DMF to support concurrent Jini and JMS DMS in GIPSY,distributed demand-driven computing; GIPSY; Jini; JMS,"The General Intensional Programming System (GIPSY) is a framework for the compilation and distributed demand-driven evaluation of context-aware declarative programs. Its distributed run-time system includes a demand migration framework that has up to now been instantiated with two different communication technologies, namely Jini and JMS. However, the different nature, APIs, and requirements of these two solutions have resulted in artifact implementation divergence from the original demand migration framework (DMF) specifications as well as flaws in the DMF itself. This also inhibited the concurrent consistent use of the technologies within the same GIPSY network instance. Thus, in this paper we report on our re-engineering effort and results to refactor and unify the two somewhat disjoint Java distributed middleware technologies - Jini and JMS - used in the implementation of the corresponding Demand Migration Systems (DMS). In doing so, we refactor their parent Demand Migration Framework (DMF), within the General Intensional Programming System (GIPSY) and realign the Jini and JMS implementation remedying the flaws, improving interoperability, and allowing for scalability testing with various real applications and comparative studies. {\copyright} 2012 ACM.",0,"Unifying and refactoring DMF to support concurrent Jini and JMS DMS in GIPSY. The General Intensional Programming System (GIPSY) is a framework for the compilation and distributed demand-driven evaluation of context-aware declarative programs. Its distributed run-time system includes a demand migration framework that has up to now been instantiated with two different communication technologies, namely Jini and JMS. However, the different nature, APIs, and requirements of these two solutions have resulted in artifact implementation divergence from the original demand migration framework (DMF) specifications as well as flaws in the DMF itself. This also inhibited the concurrent consistent use of the technologies within the same GIPSY network instance. Thus, in this paper we report on our re-engineering effort and results to refactor and unify the two somewhat disjoint Java distributed middleware technologies - Jini and JMS - used in the implementation of the corresponding Demand Migration Systems (DMS). In doing so, we refactor their parent Demand Migration Framework (DMF), within the General Intensional Programming System (GIPSY) and realign the Jini and JMS implementation remedying the flaws, improving interoperability, and allowing for scalability testing with various real applications and comparative studies. {\copyright} 2012 ACM.",unifying refactoring dmf support concurrent jini jms dms gipsy general intensional programming system gipsy framework compilation distributed demand driven evaluation context aware declarative programs distributed run time system includes demand migration framework instantiated two different communication technologies namely jini jms however different nature apis requirements two solutions resulted artifact implementation divergence original demand migration framework dmf specifications well flaws dmf also inhibited concurrent consistent use technologies within gipsy network instance thus paper report engineering effort results refactor unify two somewhat disjoint java distributed middleware technologies jini jms used implementation corresponding demand migration systems dms refactor parent demand migration framework dmf within general intensional programming system gipsy realign jini jms implementation remedying flaws improving interoperability allowing scalability testing various real applications comparative studies copyright 2012 acm,0,1,1,1,0,3
16,"Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering, MALETS'11 - Associated with 26th IEEE/ACM International Conference on Automated Software Engineering",,The proceedings contain 8 papers. The topics discussed include: learning system abstractions for human operators; software fault localization using feature selection; the inductive software engineering manifesto: principles for industrial data mining; do better IR tools improve the accuracy of engineers' traceability recovery?; evolution of legacy system comprehensibility through automated refactoring; ProbPoly - a probabilistic inductive logic programming framework with application in model checking; towards learning to detect meaningful changes in software; and software analytics as a learning case in practice: approaches and experiences.,0,"Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering, MALETS'11 - Associated with 26th IEEE/ACM International Conference on Automated Software Engineering. The proceedings contain 8 papers. The topics discussed include: learning system abstractions for human operators; software fault localization using feature selection; the inductive software engineering manifesto: principles for industrial data mining; do better IR tools improve the accuracy of engineers' traceability recovery?; evolution of legacy system comprehensibility through automated refactoring; ProbPoly - a probabilistic inductive logic programming framework with application in model checking; towards learning to detect meaningful changes in software; and software analytics as a learning case in practice: approaches and experiences.",proceedings international workshop machine learning technologies software engineering malets 11 associated 26th ieee acm international conference automated software engineering proceedings contain 8 papers topics discussed include learning system abstractions human operators software fault localization using feature selection inductive software engineering manifesto principles industrial data mining better ir tools improve accuracy engineers traceability recovery evolution legacy system comprehensibility automated refactoring probpoly probabilistic inductive logic programming framework application model checking towards learning detect meaningful changes software software analytics learning case practice approaches experiences,1,2,0,2,2,3
17,Mobile phone-enabled control of medical care and handicapped assistance,actuation; control; healthcare; mobile medicine; mobile phone; pervasive medicine; telemedicine,"Mobile phones are now playing an ever more crucial role in peoples daily lives. They are serving not only as a way of talking and delivering messages, but also for exchanging various information. Nevertheless, the functional limit of the phone is still far from being reached. Among the many promising applications, using mobile phones as an actuating element to control data or devices is useful in quite a few emerging medical care and handicapped assistance settings owing to its wireless communication feature. In this article, selected progresses of mobile phone-enabled controlling have been summarized, with more focus on evaluating its emerging roles in medical care. Several typical applications in the area are illustrated and some potential technical challenges and key issues worthy of pursuit are outlined. The intent of the article is to provide an elementary knowledge for people with different backgrounds, such as electrical or biomedical engineers, as well as people who are working on interdisciplinary areas. It is expected that medical care at any time and anywhere will be possible via the actuation platform provided by the mobile phone and mobile medicine will be pushed forward to a new height in the coming years. {\copyright} 2011 Expert Reviews Ltd.",0,"Mobile phone-enabled control of medical care and handicapped assistance. Mobile phones are now playing an ever more crucial role in peoples daily lives. They are serving not only as a way of talking and delivering messages, but also for exchanging various information. Nevertheless, the functional limit of the phone is still far from being reached. Among the many promising applications, using mobile phones as an actuating element to control data or devices is useful in quite a few emerging medical care and handicapped assistance settings owing to its wireless communication feature. In this article, selected progresses of mobile phone-enabled controlling have been summarized, with more focus on evaluating its emerging roles in medical care. Several typical applications in the area are illustrated and some potential technical challenges and key issues worthy of pursuit are outlined. The intent of the article is to provide an elementary knowledge for people with different backgrounds, such as electrical or biomedical engineers, as well as people who are working on interdisciplinary areas. It is expected that medical care at any time and anywhere will be possible via the actuation platform provided by the mobile phone and mobile medicine will be pushed forward to a new height in the coming years. {\copyright} 2011 Expert Reviews Ltd.",mobile phone enabled control medical care handicapped assistance mobile phones playing ever crucial role peoples daily lives serving way talking delivering messages also exchanging various information nevertheless functional limit phone still far reached among many promising applications using mobile phones actuating element control data devices useful quite emerging medical care handicapped assistance settings owing wireless communication feature article selected progresses mobile phone enabled controlling summarized focus evaluating emerging roles medical care several typical applications area illustrated potential technical challenges key issues worthy pursuit outlined intent article provide elementary knowledge people different backgrounds electrical biomedical engineers well people working interdisciplinary areas expected medical care time anywhere possible via actuation platform provided mobile phone mobile medicine pushed forward new height coming years copyright 2011 expert reviews ltd,2,0,1,2,2,3
18,Searching for Molecular Solutions: Empirical Discovery and Its Future,,"A comprehensive look at empirical approaches to molecular discovery, their relationships with rational design, and the future of both Empirical methods of discovery, along with serendipitous and rational design approaches, have played an important role in human history. Searching for Molecular Solutions compares empirical discovery strategies for biologically useful molecules with serendipitous discovery and rational design, while also considering the strengths and limitations of empirical pathways to molecular discovery. Logically arranged, this text examines the different modes of molecular discovery, empha-sizing the historical and ongoing importance of empirical strategies. Along with a broad overview of the subject matter, Searching for Molecular Solutions explores: The differing modes of molecular discovery Biological precedents for evolutionary approaches Directed evolutionary methods and related areas Enzyme evolution and design Functional nucleic acid discovery Antibodies and other recognition molecules General aspects of molecular recognition Small molecule discovery approaches Rational molecular design The interplay between empirical and rational strategies and their ongoing roles in the future of molecular discovery Searching for Molecular Solutions covers several major areas of modern research, development, and practical applications of molecular sciences. This text offers empirical-rational principles of broad relevance to scientists, professionals, and students interested in general aspectsof molecular discovery, as well as the thought processes behind experimental approaches. {\copyright} 2010 John Wiley & Sons, Inc.",0,"Searching for Molecular Solutions: Empirical Discovery and Its Future. A comprehensive look at empirical approaches to molecular discovery, their relationships with rational design, and the future of both Empirical methods of discovery, along with serendipitous and rational design approaches, have played an important role in human history. Searching for Molecular Solutions compares empirical discovery strategies for biologically useful molecules with serendipitous discovery and rational design, while also considering the strengths and limitations of empirical pathways to molecular discovery. Logically arranged, this text examines the different modes of molecular discovery, empha-sizing the historical and ongoing importance of empirical strategies. Along with a broad overview of the subject matter, Searching for Molecular Solutions explores: The differing modes of molecular discovery Biological precedents for evolutionary approaches Directed evolutionary methods and related areas Enzyme evolution and design Functional nucleic acid discovery Antibodies and other recognition molecules General aspects of molecular recognition Small molecule discovery approaches Rational molecular design The interplay between empirical and rational strategies and their ongoing roles in the future of molecular discovery Searching for Molecular Solutions covers several major areas of modern research, development, and practical applications of molecular sciences. This text offers empirical-rational principles of broad relevance to scientists, professionals, and students interested in general aspectsof molecular discovery, as well as the thought processes behind experimental approaches. {\copyright} 2010 John Wiley & Sons, Inc.",searching molecular solutions empirical discovery future comprehensive look empirical approaches molecular discovery relationships rational design future empirical methods discovery along serendipitous rational design approaches played important role human history searching molecular solutions compares empirical discovery strategies biologically useful molecules serendipitous discovery rational design also considering strengths limitations empirical pathways molecular discovery logically arranged text examines different modes molecular discovery empha sizing historical ongoing importance empirical strategies along broad overview subject matter searching molecular solutions explores differing modes molecular discovery biological precedents evolutionary approaches directed evolutionary methods related areas enzyme evolution design functional nucleic acid discovery antibodies recognition molecules general aspects molecular recognition small molecule discovery approaches rational molecular design interplay empirical rational strategies ongoing roles future molecular discovery searching molecular solutions covers several major areas modern research development practical applications molecular sciences text offers empirical rational principles broad relevance scientists professionals students interested general aspectsof molecular discovery well thought processes behind experimental approaches copyright 2010 john wiley sons inc,2,0,1,2,2,3
19,A unified granular fuzzy-neuro framework for predicting and understanding software quality,Approximation of Min-Max relational equations; Fuzzy sequence; Hybrid granular fuzzy-neuro possibilistic model; If-then fuzzy weighted rules; Level of stability; Possibility theory; Software quality prediction and understanding,"We propose herein a novel unified framework that uses a developed hybrid fuzzy-neuro system in order to evaluate the impact of inheritance aspects on the evolvability of a class library, and to study the relevance of using inheritance as indicator of class interface stability with respect to version change. To this goal, we propose a novel computational granular unified framework that is cognitively motivated for learning if-then fuzzy weighted rules by using a hybrid neuro-fuzzy or fuzzy-neuro possibilistic model appropriately crafted as a means to automatically extract or learn software fuzzy prediction rules from only input-output examples by integrating some useful concepts from the human cognitive processes and adding some interesting granular functionalities. This learning scheme uses an exhaustive search over the fuzzy partitions of involved variables, automatic fuzzy hypotheses generation, formulation and testing, and approximation procedure of Min-Max relational equations. The main idea is to start learning from coarse fuzzy partitions of the involved metrics variables (both input and output) and proceed progressively toward fine-grained partitions until finding the appropriate partitions that fit the data. According to the complexity of the problem at hand, it learns the whole structure of the fuzzy system, i.e. conjointly appropriate fuzzy partitions, appropriate fuzzy rules, their number and their associated membership functions.",0,"A unified granular fuzzy-neuro framework for predicting and understanding software quality. We propose herein a novel unified framework that uses a developed hybrid fuzzy-neuro system in order to evaluate the impact of inheritance aspects on the evolvability of a class library, and to study the relevance of using inheritance as indicator of class interface stability with respect to version change. To this goal, we propose a novel computational granular unified framework that is cognitively motivated for learning if-then fuzzy weighted rules by using a hybrid neuro-fuzzy or fuzzy-neuro possibilistic model appropriately crafted as a means to automatically extract or learn software fuzzy prediction rules from only input-output examples by integrating some useful concepts from the human cognitive processes and adding some interesting granular functionalities. This learning scheme uses an exhaustive search over the fuzzy partitions of involved variables, automatic fuzzy hypotheses generation, formulation and testing, and approximation procedure of Min-Max relational equations. The main idea is to start learning from coarse fuzzy partitions of the involved metrics variables (both input and output) and proceed progressively toward fine-grained partitions until finding the appropriate partitions that fit the data. According to the complexity of the problem at hand, it learns the whole structure of the fuzzy system, i.e. conjointly appropriate fuzzy partitions, appropriate fuzzy rules, their number and their associated membership functions.",unified granular fuzzy neuro framework predicting understanding software quality propose herein novel unified framework uses developed hybrid fuzzy neuro system order evaluate impact inheritance aspects evolvability class library study relevance using inheritance indicator class interface stability respect version change goal propose novel computational granular unified framework cognitively motivated learning fuzzy weighted rules using hybrid neuro fuzzy fuzzy neuro possibilistic model appropriately crafted means automatically extract learn software fuzzy prediction rules input output examples integrating useful concepts human cognitive processes adding interesting granular functionalities learning scheme uses exhaustive search fuzzy partitions involved variables automatic fuzzy hypotheses generation formulation testing approximation procedure min max relational equations main idea start learning coarse fuzzy partitions involved metrics variables input output proceed progressively toward fine grained partitions finding appropriate partitions fit data according complexity problem hand learns whole structure fuzzy system e conjointly appropriate fuzzy partitions appropriate fuzzy rules number associated membership functions,1,1,1,1,0,2
20,"3rd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2008",,"The proceedings contain 61 papers. The special focus in this conference is on Leveraging Applications of Formal Methods, Verification and Validation. The topics include: Architecture based specification and verification of embedded software systems; information system engineering supporting observation, orientation, decision, and compliant action; modelling coordination and compensation; animating event B models by formal data models; automated formal testing of C API using T2C framework; tailoring and optimising software for automotive multicore system; timing validation of automotive software; towards using reo for compliance-aware business process modeling; a use-case driven approach to formal service-oriented modelling; safety and response-time analysis of an automotive accident assistance service; a framework for analyzing and testing the performance of software services; assuring the satisfiability of sequential extended regular expressions; computing must and may alias to detect null pointer dereference; program verification by reduction to semi-algebraic systems solving; debugging statecharts via model-code traceability; formal use of design patterns and refactoring; a component-based access control monitor; navigating the requirements jungle; non-functional avionics requirements; measurement-based timing analysis; weaving a formal methods education with problem-based learning; encouraging the uptake of formal methods training in an industrial context; computer-supported collaborative learning with mind-maps; thinking in user-centric models; specialization and instantiation aspects of a standard process for developing educational modules; contexts and context awareness in view of the diagram predicate framework; the use of adaptive semantic hypermedia for ubiquitous collaboration systems; the use of formal ontology to specify context in ubiquitous computing; high service availability in MaTRICS for the OCS; the ASK system and the challenge of distributed knowledge discovery; requirements for ontology based design project assessment; organizing the worlds machine learning information; workflow testing; directed generation of test data for static semantics checker; optimizing the system observability level for diagnosability; weaving authentication and authorization requirements into the functional model of a system using Z promotion; simple gedanken experiments in leveraging applications of formal methods and composition of web services using wrappers.",0,"3rd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2008. The proceedings contain 61 papers. The special focus in this conference is on Leveraging Applications of Formal Methods, Verification and Validation. The topics include: Architecture based specification and verification of embedded software systems; information system engineering supporting observation, orientation, decision, and compliant action; modelling coordination and compensation; animating event B models by formal data models; automated formal testing of C API using T2C framework; tailoring and optimising software for automotive multicore system; timing validation of automotive software; towards using reo for compliance-aware business process modeling; a use-case driven approach to formal service-oriented modelling; safety and response-time analysis of an automotive accident assistance service; a framework for analyzing and testing the performance of software services; assuring the satisfiability of sequential extended regular expressions; computing must and may alias to detect null pointer dereference; program verification by reduction to semi-algebraic systems solving; debugging statecharts via model-code traceability; formal use of design patterns and refactoring; a component-based access control monitor; navigating the requirements jungle; non-functional avionics requirements; measurement-based timing analysis; weaving a formal methods education with problem-based learning; encouraging the uptake of formal methods training in an industrial context; computer-supported collaborative learning with mind-maps; thinking in user-centric models; specialization and instantiation aspects of a standard process for developing educational modules; contexts and context awareness in view of the diagram predicate framework; the use of adaptive semantic hypermedia for ubiquitous collaboration systems; the use of formal ontology to specify context in ubiquitous computing; high service availability in MaTRICS for the OCS; the ASK system and the challenge of distributed knowledge discovery; requirements for ontology based design project assessment; organizing the worlds machine learning information; workflow testing; directed generation of test data for static semantics checker; optimizing the system observability level for diagnosability; weaving authentication and authorization requirements into the functional model of a system using Z promotion; simple gedanken experiments in leveraging applications of formal methods and composition of web services using wrappers.",3rd international symposium leveraging applications formal methods verification validation isola 2008 proceedings contain 61 papers special focus conference leveraging applications formal methods verification validation topics include architecture based specification verification embedded software systems information system engineering supporting observation orientation decision compliant action modelling coordination compensation animating event b models formal data models automated formal testing c api using t2c framework tailoring optimising software automotive multicore system timing validation automotive software towards using reo compliance aware business process modeling use case driven approach formal service oriented modelling safety response time analysis automotive accident assistance service framework analyzing testing performance software services assuring satisfiability sequential extended regular expressions computing must may alias detect null pointer dereference program verification reduction semi algebraic systems solving debugging statecharts via model code traceability formal use design patterns refactoring component based access control monitor navigating requirements jungle non functional avionics requirements measurement based timing analysis weaving formal methods education problem based learning encouraging uptake formal methods training industrial context computer supported collaborative learning mind maps thinking user centric models specialization instantiation aspects standard process developing educational modules contexts context awareness view diagram predicate framework use adaptive semantic hypermedia ubiquitous collaboration systems use formal ontology specify context ubiquitous computing high service availability matrics ocs ask system challenge distributed knowledge discovery requirements ontology based design project assessment organizing worlds machine learning information workflow testing directed generation test data static semantics checker optimizing system observability level diagnosability weaving authentication authorization requirements functional model system using z promotion simple gedanken experiments leveraging applications formal methods composition web services using wrappers,0,1,1,2,2,3
21,Intelligent Java analyzer,,"This paper presents a software metric working prototype to evaluate Java programmer's profiles. In order to automatically detect source code patterns, a Multi Layer Perceptron neural network is applied. Features determined from such patterns constitute the basis for system's programmer profiling. Results presented here show that the proposed prototype is a confident approach for support in the software quality assurance process. {\copyright} 2008 IEEE.",0,"Intelligent Java analyzer. This paper presents a software metric working prototype to evaluate Java programmer's profiles. In order to automatically detect source code patterns, a Multi Layer Perceptron neural network is applied. Features determined from such patterns constitute the basis for system's programmer profiling. Results presented here show that the proposed prototype is a confident approach for support in the software quality assurance process. {\copyright} 2008 IEEE.",intelligent java analyzer paper presents software metric working prototype evaluate java programmer profiles order automatically detect source code patterns multi layer perceptron neural network applied features determined patterns constitute basis system programmer profiling results presented show proposed prototype confident approach support software quality assurance process copyright 2008 ieee,0,2,2,0,0,2
22,"Morphology, processing, and integrating of information from large source code warehouses for decision support",,"Source code occurs in diverse programming languages with documentation using miscellaneous standards, comments in individual styles, extracted metrics or associated test cases that are hard to exploit through information retrieval or knowledge-discovery techniques. Typically, the information about object-oriented source code for a software system is distributed across several different sources, which makes processing complex. In this chapter we describe the morphology of object-oriented source code and how we (pre-) process, integrate and use it for knowledge discovery in software engineering in order to support decision-making regarding the refactoring, reengineering and reuse of software systems. {\copyright} 2006, Idea Group Inc.",0,"Morphology, processing, and integrating of information from large source code warehouses for decision support. Source code occurs in diverse programming languages with documentation using miscellaneous standards, comments in individual styles, extracted metrics or associated test cases that are hard to exploit through information retrieval or knowledge-discovery techniques. Typically, the information about object-oriented source code for a software system is distributed across several different sources, which makes processing complex. In this chapter we describe the morphology of object-oriented source code and how we (pre-) process, integrate and use it for knowledge discovery in software engineering in order to support decision-making regarding the refactoring, reengineering and reuse of software systems. {\copyright} 2006, Idea Group Inc.",morphology processing integrating information large source code warehouses decision support source code occurs diverse programming languages documentation using miscellaneous standards comments individual styles extracted metrics associated test cases hard exploit information retrieval knowledge discovery techniques typically information object oriented source code software system distributed across several different sources makes processing complex chapter describe morphology object oriented source code pre process integrate use knowledge discovery software engineering order support decision making regarding refactoring reengineering reuse software systems copyright 2006 idea group inc,2,2,0,1,3,2
23,Genetic algorithm based restructuring of object-oriented designs using metrics,Genetic algorithm; Metrics; Object-oriented design; Software restructuring,"Software with design flaws increases maintenance costs, decreases component reuse, and reduces software life. Even well-designed software tends to deteriorate with time as it undergoes maintenance. Work on restructuring object-oriented designs involves estimating the quality of the designs using metrics, and automating transformations that preserve the behavior of the designs. However, these factors have been treated almost independently of each other. A long-term goal is to define transformations preserving the behavior of object-oriented designs, and automate the transformations using metrics. In this paper, we describe a genetic algorithm based restructuring approach using metrics to automatically modify object-oriented designs. Cohesion and coupling metrics based on abstract models are defined to quantify designs and provide criteria for comparing alternative designs. The abstract models include a call-use graph and a class-association graph that represent methods, attributes, classes, and their relationships. The metrics include cohesion, inheritance coupling, and interaction coupling based on the behavioral similarity between methods extracted from the models. We define restructuring operations, and show that the operations preserve the behavior of object-oriented designs. We also devise a fitness function using cohesion and coupling metrics, and automatically restructure object-oriented designs by applying a genetic algorithm using the fitness function.",0,"Genetic algorithm based restructuring of object-oriented designs using metrics. Software with design flaws increases maintenance costs, decreases component reuse, and reduces software life. Even well-designed software tends to deteriorate with time as it undergoes maintenance. Work on restructuring object-oriented designs involves estimating the quality of the designs using metrics, and automating transformations that preserve the behavior of the designs. However, these factors have been treated almost independently of each other. A long-term goal is to define transformations preserving the behavior of object-oriented designs, and automate the transformations using metrics. In this paper, we describe a genetic algorithm based restructuring approach using metrics to automatically modify object-oriented designs. Cohesion and coupling metrics based on abstract models are defined to quantify designs and provide criteria for comparing alternative designs. The abstract models include a call-use graph and a class-association graph that represent methods, attributes, classes, and their relationships. The metrics include cohesion, inheritance coupling, and interaction coupling based on the behavioral similarity between methods extracted from the models. We define restructuring operations, and show that the operations preserve the behavior of object-oriented designs. We also devise a fitness function using cohesion and coupling metrics, and automatically restructure object-oriented designs by applying a genetic algorithm using the fitness function.",genetic algorithm based restructuring object oriented designs using metrics software design flaws increases maintenance costs decreases component reuse reduces software life even well designed software tends deteriorate time undergoes maintenance work restructuring object oriented designs involves estimating quality designs using metrics automating transformations preserve behavior designs however factors treated almost independently long term goal define transformations preserving behavior object oriented designs automate transformations using metrics paper describe genetic algorithm based restructuring approach using metrics automatically modify object oriented designs cohesion coupling metrics based abstract models defined quantify designs provide criteria comparing alternative designs abstract models include call use graph class association graph represent methods attributes classes relationships metrics include cohesion inheritance coupling interaction coupling based behavioral similarity methods extracted models define restructuring operations show operations preserve behavior object oriented designs also devise fitness function using cohesion coupling metrics automatically restructure object oriented designs applying genetic algorithm using fitness function,1,1,2,1,0,2
24,Detecting code smells using machine learning techniques: Are we there yet?,Code Smells; Empirical Studies; Machine Learning; Replication Study,"Code smells are symptoms of poor design and implementation choices weighing heavily on the quality of produced source code. During the last decades several code smell detection tools have been proposed. However, the literature shows that the results of these tools can be subjective and are intrinsically tied to the nature and approach of the detection. In a recent work the use of Machine-Learning (ML) techniques for code smell detection has been proposed, possibly solving the issue of tool subjectivity giving to a learner the ability to discern between smelly and non-smelly source code elements. While this work opened a new perspective for code smell detection, it only considered the case where instances affected by a single type smell are contained in each dataset used to train and test the machine learners. In this work we replicate the study with a different dataset configuration containing instances of more than one type of smell. The results reveal that with this configuration the machine learning techniques reveal critical limitations in the state of the art which deserve further research. {\copyright} 2018 IEEE.",1,"Detecting code smells using machine learning techniques: Are we there yet?. Code smells are symptoms of poor design and implementation choices weighing heavily on the quality of produced source code. During the last decades several code smell detection tools have been proposed. However, the literature shows that the results of these tools can be subjective and are intrinsically tied to the nature and approach of the detection. In a recent work the use of Machine-Learning (ML) techniques for code smell detection has been proposed, possibly solving the issue of tool subjectivity giving to a learner the ability to discern between smelly and non-smelly source code elements. While this work opened a new perspective for code smell detection, it only considered the case where instances affected by a single type smell are contained in each dataset used to train and test the machine learners. In this work we replicate the study with a different dataset configuration containing instances of more than one type of smell. The results reveal that with this configuration the machine learning techniques reveal critical limitations in the state of the art which deserve further research. {\copyright} 2018 IEEE.",detecting code smells using machine learning techniques yet code smells symptoms poor design implementation choices weighing heavily quality produced source code last decades several code smell detection tools proposed however literature shows results tools subjective intrinsically tied nature approach detection recent work use machine learning ml techniques code smell detection proposed possibly solving issue tool subjectivity giving learner ability discern smelly non smelly source code elements work opened new perspective code smell detection considered case instances affected single type smell contained dataset used train test machine learners work replicate study different dataset configuration containing instances one type smell results reveal configuration machine learning techniques reveal critical limitations state art deserve research copyright 2018 ieee,0,2,2,0,3,1
25,Code smell detection: Towards a machine learning-based approach,Code smells detection; Machine learning techniques,"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Usually the detection techniques are based on the computation of different kinds of metrics, and other aspects related to the domain of the system under analysis, its size and other design features are not taken into account. In this paper we propose an approach we are studying based on machine learning techniques. We outline some common problems faced for smells detection and we describe the different steps of our approach and the algorithms we use for the classification. {\copyright} 2013 IEEE.",1,"Code smell detection: Towards a machine learning-based approach. Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Usually the detection techniques are based on the computation of different kinds of metrics, and other aspects related to the domain of the system under analysis, its size and other design features are not taken into account. In this paper we propose an approach we are studying based on machine learning techniques. We outline some common problems faced for smells detection and we describe the different steps of our approach and the algorithms we use for the classification. {\copyright} 2013 IEEE.",code smell detection towards machine learning based approach several code smells detection tools developed providing different results smells subjectively interpreted hence detected different ways usually detection techniques based computation different kinds metrics aspects related domain system analysis size design features taken account paper propose approach studying based machine learning techniques outline common problems faced smells detection describe different steps approach algorithms use classification copyright 2013 ieee,0,2,2,0,3,1
26,SMURF: A SVM-based incremental anti-pattern detection approach,Anti-pattern; empirical software engineering; program comprehension; program maintenance,"In current, typical software development projects, hundreds of developers work asynchronously in space and time and may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and - or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns incrementally and on subsets of a system could reduce costs, effort, and resources by allowing practitioners to identify and take into account occurrences of anti-patterns as they find them during their development and maintenance activities. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently four limitations: (1) they require extensive knowledge of anti-patterns, (2) they have limited precision and recall, (3) they are not incremental, and (4) they cannot be applied on subsets of systems. To overcome these limitations, we introduce SMURF, a novel approach to detect anti-patterns, based on a machine learning technique - support vector machines - and taking into account practitioners' feedback. Indeed, through an empirical study involving three systems and four anti-patterns, we showed that the accuracy of SMURF is greater than that of DETEX and BDTEX when detecting anti-patterns occurrences. We also showed that SMURF can be applied in both intra-system and inter-system configurations. Finally, we reported that SMURF accuracy improves when using practitioners' feedback. {\copyright} 2012 IEEE.",1,"SMURF: A SVM-based incremental anti-pattern detection approach. In current, typical software development projects, hundreds of developers work asynchronously in space and time and may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and - or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns incrementally and on subsets of a system could reduce costs, effort, and resources by allowing practitioners to identify and take into account occurrences of anti-patterns as they find them during their development and maintenance activities. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently four limitations: (1) they require extensive knowledge of anti-patterns, (2) they have limited precision and recall, (3) they are not incremental, and (4) they cannot be applied on subsets of systems. To overcome these limitations, we introduce SMURF, a novel approach to detect anti-patterns, based on a machine learning technique - support vector machines - and taking into account practitioners' feedback. Indeed, through an empirical study involving three systems and four anti-patterns, we showed that the accuracy of SMURF is greater than that of DETEX and BDTEX when detecting anti-patterns occurrences. We also showed that SMURF can be applied in both intra-system and inter-system configurations. Finally, we reported that SMURF accuracy improves when using practitioners' feedback. {\copyright} 2012 IEEE.",smurf svm based incremental anti pattern detection approach current typical software development projects hundreds developers work asynchronously space time may introduce anti patterns software systems time pressure lack understanding communication skills anti patterns impede development maintenance activities making source code difficult understand detecting anti patterns incrementally subsets system could reduce costs effort resources allowing practitioners identify take account occurrences anti patterns find development maintenance activities researchers proposed approaches detect occurrences anti patterns approaches currently four limitations 1 require extensive knowledge anti patterns 2 limited precision recall 3 incremental 4 applied subsets systems overcome limitations introduce smurf novel approach detect anti patterns based machine learning technique support vector machines taking account practitioners feedback indeed empirical study involving three systems four anti patterns showed accuracy smurf greater detex bdtex detecting anti patterns occurrences also showed smurf applied intra system inter system configurations finally reported smurf accuracy improves using practitioners feedback copyright 2012 ieee,1,1,2,1,3,1
27,Reducing subjectivity in code smells detection: Experimenting with the Long Method,Binary Logistic Regression; Code smells; Long Method; Refactoring process,"Guidelines for refactoring are meant to improve software systems internal quality and are widely acknowledged as among software's best practices. However, such guidelines remain mostly qualitative in nature. As a result, judgments on how to conduct refactoring processes remain mostly subjective and therefore non-automatable, prone to errors and unrepeatable. The detection of the Long Method code smell is an example. To address this problem, this paper proposes a technique to detect Long Method objectively and automatically, using a Binary Logistic Regression model calibrated by expert's knowledge. The results of an experiment illustrating the use of this technique are reported. {\copyright} 2010 IEEE.",1,"Reducing subjectivity in code smells detection: Experimenting with the Long Method. Guidelines for refactoring are meant to improve software systems internal quality and are widely acknowledged as among software's best practices. However, such guidelines remain mostly qualitative in nature. As a result, judgments on how to conduct refactoring processes remain mostly subjective and therefore non-automatable, prone to errors and unrepeatable. The detection of the Long Method code smell is an example. To address this problem, this paper proposes a technique to detect Long Method objectively and automatically, using a Binary Logistic Regression model calibrated by expert's knowledge. The results of an experiment illustrating the use of this technique are reported. {\copyright} 2010 IEEE.",reducing subjectivity code smells detection experimenting long method guidelines refactoring meant improve software systems internal quality widely acknowledged among software best practices however guidelines remain mostly qualitative nature result judgments conduct refactoring processes remain mostly subjective therefore non automatable prone errors unrepeatable detection long method code smell example address problem paper proposes technique detect long method objectively automatically using binary logistic regression model calibrated expert knowledge results experiment illustrating use technique reported copyright 2010 ieee,0,2,0,0,3,1
28,Classification model for code clones based on machine learning,Classify; Code clone detector; Filtering; Machine learning,"Results from code clone detectors may contain plentiful useless code clones, but judging whether each code clone is useful varies from user to user based on a user's purpose for the clone. In this research, we propose a classification model that applies machine learning to the judgments of each individual user regarding the code clones. To evaluate the proposed model, 32 participants completed an online survey to test its usability and accuracy. The result showed several important observations on the characteristics of the true positives of code clones for the users. Our classification model showed more than 70 % accuracy on average and more than 90 % accuracy for some particular users and projects. {\copyright} 2014, The Author(s).",1,"Classification model for code clones based on machine learning. Results from code clone detectors may contain plentiful useless code clones, but judging whether each code clone is useful varies from user to user based on a user's purpose for the clone. In this research, we propose a classification model that applies machine learning to the judgments of each individual user regarding the code clones. To evaluate the proposed model, 32 participants completed an online survey to test its usability and accuracy. The result showed several important observations on the characteristics of the true positives of code clones for the users. Our classification model showed more than 70 % accuracy on average and more than 90 % accuracy for some particular users and projects. {\copyright} 2014, The Author(s).",classification model code clones based machine learning results code clone detectors may contain plentiful useless code clones judging whether code clone useful varies user user based user purpose clone research propose classification model applies machine learning judgments individual user regarding code clones evaluate proposed model 32 participants completed online survey test usability accuracy result showed several important observations characteristics true positives code clones users classification model showed 70 accuracy average 90 accuracy particular users projects copyright 2014 author,0,2,2,1,0,0
29,IDS: An immune-inspired approach for the detection of software design smells,Antipatterns; Artificial immune systems; Code smells; Reverse engineering; System design,"We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly-available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state-of-the-art approaches. {\copyright} 2010 IEEE.",1,"IDS: An immune-inspired approach for the detection of software design smells. We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly-available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state-of-the-art approaches. {\copyright} 2010 IEEE.",ids immune inspired approach detection software design smells propose parallel object oriented system designs living creatures suggest like living creature system designs subject diseases design smells code smells anti patterns design smells conjectured literature impact quality life systems therefore detection drawn attention researchers practitioners various approaches parallel propose novel approach built models immune system responses pathogenic material show approach detect one smell time build test approach gantt project v1 10 2 xerces v2 7 0 manually validated publicly available smells exist results show significant improvement detection time precision recall comparison state art approaches copyright 2010 ieee,0,2,2,0,3,1
30,Tracking design smells: Lessons from a study of God classes,Design smells; Empirical study; Software evolution,"""God class"" is a term used to describe a certain type of large classes which ""know too much or do too much"". Often a God class (GC) is created by accident as functionalities are incrementally added to a central class over the course of its evolution. GCs are generally thought to be examples of bad code that should be detected and removed to ensure software quality. However, in some cases, a GC is created by design as the best solution to a particular problem because, for example, the problem is not easily decomposable or strong requirements on efficiency exist. In this paper, we study in two open-source systems the ""life cycle"" of GCs: how they arise, how prevalent they are, and whether they remain or they are removed as the systems evolve over time, through a number of versions. We show how to detect the degree of ""godliness"" of classes automatically. Then, we show that by identifying the evolution of ""godliness"", we can distinguish between those classes that are so by design (good code) from those that occurred by accident (bad code). This methodology can guide software quality teams in their efforts to implement prevention and correction mechanisms. {\copyright} 2009 IEEE.",1,"Tracking design smells: Lessons from a study of God classes. ""God class"" is a term used to describe a certain type of large classes which ""know too much or do too much"". Often a God class (GC) is created by accident as functionalities are incrementally added to a central class over the course of its evolution. GCs are generally thought to be examples of bad code that should be detected and removed to ensure software quality. However, in some cases, a GC is created by design as the best solution to a particular problem because, for example, the problem is not easily decomposable or strong requirements on efficiency exist. In this paper, we study in two open-source systems the ""life cycle"" of GCs: how they arise, how prevalent they are, and whether they remain or they are removed as the systems evolve over time, through a number of versions. We show how to detect the degree of ""godliness"" of classes automatically. Then, we show that by identifying the evolution of ""godliness"", we can distinguish between those classes that are so by design (good code) from those that occurred by accident (bad code). This methodology can guide software quality teams in their efforts to implement prevention and correction mechanisms. {\copyright} 2009 IEEE.",tracking design smells lessons study god classes god class term used describe certain type large classes know much much often god class gc created accident functionalities incrementally added central class course evolution gcs generally thought examples bad code detected removed ensure software quality however cases gc created design best solution particular problem example problem easily decomposable strong requirements efficiency exist paper study two open source systems life cycle gcs arise prevalent whether remain removed systems evolve time number versions show detect degree godliness classes automatically show identifying evolution godliness distinguish classes design good code occurred accident bad code methodology guide software quality teams efforts implement prevention correction mechanisms copyright 2009 ieee,0,2,0,1,3,2
31,The evolution and psychology of self-deception,deception; evolutionary psychology; motivated cognition; self-deception; social psychology,"In this article we argue that self-deception evolved to facilitate interpersonal deception by allowing people to avoid the cues to conscious deception that might reveal deceptive intent. Self-deception has two additional advantages: It eliminates the costly cognitive load that is typically associated with deceiving, and it can minimize retribution if the deception is discovered. Beyond its role in specific acts of deception, self-deceptive self-enhancement also allows people to display more confidence than is warranted, which has a host of social advantages. The question then arises of how the self can be both deceiver and deceived. We propose that this is achieved through dissociations of mental processes, including conscious versus unconscious memories, conscious versus unconscious attitudes, and automatic versus controlled processes. Given the variety of methods for deceiving others, it should come as no surprise that self-deception manifests itself in a number of different psychological processes, and we discuss various types of self-deception. We then discuss the interpersonal versus intrapersonal nature of self-deception before considering the levels of consciousness at which the self can be deceived. Finally, we contrast our evolutionary approach to self-deception with current theories and debates in psychology and consider some of the costs associated with self-deception. {\copyright} 2011 Cambridge University Press.",0,"The evolution and psychology of self-deception. In this article we argue that self-deception evolved to facilitate interpersonal deception by allowing people to avoid the cues to conscious deception that might reveal deceptive intent. Self-deception has two additional advantages: It eliminates the costly cognitive load that is typically associated with deceiving, and it can minimize retribution if the deception is discovered. Beyond its role in specific acts of deception, self-deceptive self-enhancement also allows people to display more confidence than is warranted, which has a host of social advantages. The question then arises of how the self can be both deceiver and deceived. We propose that this is achieved through dissociations of mental processes, including conscious versus unconscious memories, conscious versus unconscious attitudes, and automatic versus controlled processes. Given the variety of methods for deceiving others, it should come as no surprise that self-deception manifests itself in a number of different psychological processes, and we discuss various types of self-deception. We then discuss the interpersonal versus intrapersonal nature of self-deception before considering the levels of consciousness at which the self can be deceived. Finally, we contrast our evolutionary approach to self-deception with current theories and debates in psychology and consider some of the costs associated with self-deception. {\copyright} 2011 Cambridge University Press.",evolution psychology self deception article argue self deception evolved facilitate interpersonal deception allowing people avoid cues conscious deception might reveal deceptive intent self deception two additional advantages eliminates costly cognitive load typically associated deceiving minimize retribution deception discovered beyond role specific acts deception self deceptive self enhancement also allows people display confidence warranted host social advantages question arises self deceiver deceived propose achieved dissociations mental processes including conscious versus unconscious memories conscious versus unconscious attitudes automatic versus controlled processes given variety methods deceiving others come surprise self deception manifests number different psychological processes discuss various types self deception discuss interpersonal versus intrapersonal nature self deception considering levels consciousness self deceived finally contrast evolutionary approach self deception current theories debates psychology consider costs associated self deception copyright 2011 cambridge university press,2,0,1,2,1,3
32,A survey on software fault localization,execution trace; program debugging; Software fault localization; software testing; survey; suspicious code,"Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole. {\copyright} 1976-2012 IEEE.",0,"A survey on software fault localization. Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole. {\copyright} 1976-2012 IEEE.",survey software fault localization software fault localization act identifying locations faults program widely recognized one tedious time consuming expensive yet equally critical activities program debugging due increasing scale complexity software today manually locating faults failures occur rapidly becoming infeasible consequently strong demand techniques guide software developers locations faults program minimal human intervention demand turn fueled proposal development broad spectrum fault localization techniques aims streamline fault localization process make effective attacking problem unique way article catalog provide comprehensive overview techniques discuss key issues concerns pertinent software fault localization whole copyright 1976 2012 ieee,1,0,1,1,3,2
33,CP-Miner: A Tool for Finding Copy-paste and Related Bugs in Operating System Code,,"Copy-pasted code is very common in large software because programmers prefer reusing code via copy-paste in order to reduce programming effort. Recent studies show that copy-paste is prone to introducing bugs and a significant portion of operating system bugs concentrate in copy-pasted code. Unfortunately, it is challenging to efficiently identify copy-pasted code in large software. Existing copy-paste detection tools are either not scalable to large software, or cannot handle small modifications in copy-pasted code. Furthermore, few tools are available to detect copy-paste related bugs. In this paper we propose a tool, CP-Miner, that uses data mining techniques to efficiently identify copy-pasted code in large software including operating systems, and detects copy-paste related bugs. Specifically, it takes less than 20 minutes for CP-Miner to identify 190,000 copy-pasted segments in Linux and 150,000 in FreeBSD. Moreover, CP-Miner has detected 28 copy-paste related bugs in the latest version of Linux and 23 in FreeBSD. In addition, we analyze some interesting characteristics of copy-paste in Linux and FreeBSD, including the distribution of copy-pasted code across different length, granularity, modules, degrees of modification, and various software versions.",0,"CP-Miner: A Tool for Finding Copy-paste and Related Bugs in Operating System Code. Copy-pasted code is very common in large software because programmers prefer reusing code via copy-paste in order to reduce programming effort. Recent studies show that copy-paste is prone to introducing bugs and a significant portion of operating system bugs concentrate in copy-pasted code. Unfortunately, it is challenging to efficiently identify copy-pasted code in large software. Existing copy-paste detection tools are either not scalable to large software, or cannot handle small modifications in copy-pasted code. Furthermore, few tools are available to detect copy-paste related bugs. In this paper we propose a tool, CP-Miner, that uses data mining techniques to efficiently identify copy-pasted code in large software including operating systems, and detects copy-paste related bugs. Specifically, it takes less than 20 minutes for CP-Miner to identify 190,000 copy-pasted segments in Linux and 150,000 in FreeBSD. Moreover, CP-Miner has detected 28 copy-paste related bugs in the latest version of Linux and 23 in FreeBSD. In addition, we analyze some interesting characteristics of copy-paste in Linux and FreeBSD, including the distribution of copy-pasted code across different length, granularity, modules, degrees of modification, and various software versions.",cp miner tool finding copy paste related bugs operating system code copy pasted code common large software programmers prefer reusing code via copy paste order reduce programming effort recent studies show copy paste prone introducing bugs significant portion operating system bugs concentrate copy pasted code unfortunately challenging efficiently identify copy pasted code large software existing copy paste detection tools either scalable large software handle small modifications copy pasted code furthermore tools available detect copy paste related bugs paper propose tool cp miner uses data mining techniques efficiently identify copy pasted code large software including operating systems detects copy paste related bugs specifically takes less 20 minutes cp miner identify 190 000 copy pasted segments linux 150 000 freebsd moreover cp miner detected 28 copy paste related bugs latest version linux 23 freebsd addition analyze interesting characteristics copy paste linux freebsd including distribution copy pasted code across different length granularity modules degrees modification various software versions,0,1,0,1,3,2
34,A systematic review of machine learning techniques for software fault prediction,Machine learning; Software fault proneness; Systematic literature review,"Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work. {\copyright} 2014 Elsevier B.V. All rights reserved.",0,"A systematic review of machine learning techniques for software fault prediction. Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work. {\copyright} 2014 Elsevier B.V. All rights reserved.",systematic review machine learning techniques software fault prediction background software fault prediction process developing models used software practitioners early phases software development life cycle detecting faulty constructs modules classes various machine learning techniques used past predicting faults method study perform systematic review studies january 1991 october 2013 literature use machine learning techniques software fault prediction assess performance capability machine learning techniques existing research software fault prediction also compare performance machine learning techniques statistical techniques machine learning techniques strengths weaknesses machine learning techniques summarized results paper identified 64 primary studies seven categories machine learning techniques results prove prediction capability machine learning techniques classifying module class fault prone fault prone models using machine learning techniques estimating software fault proneness outperform traditional statistical models conclusion based results obtained systematic review conclude machine learning techniques ability predicting software fault proneness used software practitioners researchers however application machine learning techniques software fault prediction still limited number studies carried order obtain well formed generalizable results provide future guidelines practitioners researchers based results obtained work copyright 2014 elsevier b v rights reserved,1,1,2,1,0,0
35,Foundations of Voice Studies: An Interdisciplinary Approach to Voice Production and Perception,,"Foundations of Voice Studies provides a comprehensive description and analysis of the multifaceted role that voice quality plays in human existence. Offers a unique interdisciplinary perspective on all facets of voice perception, illustrating why listeners hear what they do and how they reach conclusions based on voice quality. Integrates voice literature from a multitude of sources and disciplines. Supplemented with practical and approachable examples, including a companion website with sound files at www.wiley.com/go/voicestudies. Explores the choice of various voices in advertising and broadcasting, and voice perception in singing voices and forensic applications. Provides a straightforward and thorough overview of vocal physiology and control. {\copyright} 2011 Jody Kreiman and Diana Sidtis.",0,"Foundations of Voice Studies: An Interdisciplinary Approach to Voice Production and Perception. Foundations of Voice Studies provides a comprehensive description and analysis of the multifaceted role that voice quality plays in human existence. Offers a unique interdisciplinary perspective on all facets of voice perception, illustrating why listeners hear what they do and how they reach conclusions based on voice quality. Integrates voice literature from a multitude of sources and disciplines. Supplemented with practical and approachable examples, including a companion website with sound files at www.wiley.com/go/voicestudies. Explores the choice of various voices in advertising and broadcasting, and voice perception in singing voices and forensic applications. Provides a straightforward and thorough overview of vocal physiology and control. {\copyright} 2011 Jody Kreiman and Diana Sidtis.",foundations voice studies interdisciplinary approach voice production perception foundations voice studies provides comprehensive description analysis multifaceted role voice quality plays human existence offers unique interdisciplinary perspective facets voice perception illustrating listeners hear reach conclusions based voice quality integrates voice literature multitude sources disciplines supplemented practical approachable examples including companion website sound files www wiley com go voicestudies explores choice various voices advertising broadcasting voice perception singing voices forensic applications provides straightforward thorough overview vocal physiology control copyright 2011 jody kreiman diana sidtis,2,0,1,2,1,3
36,What science offers the humanities: Integrating body and culture,,"What Science Offers the Humanities examines some of the deep problems facing current approaches to the study of culture. It focuses on the excesses of postmodernism, but also acknowledges serious problems with postmodernism's harshest critics. In short, Edward Slingerland argues that in order for the humanities to progress, its scholars need to take seriously contributions from the natural sciences - and particular research on human cognition - which demonstrate that any separation of the mind and the body is entirely untenable. The author provides suggestions for how humanists might begin to utilize these scientific discoveries without conceding that science has the last word on morality, religion, art, and literature. Calling into question such deeply entrenched dogmas as the `blank slate' theory of nature, strong social constructivism, and the ideal of disembodied reason, What Science Offers the Humanities replaces the human-sciences divide with a more integrated approach to the study of culture. {\copyright} Edward Slingerland 2008.",0,"What science offers the humanities: Integrating body and culture. What Science Offers the Humanities examines some of the deep problems facing current approaches to the study of culture. It focuses on the excesses of postmodernism, but also acknowledges serious problems with postmodernism's harshest critics. In short, Edward Slingerland argues that in order for the humanities to progress, its scholars need to take seriously contributions from the natural sciences - and particular research on human cognition - which demonstrate that any separation of the mind and the body is entirely untenable. The author provides suggestions for how humanists might begin to utilize these scientific discoveries without conceding that science has the last word on morality, religion, art, and literature. Calling into question such deeply entrenched dogmas as the `blank slate' theory of nature, strong social constructivism, and the ideal of disembodied reason, What Science Offers the Humanities replaces the human-sciences divide with a more integrated approach to the study of culture. {\copyright} Edward Slingerland 2008.",science offers humanities integrating body culture science offers humanities examines deep problems facing current approaches study culture focuses excesses postmodernism also acknowledges serious problems postmodernism harshest critics short edward slingerland argues order humanities progress scholars need take seriously contributions natural sciences particular research human cognition demonstrate separation mind body entirely untenable author provides suggestions humanists might begin utilize scientific discoveries without conceding science last word morality religion art literature calling question deeply entrenched dogmas blank slate theory nature strong social constructivism ideal disembodied reason science offers humanities replaces human sciences divide integrated approach study culture copyright edward slingerland 2008,2,0,1,2,1,3
37,"Memory, Attention, and Decision-Making: A Unifying Computational Neuroscience Approach",Atention; Computational neuroscience; Decision-making; Emotion; Perception; Short-term memory; Smell; Taste; Vision; Visual object recognition,"This book presents a unified approach to understanding memory, attention, and decision-making. It shows how these fundamental functions for cognitive neuroscience can be understood in a common and unifying computational neuroscience framework. This framework links empirical research on brain function from neurophysiology, functional neuroimaging, and the effects of brain damage, to a description of how neural networks in the brain implement these functions using a set of common principles. The book describes the principles of operation of these networks, and how they could implement such important functions as memory, attention, and decision-making. The book discusses the hippocampus and memory, reward- and punishment-related learning, emotion and motivation, invariant visual object recognition learning, short-term memory, attention, biased competition, probabilistic decision-making, action selection, and decision-making. {\copyright} Edmund T Rolls, 2008. All rights reserved.",0,"Memory, Attention, and Decision-Making: A Unifying Computational Neuroscience Approach. This book presents a unified approach to understanding memory, attention, and decision-making. It shows how these fundamental functions for cognitive neuroscience can be understood in a common and unifying computational neuroscience framework. This framework links empirical research on brain function from neurophysiology, functional neuroimaging, and the effects of brain damage, to a description of how neural networks in the brain implement these functions using a set of common principles. The book describes the principles of operation of these networks, and how they could implement such important functions as memory, attention, and decision-making. The book discusses the hippocampus and memory, reward- and punishment-related learning, emotion and motivation, invariant visual object recognition learning, short-term memory, attention, biased competition, probabilistic decision-making, action selection, and decision-making. {\copyright} Edmund T Rolls, 2008. All rights reserved.",memory attention decision making unifying computational neuroscience approach book presents unified approach understanding memory attention decision making shows fundamental functions cognitive neuroscience understood common unifying computational neuroscience framework framework links empirical research brain function neurophysiology functional neuroimaging effects brain damage description neural networks brain implement functions using set common principles book describes principles operation networks could implement important functions memory attention decision making book discusses hippocampus memory reward punishment related learning emotion motivation invariant visual object recognition learning short term memory attention biased competition probabilistic decision making action selection decision making copyright edmund rolls 2008 rights reserved,2,0,1,2,1,3
38,Software evolution,,"Software has become omnipresent and vital in our information-based society, so all software producers should assume responsibility for its reliability. While ""reliable"" originally assumed implementations that were effective and mainly error-free, additional issues like adaptability and maintainability have gained equal importance recently. For example, the 2004 ACM/IEEE Software Engineering Curriculum Guidelines list software evolution as one of ten key areas of software engineering education.Mens and Demeyer, both international authorities in the field of software evolution, together with the invited contributors, focus on novel trends in software evolution research and its relations with other emerging disciplines such as model-driven software engineering, service-oriented software development, and aspect-oriented software development. They do not restrict themselves to the evolution of source code but also address the evolution of other, equally important software artifacts such as databases and database schemas, design models, software architectures, and process management. The contributing authors provide broad overviews of related work, and they also contribute to a comprehensive glossary, a list of acronyms, and a list of books, journals, websites, standards and conferences that together represent the community's body of knowledge. Combining all these features, this book is the indispensable source for researchers and professionals looking for an introduction and comprehensive overview of the state of the art. In addition, it is an ideal basis for an advanced course on software evolution. {\copyright} Springer-Verlag Berlin Heidelberg 2008.",0,"Software evolution. Software has become omnipresent and vital in our information-based society, so all software producers should assume responsibility for its reliability. While ""reliable"" originally assumed implementations that were effective and mainly error-free, additional issues like adaptability and maintainability have gained equal importance recently. For example, the 2004 ACM/IEEE Software Engineering Curriculum Guidelines list software evolution as one of ten key areas of software engineering education.Mens and Demeyer, both international authorities in the field of software evolution, together with the invited contributors, focus on novel trends in software evolution research and its relations with other emerging disciplines such as model-driven software engineering, service-oriented software development, and aspect-oriented software development. They do not restrict themselves to the evolution of source code but also address the evolution of other, equally important software artifacts such as databases and database schemas, design models, software architectures, and process management. The contributing authors provide broad overviews of related work, and they also contribute to a comprehensive glossary, a list of acronyms, and a list of books, journals, websites, standards and conferences that together represent the community's body of knowledge. Combining all these features, this book is the indispensable source for researchers and professionals looking for an introduction and comprehensive overview of the state of the art. In addition, it is an ideal basis for an advanced course on software evolution. {\copyright} Springer-Verlag Berlin Heidelberg 2008.",software evolution software become omnipresent vital information based society software producers assume responsibility reliability reliable originally assumed implementations effective mainly error free additional issues like adaptability maintainability gained equal importance recently example 2004 acm ieee software engineering curriculum guidelines list software evolution one ten key areas software engineering education mens demeyer international authorities field software evolution together invited contributors focus novel trends software evolution research relations emerging disciplines model driven software engineering service oriented software development aspect oriented software development restrict evolution source code also address evolution equally important software artifacts databases database schemas design models software architectures process management contributing authors provide broad overviews related work also contribute comprehensive glossary list acronyms list books journals websites standards conferences together represent community body knowledge combining features book indispensable source researchers professionals looking introduction comprehensive overview state art addition ideal basis advanced course software evolution copyright springer verlag berlin heidelberg 2008,2,0,1,1,2,2
39,Revisiting the impact of classification techniques on the performance of defect prediction models,,"Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others. {\copyright} 2015 IEEE.",0,"Revisiting the impact of classification techniques on the performance of defect prediction models. Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others. {\copyright} 2015 IEEE.",revisiting impact classification techniques performance defect prediction models defect prediction models help software quality assurance teams effectively allocate limited resources defect prone software modules variety classification techniques used build defect prediction models ranging simple e g logistic regression advanced techniques e g multivariate adaptive regression splines mars surprisingly recent research nasa dataset suggests performance defect prediction model significantly impacted classification technique used train however dataset used prior study noisy e contains erroneous entries b biased e contains software developed one setting hence set replicate prior study two experimental settings first apply replicated procedure known noisy nasa dataset derive similar results prior study e impact classification techniques appear minimal next apply replicated procedure two new datasets cleaned version nasa dataset b promise dataset contains open source software developed variety settings e g apache gnu results new datasets show clear statistically distinct separation groups techniques e choice classification technique impact performance defect prediction models indeed contrary earlier research results suggest classification techniques tend produce defect prediction models outperform others copyright 2015 ieee,1,1,0,1,0,0
40,Learning NATURAL coding conventions,Coding conventions; Naturalness of software,"Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project's coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94% accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted. Copyright 2014 ACM.",0,"Learning NATURAL coding conventions. Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project's coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94% accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted. Copyright 2014 ACM.",learning natural coding conventions every programmer characteristic style ranging preferences identifier naming preferences object relationships design patterns coding conventions define consistent syntactic style fostering readability hence maintainability collaborating programmers strive obey project coding conventions however one third reviews changes contain feedback coding conventions indicating programmers always follow project members care deeply adherence unfortunately programmers often unaware coding conventions inferring requires global view one aggregates many local decisions programmers make identifies emergent consensus style present naturalize framework learns style codebase suggests revisions improve stylistic consistency naturalize builds recent work applying statistical natural language processing source code apply naturalize suggest natural identifier names formatting conventions present four tools focused ensuring natural code development release management including code review naturalize achieves 94 accuracy top suggestions identifier names used naturalize generate 18 patches 5 open source projects 14 accepted copyright 2014 acm,1,2,1,1,3,2
41,Evolution of software in automated production systems: Challenges and research directions,Automated production systems; Automation; Evolution; Software engineering,"Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. {\copyright} 2015 Elsevier Inc. All rights reserved.",0,"Evolution of software in automated production systems: Challenges and research directions. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. {\copyright} 2015 Elsevier Inc. All rights reserved.",evolution software automated production systems challenges research directions coping evolution automated production systems implies cross disciplinary challenge along system life cycle variant rich systems high complexity authors computer science automation provide interdisciplinary survey challenges state art evolution automated production systems selected challenges illustrated case simple pick place unit first part paper discuss development process automated production systems well different type evolutions system life cycle case pick place unit second part survey challenges associated evolution different development phases couple cross cutting areas review existing approaches addressing challenges close summarizing future research directions address challenges evolution automated production systems copyright 2015 elsevier inc rights reserved,2,0,0,2,2,3
42,"User requirements notation: The first ten years, the next ten years",Goal-oriented requirement language (GRL); Goals; Modeling; Review; Scenarios; Tools; Use case maps (UCM); User requirements notation (URN),"The User Requirements Notation (URN), standardized by the International Telecommunication Union in 2008, is used to model and analyze requirements with goals and scenarios. This paper describes the first ten years of development of URN, and discusses ongoing efforts targeting the next ten years. We did a study inspired by the systematic literature review approach, querying five major search engines and using the existing URN Virtual Library. Based on the 281 scientific publications related to URN we collected and analyzed, we observe a shift from a more conventional use of URN for telecommunications and reactive systems to business process management and aspect-oriented modeling, with relevant extensions to the language being proposed. URN also benefits from a global and active research community, although industrial contributions are still sparse. URN is now a leading language for goal-driven and scenario-oriented modeling with a promising future for many application domains. {\copyright} 2011 ACADEMY PUBLISHER.",0,"User requirements notation: The first ten years, the next ten years. The User Requirements Notation (URN), standardized by the International Telecommunication Union in 2008, is used to model and analyze requirements with goals and scenarios. This paper describes the first ten years of development of URN, and discusses ongoing efforts targeting the next ten years. We did a study inspired by the systematic literature review approach, querying five major search engines and using the existing URN Virtual Library. Based on the 281 scientific publications related to URN we collected and analyzed, we observe a shift from a more conventional use of URN for telecommunications and reactive systems to business process management and aspect-oriented modeling, with relevant extensions to the language being proposed. URN also benefits from a global and active research community, although industrial contributions are still sparse. URN is now a leading language for goal-driven and scenario-oriented modeling with a promising future for many application domains. {\copyright} 2011 ACADEMY PUBLISHER.",user requirements notation first ten years next ten years user requirements notation urn standardized international telecommunication union 2008 used model analyze requirements goals scenarios paper describes first ten years development urn discusses ongoing efforts targeting next ten years study inspired systematic literature review approach querying five major search engines using existing urn virtual library based 281 scientific publications related urn collected analyzed observe shift conventional use urn telecommunications reactive systems business process management aspect oriented modeling relevant extensions language proposed urn also benefits global active research community although industrial contributions still sparse urn leading language goal driven scenario oriented modeling promising future many application domains copyright 2011 academy publisher,2,2,1,2,2,3
43,Discovering and representing systematic code changes,,"Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes. {\copyright} 2009 IEEE.",0,"Discovering and representing systematic code changes. Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes. {\copyright} 2009 IEEE.",discovering representing systematic code changes software engineers often inspect program differences reviewing others code changes writing check comments determining program behaves differently expected behavior modification program differencing tools support tasks limited ability group related code changes detect potential inconsistencies changes overcome limitations complement existing approaches built logical structural diff lsdiff tool infers systematic structural differences logic rules lsdiff notes anomalies systematic changes exceptions logic rules conducted focus group study professional software engineers large e commerce company also compared lsdiff results textual differences structural differences without rules evaluation suggests lsdiff complements existing differencing tools grouping code changes form systematic change patterns regardless distribution throughout code ability discover anomalies shows promise detecting inconsistent changes copyright 2009 ieee,0,2,1,1,3,2
44,Stepping into virtual reality,,"The fruit of many years experience on the creation of synthetic worlds and virtual realities, this book is based on the considerable expertise of the authors, who share their knowledge of mastering the complexities behind the creation of Virtual Reality (VR) applications. The first part of the book reviews the basic theoretical and practical concepts involved in the visual aspect of virtual environments. Part 2 provides more details on the components, structure and types of virtual worlds that can be created, including detailed explanations of the main modeling and animation techniques for virtual characters - one of the most important aspects in a virtual world. A review and discussion of the main types of VR system architectures defines the different alternatives for organizing and designing a VR application. The final part covers the main principles of Virtual Reality hardware using a generic classification of interaction devices based on a human-centered approach (via the five human senses: vision, sound, touch, smell and taste). The book closes with an overview of successful VR systems and applications and gives a glimpse of what lies in the future. This book was conceived as a guided tour and provides practical explanations of each step in the process of creating a Virtual Reality application. It can be used both as textbook for a Virtual Reality course, and as a reference for courses covering computer graphics, computer animation or human-computer interaction topics. {\copyright} Springer-Verlag London Limited 2008.",0,"Stepping into virtual reality. The fruit of many years experience on the creation of synthetic worlds and virtual realities, this book is based on the considerable expertise of the authors, who share their knowledge of mastering the complexities behind the creation of Virtual Reality (VR) applications. The first part of the book reviews the basic theoretical and practical concepts involved in the visual aspect of virtual environments. Part 2 provides more details on the components, structure and types of virtual worlds that can be created, including detailed explanations of the main modeling and animation techniques for virtual characters - one of the most important aspects in a virtual world. A review and discussion of the main types of VR system architectures defines the different alternatives for organizing and designing a VR application. The final part covers the main principles of Virtual Reality hardware using a generic classification of interaction devices based on a human-centered approach (via the five human senses: vision, sound, touch, smell and taste). The book closes with an overview of successful VR systems and applications and gives a glimpse of what lies in the future. This book was conceived as a guided tour and provides practical explanations of each step in the process of creating a Virtual Reality application. It can be used both as textbook for a Virtual Reality course, and as a reference for courses covering computer graphics, computer animation or human-computer interaction topics. {\copyright} Springer-Verlag London Limited 2008.",stepping virtual reality fruit many years experience creation synthetic worlds virtual realities book based considerable expertise authors share knowledge mastering complexities behind creation virtual reality vr applications first part book reviews basic theoretical practical concepts involved visual aspect virtual environments part 2 provides details components structure types virtual worlds created including detailed explanations main modeling animation techniques virtual characters one important aspects virtual world review discussion main types vr system architectures defines different alternatives organizing designing vr application final part covers main principles virtual reality hardware using generic classification interaction devices based human centered approach via five human senses vision sound touch smell taste book closes overview successful vr systems applications gives glimpse lies future book conceived guided tour provides practical explanations step process creating virtual reality application used textbook virtual reality course reference courses covering computer graphics computer animation human computer interaction topics copyright springer verlag london limited 2008,2,0,1,2,2,3
45,A bayesian approach for the detection of code and design smells,,"The presence of code and design smells can have a severe impact on the quality of a program. Consequently, their detection and correction have drawn the attention of both researchers and practitioners who have proposed various approaches to detect code and design smells in programs. However, none of these approaches handle the inherent uncertainty of the detection process. We propose a Bayesian approach to manage this uncertainty. First, we present a systematic process to convert existing state-of-the-art detection rules into a probabilistic model. We illustrate this process by generating a model to detect occurrences of the Blob antipattern. Second, we present results of the validation of the model: we built this model on two open-source programs, GanttProject v1.10.2 and Xerces v2.7.0, and measured its accuracy. Third, we compare our model with another approach to show that it returns the same candidate classes while ordering them to minimise the quality analysts' effort. Finally, we show that when past detection results are available, our model can be calibrated using machine learning techniques to offer an improved, context-specfic detection. {\copyright} 2009 IEEE.",1,"A bayesian approach for the detection of code and design smells. The presence of code and design smells can have a severe impact on the quality of a program. Consequently, their detection and correction have drawn the attention of both researchers and practitioners who have proposed various approaches to detect code and design smells in programs. However, none of these approaches handle the inherent uncertainty of the detection process. We propose a Bayesian approach to manage this uncertainty. First, we present a systematic process to convert existing state-of-the-art detection rules into a probabilistic model. We illustrate this process by generating a model to detect occurrences of the Blob antipattern. Second, we present results of the validation of the model: we built this model on two open-source programs, GanttProject v1.10.2 and Xerces v2.7.0, and measured its accuracy. Third, we compare our model with another approach to show that it returns the same candidate classes while ordering them to minimise the quality analysts' effort. Finally, we show that when past detection results are available, our model can be calibrated using machine learning techniques to offer an improved, context-specfic detection. {\copyright} 2009 IEEE.",bayesian approach detection code design smells presence code design smells severe impact quality program consequently detection correction drawn attention researchers practitioners proposed various approaches detect code design smells programs however none approaches handle inherent uncertainty detection process propose bayesian approach manage uncertainty first present systematic process convert existing state art detection rules probabilistic model illustrate process generating model detect occurrences blob antipattern second present results validation model built model two open source programs ganttproject v1 10 2 xerces v2 7 0 measured accuracy third compare model another approach show returns candidate classes ordering minimise quality analysts effort finally show past detection results available model calibrated using machine learning techniques offer improved context specfic detection copyright 2009 ieee,0,2,2,0,3,1
46,Automated API property inference techniques,API evolution; API property; API usage pattern; data mining; interface; pattern mining; programming rules; protocols; specifications,"Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information. {\copyright} 1976-2012 IEEE.",0,"Automated API property inference techniques. Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information. {\copyright} 1976-2012 IEEE.",automated api property inference techniques frameworks libraries offer reusable customizable functionality application programming interfaces apis correctly using large sophisticated apis represent challenge due hidden assumptions requirements numerous approaches developed infer properties apis intended guide use developers approach come new definitions api properties new techniques inferring properties new ways assess correctness usefulness paper provides comprehensive survey decade research automated property inference apis survey provides synthesis complex technical field along different dimensions analysis properties inferred mining techniques empirical results particular derive classification organization 60 techniques five different categories based type api property inferred unordered usage patterns sequential usage patterns behavioral specifications migration mappings general information copyright 1976 2012 ieee,0,2,0,0,0,2
47,Innovative diagnostic tools for early detection of Alzheimer's disease,Alzheimer's disease; Diagnostic tools; Early detection; Noninvasive tests; Screening tests,"Current state-of-the-art diagnostic measures of Alzheimer's disease (AD) are invasive (cerebrospinal fluid analysis), expensive (neuroimaging) and time-consuming (neuropsychological assessment) and thus have limited accessibility as frontline screening and diagnostic tools for AD. Thus, there is an increasing need for additional noninvasive and/or cost-effective tools, allowing identification of subjects in the preclinical or early clinical stages of AD who could be suitable for further cognitive evaluation and dementia diagnostics. Implementation of such tests may facilitate early and potentially more effective therapeutic and preventative strategies for AD. Before applying them in clinical practice, these tools should be examined in ongoing large clinical trials. This review will summarize and highlight the most promising screening tools including neuropsychometric, clinical, blood, and neurophysiological tests. {\copyright} 2015 The Alzheimer's Association.",0,"Innovative diagnostic tools for early detection of Alzheimer's disease. Current state-of-the-art diagnostic measures of Alzheimer's disease (AD) are invasive (cerebrospinal fluid analysis), expensive (neuroimaging) and time-consuming (neuropsychological assessment) and thus have limited accessibility as frontline screening and diagnostic tools for AD. Thus, there is an increasing need for additional noninvasive and/or cost-effective tools, allowing identification of subjects in the preclinical or early clinical stages of AD who could be suitable for further cognitive evaluation and dementia diagnostics. Implementation of such tests may facilitate early and potentially more effective therapeutic and preventative strategies for AD. Before applying them in clinical practice, these tools should be examined in ongoing large clinical trials. This review will summarize and highlight the most promising screening tools including neuropsychometric, clinical, blood, and neurophysiological tests. {\copyright} 2015 The Alzheimer's Association.",innovative diagnostic tools early detection alzheimer disease current state art diagnostic measures alzheimer disease ad invasive cerebrospinal fluid analysis expensive neuroimaging time consuming neuropsychological assessment thus limited accessibility frontline screening diagnostic tools ad thus increasing need additional noninvasive cost effective tools allowing identification subjects preclinical early clinical stages ad could suitable cognitive evaluation dementia diagnostics implementation tests may facilitate early potentially effective therapeutic preventative strategies ad applying clinical practice tools examined ongoing large clinical trials review summarize highlight promising screening tools including neuropsychometric clinical blood neurophysiological tests copyright 2015 alzheimer association,0,0,1,2,2,3
48,Revisiting common bug prediction findings using effort-aware models,,"Bug prediction models are often used to help allocate software quality assurance efforts (e.g. testing and code reviews). Mende and Koschke have recently proposed bug prediction models that are effort-aware. These models factor in the effort needed to review or test code when evaluating the effectiveness of prediction models, leading to more realistic performance evaluations. In this paper, we revisit two common findings in the bug prediction literature: 1) Process metrics (e.g., change history) outperform product metrics (e.g., LOC), 2) Packagelevel predictions outperform file-level predictions. Through a case study on three projects from the Eclipse Foundation, we find that the first finding holds when effort is considered, while the second finding does not hold. These findings validate the practical significance of prior findings in the bug prediction literature and encourage their adoption in practice. {\copyright} 2010 IEEE.",0,"Revisiting common bug prediction findings using effort-aware models. Bug prediction models are often used to help allocate software quality assurance efforts (e.g. testing and code reviews). Mende and Koschke have recently proposed bug prediction models that are effort-aware. These models factor in the effort needed to review or test code when evaluating the effectiveness of prediction models, leading to more realistic performance evaluations. In this paper, we revisit two common findings in the bug prediction literature: 1) Process metrics (e.g., change history) outperform product metrics (e.g., LOC), 2) Packagelevel predictions outperform file-level predictions. Through a case study on three projects from the Eclipse Foundation, we find that the first finding holds when effort is considered, while the second finding does not hold. These findings validate the practical significance of prior findings in the bug prediction literature and encourage their adoption in practice. {\copyright} 2010 IEEE.",revisiting common bug prediction findings using effort aware models bug prediction models often used help allocate software quality assurance efforts e g testing code reviews mende koschke recently proposed bug prediction models effort aware models factor effort needed review test code evaluating effectiveness prediction models leading realistic performance evaluations paper revisit two common findings bug prediction literature 1 process metrics e g change history outperform product metrics e g loc 2 packagelevel predictions outperform file level predictions case study three projects eclipse foundation find first finding holds effort considered second finding hold findings validate practical significance prior findings bug prediction literature encourage adoption practice copyright 2010 ieee,1,2,2,1,0,0
49,Subjective evaluation of software evolvability using code smells: An empirical study,Code metrics; Code smells; Evolvability; Human factors; Maintainability; Perceived evaluation; Software metrics; Subjective evaluation,"This paper presents the results of an empirical study on the subjective evaluation of code smells that identify poorly evolvable structures in software. We propose use of the term software evolvability to describe the ease of further developing a piece of software and outline the research area based on four different viewpoints. Furthermore, we describe the differences between human evaluations and automatic program analysis based on software evolvability metrics. The empirical component is based on a case study in a Finnish software product company, in which we studied two topics. First, we looked at the effect of the evaluator when subjectively evaluating the existence of smells in code modules. We found that the use of smells for code evaluation purposes can be difficult due to conflicting perceptions of different evaluators. However, the demographics of the evaluators partly explain the variation. Second, we applied selected source code metrics for identifying four smells and compared these results to the subjective evaluations. The metrics based on automatic program analysis and the human-based smell evaluations did not fully correlate. Based upon our results, we suggest that organizations should make decisions regarding software evolvability improvement based on a combination of subjective evaluations and code metrics. Due to the limitations of the study we also recognize the need for conducting more refined studies and experiments in the area of software evolvability.",0,"Subjective evaluation of software evolvability using code smells: An empirical study. This paper presents the results of an empirical study on the subjective evaluation of code smells that identify poorly evolvable structures in software. We propose use of the term software evolvability to describe the ease of further developing a piece of software and outline the research area based on four different viewpoints. Furthermore, we describe the differences between human evaluations and automatic program analysis based on software evolvability metrics. The empirical component is based on a case study in a Finnish software product company, in which we studied two topics. First, we looked at the effect of the evaluator when subjectively evaluating the existence of smells in code modules. We found that the use of smells for code evaluation purposes can be difficult due to conflicting perceptions of different evaluators. However, the demographics of the evaluators partly explain the variation. Second, we applied selected source code metrics for identifying four smells and compared these results to the subjective evaluations. The metrics based on automatic program analysis and the human-based smell evaluations did not fully correlate. Based upon our results, we suggest that organizations should make decisions regarding software evolvability improvement based on a combination of subjective evaluations and code metrics. Due to the limitations of the study we also recognize the need for conducting more refined studies and experiments in the area of software evolvability.",subjective evaluation software evolvability using code smells empirical study paper presents results empirical study subjective evaluation code smells identify poorly evolvable structures software propose use term software evolvability describe ease developing piece software outline research area based four different viewpoints furthermore describe differences human evaluations automatic program analysis based software evolvability metrics empirical component based case study finnish software product company studied two topics first looked effect evaluator subjectively evaluating existence smells code modules found use smells code evaluation purposes difficult due conflicting perceptions different evaluators however demographics evaluators partly explain variation second applied selected source code metrics identifying four smells compared results subjective evaluations metrics based automatic program analysis human based smell evaluations fully correlate based upon results suggest organizations make decisions regarding software evolvability improvement based combination subjective evaluations code metrics due limitations study also recognize need conducting refined studies experiments area software evolvability,0,1,2,0,3,1
50,Individual variation in resisting temptation: Implications for addiction,Accumbens; Binge eating; Dopamine; Goal-tracking; Human; Individual differences; Learning; Motivation; Obesity; Pavlovian; Rat; Relapse; Sign-tracking,"When exposed to the sights, sounds, smells and/or places that have been associated with rewards, such as food or drugs, some individuals have difficulty resisting the temptation to seek out and consume them. Others have less difficulty restraining themselves. Thus, Pavlovian reward cues may motivate maladaptive patterns of behavior to a greater extent in some individuals than in others. We are just beginning to understand the factors underlying individual differences in the extent to which reward cues acquire powerful motivational properties, and therefore, the ability to act as incentive stimuli. Here we review converging evidence from studies in both human and non-human animals suggesting that a subset of individuals are more ""cue reactive"", in that certain reward cues are more likely to attract these individuals to them and motivate actions to get them. We suggest that those individuals for whom Pavlovian reward cues become especially powerful incentives may be more vulnerable to impulse control disorders, such as binge eating and addiction. {\copyright} 2013 Elsevier Ltd.",0,"Individual variation in resisting temptation: Implications for addiction. When exposed to the sights, sounds, smells and/or places that have been associated with rewards, such as food or drugs, some individuals have difficulty resisting the temptation to seek out and consume them. Others have less difficulty restraining themselves. Thus, Pavlovian reward cues may motivate maladaptive patterns of behavior to a greater extent in some individuals than in others. We are just beginning to understand the factors underlying individual differences in the extent to which reward cues acquire powerful motivational properties, and therefore, the ability to act as incentive stimuli. Here we review converging evidence from studies in both human and non-human animals suggesting that a subset of individuals are more ""cue reactive"", in that certain reward cues are more likely to attract these individuals to them and motivate actions to get them. We suggest that those individuals for whom Pavlovian reward cues become especially powerful incentives may be more vulnerable to impulse control disorders, such as binge eating and addiction. {\copyright} 2013 Elsevier Ltd.",individual variation resisting temptation implications addiction exposed sights sounds smells places associated rewards food drugs individuals difficulty resisting temptation seek consume others less difficulty restraining thus pavlovian reward cues may motivate maladaptive patterns behavior greater extent individuals others beginning understand factors underlying individual differences extent reward cues acquire powerful motivational properties therefore ability act incentive stimuli review converging evidence studies human non human animals suggesting subset individuals cue reactive certain reward cues likely attract individuals motivate actions get suggest individuals pavlovian reward cues become especially powerful incentives may vulnerable impulse control disorders binge eating addiction copyright 2013 elsevier ltd,2,0,1,2,1,3
51,Neuromarketing: Exploring the brain of the consumer,,"Over the last 10 years advances in the new field of neuromarketing have yielded a host of findings which defy common stereotypes about consumer behavior. Reason and emotions do not necessarily appear as opposing forces. Rather, they complement one another. Hence, it reveals that consumers utilize mental accounting processes different from those assumed in marketers' logical inferences when it comes to time, problems with rating and choosing, and in post-purchase evaluation. People are often guided by illusions not only when they perceive the outside world but also when planning their actions - and consumer behavior is no exception. Strengthening the control over their own desires and the ability to navigate the maze of data are crucial skills consumers can gain to benefit themselves, marketers and the public. Understanding the mind of the consumer is the hardest task faced by business researchers. This book presents the first analytical perspective on the brain - and biometric studies which open a new frontier in market research. {\copyright} Springer-Verlag Berlin Heidelberg 2010.",0,"Neuromarketing: Exploring the brain of the consumer. Over the last 10 years advances in the new field of neuromarketing have yielded a host of findings which defy common stereotypes about consumer behavior. Reason and emotions do not necessarily appear as opposing forces. Rather, they complement one another. Hence, it reveals that consumers utilize mental accounting processes different from those assumed in marketers' logical inferences when it comes to time, problems with rating and choosing, and in post-purchase evaluation. People are often guided by illusions not only when they perceive the outside world but also when planning their actions - and consumer behavior is no exception. Strengthening the control over their own desires and the ability to navigate the maze of data are crucial skills consumers can gain to benefit themselves, marketers and the public. Understanding the mind of the consumer is the hardest task faced by business researchers. This book presents the first analytical perspective on the brain - and biometric studies which open a new frontier in market research. {\copyright} Springer-Verlag Berlin Heidelberg 2010.",neuromarketing exploring brain consumer last 10 years advances new field neuromarketing yielded host findings defy common stereotypes consumer behavior reason emotions necessarily appear opposing forces rather complement one another hence reveals consumers utilize mental accounting processes different assumed marketers logical inferences comes time problems rating choosing post purchase evaluation people often guided illusions perceive outside world also planning actions consumer behavior exception strengthening control desires ability navigate maze data crucial skills consumers gain benefit marketers public understanding mind consumer hardest task faced business researchers book presents first analytical perspective brain biometric studies open new frontier market research copyright springer verlag berlin heidelberg 2010,2,0,1,2,1,3
52,Comparing and experimenting machine learning techniques for code smell detection,Benchmark for code smell detection; Code smells detection; Machine learning techniques,"Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (>96 %), and only a hundred training examples are needed to reach at least 95 % accuracy. {\copyright} 2015, Springer Science+Business Media New York.",1,"Comparing and experimenting machine learning techniques for code smell detection. Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (>96 %), and only a hundred training examples are needed to reach at least 95 % accuracy. {\copyright} 2015, Springer Science+Business Media New York.",comparing experimenting machine learning techniques code smell detection several code smell detection tools developed providing different results smells subjectively interpreted hence detected different ways paper perform largest experiment applying machine learning algorithms code smells best knowledge experiment 16 different machine learning algorithms four code smells data class large class feature envy long method 74 software systems 1986 manually validated code smell samples found algorithms achieved high performances cross validation data set yet highest performances obtained j48 random forest worst performance achieved support vector machines however lower prevalence code smells e imbalanced data entire data set caused varying performances need addressed future studies conclude application machine learning detection code smells provide high accuracy 96 hundred training examples needed reach least 95 accuracy copyright 2015 springer science business media new york,0,2,0,0,3,1
53,A quantitative investigation of the acceptable risk levels of object-oriented metrics in open-source systems,CK metrics; Object-oriented programming; Open-source software; Product metrics; Threshold values,"Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source systemthe-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects. {\copyright} 2006 IEEE.",0,"A quantitative investigation of the acceptable risk levels of object-oriented metrics in open-source systems. Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source systemthe-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects. {\copyright} 2006 IEEE.",quantitative investigation acceptable risk levels object oriented metrics open source systems object oriented metrics validated empirically measures design complexity metrics used mitigate potential problems software complexity however studies conducted formulate guidelines represented threshold values interpret complexity software design using metrics classes clustered low high risk levels using threshold values paper use statistical model derived logistic regression identify threshold values chidamber kemerer ck metrics methodology validated empirically large open source systemthe eclipse project empirical results indicate ck metrics threshold effects various risk levels validated use thresholds next release eclipse project version 2 1 using decision trees addition selected threshold values accurate selected based either intuitive perspectives data distribution parameters furthermore proposed model exploited find risk level arbitrary threshold value findings suggest relationship risk levels object oriented metrics risk levels used identify threshold effects copyright 2006 ieee,1,2,2,1,0,2
54,Software defect prediction using relational association rule mining,Association rule; Data mining; Defect prediction; Software engineering,"This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. {\copyright} 2014 Elsevier Inc. All rights reserved.",0,"Software defect prediction using relational association rule mining. This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. {\copyright} 2014 Elsevier Inc. All rights reserved.",software defect prediction using relational association rule mining paper focuses problem defect prediction problem major importance software maintenance evolution essential software developers identify defective software modules order continuously improve quality software system conditions software module defects hard identify machine learning based classification models still developed approach problem defect prediction propose novel classification model based relational association rules mining relational association rules extension ordinal association rules particular type association rules describe numerical orderings attributes commonly occur dataset classifier based discovery relational association rules predicting whether software module defective experimental evaluation proposed model open source nasa datasets well comparison similar existing approaches provided obtained results show classifier overperforms considered evaluation measures existing machine learning based techniques defect prediction confirms potential proposal copyright 2014 elsevier inc rights reserved,1,2,0,1,0,0
55,Maintainability defects detection and correction: A multi-objective approach,By example; Effort; Maintainability defects; Multi-objective optimization; Search-based software engineering; Software maintenance,"Software defects often lead to bugs, runtime errors and software maintenance difficulties. They should be systematically prevented, found, removed or fixed all along the software lifecycle. However, detecting and fixing these defects is still, to some extent, a difficult, time-consuming and manual process. In this paper, we propose a two-step automated approach to detect and then to correct various types of maintainability defects in source code. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. Then, we correct the detected defects while minimizing the correction effort. A correction solution is defined as the combination of refactoring operations that should maximize as much as possible the number of corrected defects with minimal code modification effort. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise. For six open source projects, we succeeded in detecting the majority of known defects, and the proposed corrections fixed most of them with minimal effort. {\copyright} 2012 Springer Science+Business Media, LLC.",0,"Maintainability defects detection and correction: A multi-objective approach. Software defects often lead to bugs, runtime errors and software maintenance difficulties. They should be systematically prevented, found, removed or fixed all along the software lifecycle. However, detecting and fixing these defects is still, to some extent, a difficult, time-consuming and manual process. In this paper, we propose a two-step automated approach to detect and then to correct various types of maintainability defects in source code. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. Then, we correct the detected defects while minimizing the correction effort. A correction solution is defined as the combination of refactoring operations that should maximize as much as possible the number of corrected defects with minimal code modification effort. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise. For six open source projects, we succeeded in detecting the majority of known defects, and the proposed corrections fixed most of them with minimal effort. {\copyright} 2012 Springer Science+Business Media, LLC.",maintainability defects detection correction multi objective approach software defects often lead bugs runtime errors software maintenance difficulties systematically prevented found removed fixed along software lifecycle however detecting fixing defects still extent difficult time consuming manual process paper propose two step automated approach detect correct various types maintainability defects source code using genetic programming approach allows automatic generation rules detect defects thus relieving designer fastidious manual rule definition task correct detected defects minimizing correction effort correction solution defined combination refactoring operations maximize much possible number corrected defects minimal code modification effort use non dominated sorting genetic algorithm nsga ii find best compromise six open source projects succeeded detecting majority known defects proposed corrections fixed minimal effort copyright 2012 springer science business media llc,0,2,2,1,3,2
56,On the ability of complexity metrics to predict fault-prone classes in object-oriented systems,Complexity; Concordant pairs; Fault-prone; Logistic regression; Metrics; Odds ratio; Prediction,"Many studies use logistic regression models to investigate the ability of complexity metrics to predict fault-prone classes. However, it is not uncommon to see the inappropriate use of performance indictors such as odds ratio in previous studies. In particular, a recent study by Olague et al. uses the odds ratio associated with one unit increase in a metric to compare the relative magnitude of the associations between individual metrics and fault-proneness. In addition, the percents of concordant, discordant, and tied pairs are used to evaluate the predictive effectiveness of a univariate logistic regression model. Their results suggest that lesser known complexity metrics such as standard deviation method complexity (SDMC) and average method complexity (AMC) are better predictors than the two commonly used metrics: lines of code (LOC) and weighted method McCabe complexity (WMC). In this paper, however, we show that (1) the odds ratio associated with one standard deviation increase, rather than one unit increase, in a metric should be used to compare the relative magnitudes of the effects of individual metrics on fault-proneness. Otherwise, misleading results may be obtained; and that (2) the connection of the percents of concordant, discordant, and tied pairs with the predictive effectiveness of a univariate logistic regression model is false, as they indeed do not depend on the model. Furthermore, we use the data collected from three versions of Eclipse to re-examine the ability of complexity metrics to predict fault-proneness. Our experimental results reveal that: (1) many metrics exhibit moderate or almost moderate ability in discriminating between fault-prone and not fault-prone classes; (2) LOC and WMC are indeed better fault-proneness predictors than SDMC and AMC; and (3) the explanatory power of other complexity metrics in addition to LOC is limited. {\copyright} 2009 Elsevier Inc. All rights reserved.",0,"On the ability of complexity metrics to predict fault-prone classes in object-oriented systems. Many studies use logistic regression models to investigate the ability of complexity metrics to predict fault-prone classes. However, it is not uncommon to see the inappropriate use of performance indictors such as odds ratio in previous studies. In particular, a recent study by Olague et al. uses the odds ratio associated with one unit increase in a metric to compare the relative magnitude of the associations between individual metrics and fault-proneness. In addition, the percents of concordant, discordant, and tied pairs are used to evaluate the predictive effectiveness of a univariate logistic regression model. Their results suggest that lesser known complexity metrics such as standard deviation method complexity (SDMC) and average method complexity (AMC) are better predictors than the two commonly used metrics: lines of code (LOC) and weighted method McCabe complexity (WMC). In this paper, however, we show that (1) the odds ratio associated with one standard deviation increase, rather than one unit increase, in a metric should be used to compare the relative magnitudes of the effects of individual metrics on fault-proneness. Otherwise, misleading results may be obtained; and that (2) the connection of the percents of concordant, discordant, and tied pairs with the predictive effectiveness of a univariate logistic regression model is false, as they indeed do not depend on the model. Furthermore, we use the data collected from three versions of Eclipse to re-examine the ability of complexity metrics to predict fault-proneness. Our experimental results reveal that: (1) many metrics exhibit moderate or almost moderate ability in discriminating between fault-prone and not fault-prone classes; (2) LOC and WMC are indeed better fault-proneness predictors than SDMC and AMC; and (3) the explanatory power of other complexity metrics in addition to LOC is limited. {\copyright} 2009 Elsevier Inc. All rights reserved.",ability complexity metrics predict fault prone classes object oriented systems many studies use logistic regression models investigate ability complexity metrics predict fault prone classes however uncommon see inappropriate use performance indictors odds ratio previous studies particular recent study olague et al uses odds ratio associated one unit increase metric compare relative magnitude associations individual metrics fault proneness addition percents concordant discordant tied pairs used evaluate predictive effectiveness univariate logistic regression model results suggest lesser known complexity metrics standard deviation method complexity sdmc average method complexity amc better predictors two commonly used metrics lines code loc weighted method mccabe complexity wmc paper however show 1 odds ratio associated one standard deviation increase rather one unit increase metric used compare relative magnitudes effects individual metrics fault proneness otherwise misleading results may obtained 2 connection percents concordant discordant tied pairs predictive effectiveness univariate logistic regression model false indeed depend model furthermore use data collected three versions eclipse examine ability complexity metrics predict fault proneness experimental results reveal 1 many metrics exhibit moderate almost moderate ability discriminating fault prone fault prone classes 2 loc wmc indeed better fault proneness predictors sdmc amc 3 explanatory power complexity metrics addition loc limited copyright 2009 elsevier inc rights reserved,1,1,0,1,0,2
57,International Consensus Statement on Allergy and Rhinology: Allergic Rhinitis,allergen extract; allergen immunotherapy; allergic rhinitis; allergy; antihistamine; asthma; atopic dermatitis; avoidance; biologic; cockroach; conjunctivitis; consensus; corticosteroid; cough; cromolyn; decongestant; environment; eosinophilic esophagitis; epicutaneous immunotherapy; epidemiology; evidence-based medicine; food allergy; genetics; house dust mite; IgE; immunoglobulin E; immunotherapy; inhalant allergy; leukotriene; microbiome; occupational rhinitis; omalizumab; pathophysiology; perennial; pet dander; pollen; probiotic; quality of life; rhinitis; rhinitis; rhinosinusitis; risk factor; saline; seasonal; sensitization; sinusitis; sleep; socioeconomic; specific IgE; subcutaneous immunotherapy; sublingual immunotherapy; systematic review; total IgE; transcutaneous immunotherapy; validated survey,"Background: Critical examination of the quality and validity of available allergic rhinitis (AR) literature is necessary to improve understanding and to appropriately translate this knowledge to clinical care of the AR patient. To evaluate the existing AR literature, international multidisciplinary experts with an interest in AR have produced the International Consensus statement on Allergy and Rhinology: Allergic Rhinitis (ICAR:AR). Methods: Using previously described methodology, specific topics were developed relating to AR. Each topic was assigned a literature review, evidence-based review (EBR), or evidence-based review with recommendations (EBRR) format as dictated by available evidence and purpose within the ICAR:AR document. Following iterative reviews of each topic, the ICAR:AR document was synthesized and reviewed by all authors for consensus. Results: The ICAR:AR document addresses over 100 individual topics related to AR, including diagnosis, pathophysiology, epidemiology, disease burden, risk factors for the development of AR, allergy testing modalities, treatment, and other conditions/comorbidities associated with AR. Conclusion: This critical review of the AR literature has identified several strengths; providers can be confident that treatment decisions are supported by rigorous studies. However, there are also substantial gaps in the AR literature. These knowledge gaps should be viewed as opportunities for improvement, as often the things that we teach and the medicine that we practice are not based on the best quality evidence. This document aims to highlight the strengths and weaknesses of the AR literature to identify areas for future AR research and improved understanding. {\copyright} 2018 ARS-AAOA, LLC",0,"International Consensus Statement on Allergy and Rhinology: Allergic Rhinitis. Background: Critical examination of the quality and validity of available allergic rhinitis (AR) literature is necessary to improve understanding and to appropriately translate this knowledge to clinical care of the AR patient. To evaluate the existing AR literature, international multidisciplinary experts with an interest in AR have produced the International Consensus statement on Allergy and Rhinology: Allergic Rhinitis (ICAR:AR). Methods: Using previously described methodology, specific topics were developed relating to AR. Each topic was assigned a literature review, evidence-based review (EBR), or evidence-based review with recommendations (EBRR) format as dictated by available evidence and purpose within the ICAR:AR document. Following iterative reviews of each topic, the ICAR:AR document was synthesized and reviewed by all authors for consensus. Results: The ICAR:AR document addresses over 100 individual topics related to AR, including diagnosis, pathophysiology, epidemiology, disease burden, risk factors for the development of AR, allergy testing modalities, treatment, and other conditions/comorbidities associated with AR. Conclusion: This critical review of the AR literature has identified several strengths; providers can be confident that treatment decisions are supported by rigorous studies. However, there are also substantial gaps in the AR literature. These knowledge gaps should be viewed as opportunities for improvement, as often the things that we teach and the medicine that we practice are not based on the best quality evidence. This document aims to highlight the strengths and weaknesses of the AR literature to identify areas for future AR research and improved understanding. {\copyright} 2018 ARS-AAOA, LLC",international consensus statement allergy rhinology allergic rhinitis background critical examination quality validity available allergic rhinitis ar literature necessary improve understanding appropriately translate knowledge clinical care ar patient evaluate existing ar literature international multidisciplinary experts interest ar produced international consensus statement allergy rhinology allergic rhinitis icar ar methods using previously described methodology specific topics developed relating ar topic assigned literature review evidence based review ebr evidence based review recommendations ebrr format dictated available evidence purpose within icar ar document following iterative reviews topic icar ar document synthesized reviewed authors consensus results icar ar document addresses 100 individual topics related ar including diagnosis pathophysiology epidemiology disease burden risk factors development ar allergy testing modalities treatment conditions comorbidities associated ar conclusion critical review ar literature identified several strengths providers confident treatment decisions supported rigorous studies however also substantial gaps ar literature knowledge gaps viewed opportunities improvement often things teach medicine practice based best quality evidence document aims highlight strengths weaknesses ar literature identify areas future ar research improved understanding copyright 2018 ars aaoa llc,2,1,1,2,2,3
58,Imaging brain function with EEG: Advanced temporal and spatial analysis of electroencephalographic signals,,"The scalp and cortex lie like pages of an open book on which the cortex enciphers vast quantities of information and knowledge. They are recorded and analyzed as temporal and spatial patterns in the electroencephalogram and electrocorticogram. This book describes basic tools and concepts needed to measure and decipher the patterns extracted from the EEG and ECoG. This book emphasizes the need for single trial analysis using new methods and paradigms, as well as large, high-density spatial arrays of electrodes for pattern sampling. The deciphered patterns reveal neural mechanisms by which brains process sensory information into precepts and concepts. It describes the brain as a thermodynamic system that uses chemical energy to construct knowledge. The results are intended for use in the search for the neural correlates of intention, attention, perception and learning; in the design of human brain-computer interfaces enabling mental control of machines; and in exploring and explaining the physicochemical foundation of biological intelligence. {\copyright} Springer Science+Business Media New York 2013. All rights are reserved.",0,"Imaging brain function with EEG: Advanced temporal and spatial analysis of electroencephalographic signals. The scalp and cortex lie like pages of an open book on which the cortex enciphers vast quantities of information and knowledge. They are recorded and analyzed as temporal and spatial patterns in the electroencephalogram and electrocorticogram. This book describes basic tools and concepts needed to measure and decipher the patterns extracted from the EEG and ECoG. This book emphasizes the need for single trial analysis using new methods and paradigms, as well as large, high-density spatial arrays of electrodes for pattern sampling. The deciphered patterns reveal neural mechanisms by which brains process sensory information into precepts and concepts. It describes the brain as a thermodynamic system that uses chemical energy to construct knowledge. The results are intended for use in the search for the neural correlates of intention, attention, perception and learning; in the design of human brain-computer interfaces enabling mental control of machines; and in exploring and explaining the physicochemical foundation of biological intelligence. {\copyright} Springer Science+Business Media New York 2013. All rights are reserved.",imaging brain function eeg advanced temporal spatial analysis electroencephalographic signals scalp cortex lie like pages open book cortex enciphers vast quantities information knowledge recorded analyzed temporal spatial patterns electroencephalogram electrocorticogram book describes basic tools concepts needed measure decipher patterns extracted eeg ecog book emphasizes need single trial analysis using new methods paradigms well large high density spatial arrays electrodes pattern sampling deciphered patterns reveal neural mechanisms brains process sensory information precepts concepts describes brain thermodynamic system uses chemical energy construct knowledge results intended use search neural correlates intention attention perception learning design human brain computer interfaces enabling mental control machines exploring explaining physicochemical foundation biological intelligence copyright springer science business media new york 2013 rights reserved,2,0,1,2,1,3
59,Short-term forecasting of high-speed rail demand: A hybrid approach combining ensemble empirical mode decomposition and gray support vector machine with real-world applications in China,Demand forecasting; Ensemble empirical mode decomposition (EEMD); Grey support vector machine (GSVM); High-speed rail (HSR); Hybrid model,"Short-term forecasting of high-speed rail (HSR) passenger flow provides daily ridership estimates that account for day-to-day demand variations in the near future (e.g., next week, next month). It is one of the most critical tasks in high-speed passenger rail planning, operational decision-making and dynamic operation adjustment. An accurate short-term HSR demand prediction provides a basis for effective rail revenue management. In this paper, a hybrid short-term demand forecasting approach is developed by combining the ensemble empirical mode decomposition (EEMD) and grey support vector machine (GSVM) models. There are three steps in this hybrid forecasting approach: (i) decompose short-term passenger flow data with noises into a number of intrinsic mode functions (IMFs) and a trend term; (ii) predict each IMF using GSVM calibrated by the particle swarm optimization (PSO); (iii) reconstruct the refined IMF components to produce the final predicted daily HSR passenger flow, where the PSO is also applied to achieve the optimal refactoring combination. This innovative hybrid approach is demonstrated with three typical origin-destination pairs along the Wuhan-Guangzhou HSR in China. Mean absolute percentage errors of the EEMD-GSVM predictions using testing sets are 6.7%, 5.1% and 6.5%, respectively, which are much lower than those of two existing forecasting approaches (support vector machine and autoregressive integrated moving average). Application results indicate that the proposed hybrid forecasting approach performs well in terms of prediction accuracy and is especially suitable for short-term HSR passenger flow forecasting. {\copyright} 2014 Elsevier Ltd.",0,"Short-term forecasting of high-speed rail demand: A hybrid approach combining ensemble empirical mode decomposition and gray support vector machine with real-world applications in China. Short-term forecasting of high-speed rail (HSR) passenger flow provides daily ridership estimates that account for day-to-day demand variations in the near future (e.g., next week, next month). It is one of the most critical tasks in high-speed passenger rail planning, operational decision-making and dynamic operation adjustment. An accurate short-term HSR demand prediction provides a basis for effective rail revenue management. In this paper, a hybrid short-term demand forecasting approach is developed by combining the ensemble empirical mode decomposition (EEMD) and grey support vector machine (GSVM) models. There are three steps in this hybrid forecasting approach: (i) decompose short-term passenger flow data with noises into a number of intrinsic mode functions (IMFs) and a trend term; (ii) predict each IMF using GSVM calibrated by the particle swarm optimization (PSO); (iii) reconstruct the refined IMF components to produce the final predicted daily HSR passenger flow, where the PSO is also applied to achieve the optimal refactoring combination. This innovative hybrid approach is demonstrated with three typical origin-destination pairs along the Wuhan-Guangzhou HSR in China. Mean absolute percentage errors of the EEMD-GSVM predictions using testing sets are 6.7%, 5.1% and 6.5%, respectively, which are much lower than those of two existing forecasting approaches (support vector machine and autoregressive integrated moving average). Application results indicate that the proposed hybrid forecasting approach performs well in terms of prediction accuracy and is especially suitable for short-term HSR passenger flow forecasting. {\copyright} 2014 Elsevier Ltd.",short term forecasting high speed rail demand hybrid approach combining ensemble empirical mode decomposition gray support vector machine real world applications china short term forecasting high speed rail hsr passenger flow provides daily ridership estimates account day day demand variations near future e g next week next month one critical tasks high speed passenger rail planning operational decision making dynamic operation adjustment accurate short term hsr demand prediction provides basis effective rail revenue management paper hybrid short term demand forecasting approach developed combining ensemble empirical mode decomposition eemd grey support vector machine gsvm models three steps hybrid forecasting approach decompose short term passenger flow data noises number intrinsic mode functions imfs trend term ii predict imf using gsvm calibrated particle swarm optimization pso iii reconstruct refined imf components produce final predicted daily hsr passenger flow pso also applied achieve optimal refactoring combination innovative hybrid approach demonstrated three typical origin destination pairs along wuhan guangzhou hsr china mean absolute percentage errors eemd gsvm predictions using testing sets 6 7 5 1 6 5 respectively much lower two existing forecasting approaches support vector machine autoregressive integrated moving average application results indicate proposed hybrid forecasting approach performs well terms prediction accuracy especially suitable short term hsr passenger flow forecasting copyright 2014 elsevier ltd,1,1,1,2,0,3
60,Some code smells have a significant but small effect on faults,Defects; Software code smells,"We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness. {\copyright} 2014 ACM.",0,"Some code smells have a significant but small effect on faults. We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness. {\copyright} 2014 ACM.",code smells significant small effect faults investigate relationship faults five fowler et al least studied smells code data clumps switch statements speculative generality message chains middle man developed tool detect five smells three open source systems eclipse argouml apache commons collected fault data change fault repositories system built negative binomial regression models analyse relationships smells faults report mcfadden effect size relationships results suggest switch statements effect faults three systems message chains increased faults two systems message chains occurred larger files reduced faults data clumps reduced faults apache eclipse increased faults argouml middle man reduced faults argouml speculative generality reduced faults eclipse file size alone affects faults systems systems smells significantly affect faults size effect small always 10 percent findings suggest smells indicate fault prone code circumstances effect smells faults small findings also show smells different effects different systems conclude arbitrary refactoring unlikely significantly reduce fault proneness cases may increase fault proneness copyright 2014 acm,1,1,1,0,3,2
61,Self-organization in the olfactory system: One shot odor recognition in insects,Fan-in; Fan-out; Information coding; Olfaction; Pattern recognition; Synaptic convergence,"We show in a model of spiking neurons that synaptic plasticity in the mushroom bodies in combination with the general fan-in, fan-out properties of the early processing layers of the olfactory system might be sufficient to account for its efficient recognition of odors. For a large variety of initial conditions the model system consistently finds a working solution without any fine-tuning, and is, therefore, inherently robust. We demonstrate that gain control through the known feedforward inhibition of lateral horn interneurons increases the capacity of the system but is not essential for its general function. We also predict an upper limit for the number of odor classes Drosophila can discriminate based on the number and connectivity of its olfactory neurons.",0,"Self-organization in the olfactory system: One shot odor recognition in insects. We show in a model of spiking neurons that synaptic plasticity in the mushroom bodies in combination with the general fan-in, fan-out properties of the early processing layers of the olfactory system might be sufficient to account for its efficient recognition of odors. For a large variety of initial conditions the model system consistently finds a working solution without any fine-tuning, and is, therefore, inherently robust. We demonstrate that gain control through the known feedforward inhibition of lateral horn interneurons increases the capacity of the system but is not essential for its general function. We also predict an upper limit for the number of odor classes Drosophila can discriminate based on the number and connectivity of its olfactory neurons.",self organization olfactory system one shot odor recognition insects show model spiking neurons synaptic plasticity mushroom bodies combination general fan fan properties early processing layers olfactory system might sufficient account efficient recognition odors large variety initial conditions model system consistently finds working solution without fine tuning therefore inherently robust demonstrate gain control known feedforward inhibition lateral horn interneurons increases capacity system essential general function also predict upper limit number odor classes drosophila discriminate based number connectivity olfactory neurons,1,0,1,2,0,3
62,An empirical study of just-in-time defect prediction using cross-project models,Empirical study; Software quality,"Prior research suggests that predicting defect-inducing changes, i.e., Just-In-Time (JIT) defect prediction is a more practical alternative to traditional defect prediction techniques, providing immediate feedback while design decisions are still fresh in the minds of developers. Unfortunately, similar to traditional defect prediction models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this flaw in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from older projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT cross-project models. Through a case study on 11 open source projects, we find that in a JIT cross-project context: (1) high performance within-project models rarely perform well; (2) models trained on projects that have similar correlations between predictor and dependent variables often perform well; and (3) ensemble learning techniques that leverage historical data from several other projects (e.g., voting experts) often perform well. Our findings empirically confirm that JIT cross-project models learned using other projects are a viable solution for projects with little historical data. However, JIT cross-project models perform best when the data used to learn them is carefully selected. Copyright 2014 ACM.",0,"An empirical study of just-in-time defect prediction using cross-project models. Prior research suggests that predicting defect-inducing changes, i.e., Just-In-Time (JIT) defect prediction is a more practical alternative to traditional defect prediction techniques, providing immediate feedback while design decisions are still fresh in the minds of developers. Unfortunately, similar to traditional defect prediction models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this flaw in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from older projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT cross-project models. Through a case study on 11 open source projects, we find that in a JIT cross-project context: (1) high performance within-project models rarely perform well; (2) models trained on projects that have similar correlations between predictor and dependent variables often perform well; and (3) ensemble learning techniques that leverage historical data from several other projects (e.g., voting experts) often perform well. Our findings empirically confirm that JIT cross-project models learned using other projects are a viable solution for projects with little historical data. However, JIT cross-project models perform best when the data used to learn them is carefully selected. Copyright 2014 ACM.",empirical study time defect prediction using cross project models prior research suggests predicting defect inducing changes e time jit defect prediction practical alternative traditional defect prediction techniques providing immediate feedback design decisions still fresh minds developers unfortunately similar traditional defect prediction models jit models require large amount training data available projects initial development phases address flaw traditional defect prediction prior work proposed cross project models e models learned older projects sufficient history however cross project models yet explored context jit prediction therefore study empirically evaluate performance jit cross project models case study 11 open source projects find jit cross project context 1 high performance within project models rarely perform well 2 models trained projects similar correlations predictor dependent variables often perform well 3 ensemble learning techniques leverage historical data several projects e g voting experts often perform well findings empirically confirm jit cross project models learned using projects viable solution projects little historical data however jit cross project models perform best data used learn carefully selected copyright 2014 acm,1,1,2,1,0,0
63,Why Red Doesn't Sound Like A Bell: Understanding the feel of consciousness,Color perception; Feeling; Hearing; Seeing; Sensorimotor approach; Sensory experiences; Touch; Visual perception; Visual sensation,"This book proposes a novel view to explain how we as humans-contrary to current robots-can have the impression of consciously feeling things: for example the red of a sunset, the smell of a rose, the sound of a symphony, or a pain. The book starts off by looking at visual perception. Our ability to see turns out to be much more mysterious than one might think. The eye contains many defects which should seriously interfere with vision. Yet we have the impression of seeing the world in glorious panavision and technicolor. Explaining how this can be the case leads to a new idea about what seeing really is. Seeing is not passively receiving information in the brain, but rather a way of interacting with the world. The role of the brain is not to create visual sensation, but to enable the necessary interactions with the world. This new approach to seeing is extended in the second part of the book to encompass the other senses: hearing, touch, taste, and smell. Taking sensory experiences to be modes of interacting with the world explains why these experiences are different in the way they are. It also explains why thoughts or automatic functions in the body, and indeed the vast majority brain functions, are not accompanied by any real feeling. The ""sensorimotor"" approach is not simply a philosophical argument: It leads to scientifically verifiable predictions and new research directions. Among these are the phenomena of change blindness, sensory substitution, ""looked but failed to see"", as well as results on color naming and color perception and the localization of touch on the body. {\copyright} 2011 by Oxford University Press, Inc. All rights reserved.",0,"Why Red Doesn't Sound Like A Bell: Understanding the feel of consciousness. This book proposes a novel view to explain how we as humans-contrary to current robots-can have the impression of consciously feeling things: for example the red of a sunset, the smell of a rose, the sound of a symphony, or a pain. The book starts off by looking at visual perception. Our ability to see turns out to be much more mysterious than one might think. The eye contains many defects which should seriously interfere with vision. Yet we have the impression of seeing the world in glorious panavision and technicolor. Explaining how this can be the case leads to a new idea about what seeing really is. Seeing is not passively receiving information in the brain, but rather a way of interacting with the world. The role of the brain is not to create visual sensation, but to enable the necessary interactions with the world. This new approach to seeing is extended in the second part of the book to encompass the other senses: hearing, touch, taste, and smell. Taking sensory experiences to be modes of interacting with the world explains why these experiences are different in the way they are. It also explains why thoughts or automatic functions in the body, and indeed the vast majority brain functions, are not accompanied by any real feeling. The ""sensorimotor"" approach is not simply a philosophical argument: It leads to scientifically verifiable predictions and new research directions. Among these are the phenomena of change blindness, sensory substitution, ""looked but failed to see"", as well as results on color naming and color perception and the localization of touch on the body. {\copyright} 2011 by Oxford University Press, Inc. All rights reserved.",red sound like bell understanding feel consciousness book proposes novel view explain humans contrary current robots impression consciously feeling things example red sunset smell rose sound symphony pain book starts looking visual perception ability see turns much mysterious one might think eye contains many defects seriously interfere vision yet impression seeing world glorious panavision technicolor explaining case leads new idea seeing really seeing passively receiving information brain rather way interacting world role brain create visual sensation enable necessary interactions world new approach seeing extended second part book encompass senses hearing touch taste smell taking sensory experiences modes interacting world explains experiences different way also explains thoughts automatic functions body indeed vast majority brain functions accompanied real feeling sensorimotor approach simply philosophical argument leads scientifically verifiable predictions new research directions among phenomena change blindness sensory substitution looked failed see well results color naming color perception localization touch body copyright 2011 oxford university press inc rights reserved,2,0,1,2,1,3
64,Artificial immune multi-objective SAR image segmentation with fused complementary features,Artificial immune system; Clustering validity indices; Evolutionary computation; Feature fusion; Gabor filter; Gray level co-occurrence probability; Multi-objective optimization (MO); Single-objective optimization (SO),"Artificial immune systems (AIS) are the computational systems inspired by the principles and processes of the vertebrate immune system. AIS-based algorithms typically mimic the human immune system's characteristics of learning and adaptability to solve some complicated problems. Here, an artificial immune multi-objective optimization framework is formulated and applied to synthetic aperture radar (SAR) image segmentation. The important innovations of the framework are listed as follows: (1) an efficient and robust immune, multi-objective optimization algorithm is proposed, which has the features of adaptive rank clones and diversity maintenance by K-nearest-neighbor list; (2) besides, two conflicting, fuzzy clustering validity indices are incorporated into this framework and optimized simultaneously and (3) moreover, an effective, fused feature set for texture representation and discrimination is constructed and researched, which utilizes both the Gabor filter's ability to precisely extract texture features in low- and mid-frequency components and the gray level co-occurrence probability's (GLCP) ability to measure information in high-frequency. Two experiments with synthetic texture images and SAR images are implemented to evaluate the performance of the proposed framework in comparison with other five clustering algorithms: fuzzy C-means (FCM), single-objective genetic algorithm (SOGA), self-organizing map (SOM), wavelet-domain hidden Markov models (HMTseg), and spectral clustering ensemble (SCE). Experimental results show the proposed framework has obtained the better performance in segmenting SAR images than other five algorithms and behaves insensitive to the speckle noise. {\copyright} 2011 Elsevier Inc. All rights reserved.",0,"Artificial immune multi-objective SAR image segmentation with fused complementary features. Artificial immune systems (AIS) are the computational systems inspired by the principles and processes of the vertebrate immune system. AIS-based algorithms typically mimic the human immune system's characteristics of learning and adaptability to solve some complicated problems. Here, an artificial immune multi-objective optimization framework is formulated and applied to synthetic aperture radar (SAR) image segmentation. The important innovations of the framework are listed as follows: (1) an efficient and robust immune, multi-objective optimization algorithm is proposed, which has the features of adaptive rank clones and diversity maintenance by K-nearest-neighbor list; (2) besides, two conflicting, fuzzy clustering validity indices are incorporated into this framework and optimized simultaneously and (3) moreover, an effective, fused feature set for texture representation and discrimination is constructed and researched, which utilizes both the Gabor filter's ability to precisely extract texture features in low- and mid-frequency components and the gray level co-occurrence probability's (GLCP) ability to measure information in high-frequency. Two experiments with synthetic texture images and SAR images are implemented to evaluate the performance of the proposed framework in comparison with other five clustering algorithms: fuzzy C-means (FCM), single-objective genetic algorithm (SOGA), self-organizing map (SOM), wavelet-domain hidden Markov models (HMTseg), and spectral clustering ensemble (SCE). Experimental results show the proposed framework has obtained the better performance in segmenting SAR images than other five algorithms and behaves insensitive to the speckle noise. {\copyright} 2011 Elsevier Inc. All rights reserved.",artificial immune multi objective sar image segmentation fused complementary features artificial immune systems ais computational systems inspired principles processes vertebrate immune system ais based algorithms typically mimic human immune system characteristics learning adaptability solve complicated problems artificial immune multi objective optimization framework formulated applied synthetic aperture radar sar image segmentation important innovations framework listed follows 1 efficient robust immune multi objective optimization algorithm proposed features adaptive rank clones diversity maintenance k nearest neighbor list 2 besides two conflicting fuzzy clustering validity indices incorporated framework optimized simultaneously 3 moreover effective fused feature set texture representation discrimination constructed researched utilizes gabor filter ability precisely extract texture features low mid frequency components gray level co occurrence probability glcp ability measure information high frequency two experiments synthetic texture images sar images implemented evaluate performance proposed framework comparison five clustering algorithms fuzzy c means fcm single objective genetic algorithm soga self organizing map som wavelet domain hidden markov models hmtseg spectral clustering ensemble sce experimental results show proposed framework obtained better performance segmenting sar images five algorithms behaves insensitive speckle noise copyright 2011 elsevier inc rights reserved,0,1,1,2,0,3
65,Static analysis of android apps: A systematic literature review,,"Context Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps. Objective We aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed. Method We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011--December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed. Results Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available. Conclusion The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers. {\copyright} 2017 Elsevier B.V.",0,"Static analysis of android apps: A systematic literature review. Context Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps. Objective We aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed. Method We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011--December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed. Results Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available. Conclusion The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers. {\copyright} 2017 Elsevier B.V.",static analysis android apps systematic literature review context static analysis exploits techniques parse program source code bytecode often traversing program paths check program properties static analysis approaches proposed different tasks including assessing security android apps detecting app clones automating test cases generation uncovering non functional issues related performance energy literature thus proposed large body works attempts tackle one several challenges program analyzers face dealing android apps objective aim provide clear view state art works statically analyze android apps highlight trends static analysis approaches pinpoint focus put enumerate key aspects future researches still needed method performed systematic literature review slr involves studying 124 research papers published software engineering programming languages security venues last 5 years january 2011 december 2015 review performed mainly five dimensions problems targeted approach fundamental techniques used authors static analysis sensitivities considered android characteristics taken account scale evaluation performed results depth examination led several key findings 1 static analysis largely performed uncover security privacy issues 2 soot framework jimple intermediate representation adopted basic support tool format respectively 3 taint analysis remains applied technique research approaches 4 approaches support several analysis sensitivities approaches consider path sensitivity 5 single work proposed tackle challenges static analysis related android programming 6 small portion state art works made artifacts publicly available conclusion research community still facing number challenges building approaches aware altogether implicit flows dynamic code loading features reflective calls native code multi threading order implement sound highly precise static analyzers copyright 2017 elsevier b v,0,1,1,0,2,2
66,Studying just-in-time defect prediction using cross-project models,Defect prediction; Empirical study; Just-in-time prediction,"Unlike traditional defect prediction models that identify defect-prone modules, Just-In-Time (JIT) defect prediction models identify defect-inducing changes. As such, JIT defect models can provide earlier feedback for developers, while design decisions are still fresh in their minds. Unfortunately, similar to traditional defect models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this limitation in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from other projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT models in a cross-project context. Through an empirical study on 11 open source projects, we find that while JIT models rarely perform well in a cross-project context, their performance tends to improve when using approaches that: (1) select models trained using other projects that are similar to the testing project, (2) combine the data of several other projects to produce a larger pool of training data, and (3) combine the models of several other projects to produce an ensemble model. Our findings empirically confirm that JIT models learned using other projects are a viable solution for projects with limited historical data. However, JIT models tend to perform best in a cross-project context when the data used to learn them are carefully selected. {\copyright} 2015, Springer Science+Business Media New York.",0,"Studying just-in-time defect prediction using cross-project models. Unlike traditional defect prediction models that identify defect-prone modules, Just-In-Time (JIT) defect prediction models identify defect-inducing changes. As such, JIT defect models can provide earlier feedback for developers, while design decisions are still fresh in their minds. Unfortunately, similar to traditional defect models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this limitation in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from other projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT models in a cross-project context. Through an empirical study on 11 open source projects, we find that while JIT models rarely perform well in a cross-project context, their performance tends to improve when using approaches that: (1) select models trained using other projects that are similar to the testing project, (2) combine the data of several other projects to produce a larger pool of training data, and (3) combine the models of several other projects to produce an ensemble model. Our findings empirically confirm that JIT models learned using other projects are a viable solution for projects with limited historical data. However, JIT models tend to perform best in a cross-project context when the data used to learn them are carefully selected. {\copyright} 2015, Springer Science+Business Media New York.",studying time defect prediction using cross project models unlike traditional defect prediction models identify defect prone modules time jit defect prediction models identify defect inducing changes jit defect models provide earlier feedback developers design decisions still fresh minds unfortunately similar traditional defect models jit models require large amount training data available projects initial development phases address limitation traditional defect prediction prior work proposed cross project models e models learned projects sufficient history however cross project models yet explored context jit prediction therefore study empirically evaluate performance jit models cross project context empirical study 11 open source projects find jit models rarely perform well cross project context performance tends improve using approaches 1 select models trained using projects similar testing project 2 combine data several projects produce larger pool training data 3 combine models several projects produce ensemble model findings empirically confirm jit models learned using projects viable solution projects limited historical data however jit models tend perform best cross project context data used learn carefully selected copyright 2015 springer science business media new york,1,1,2,1,0,0
67,How do code refactorings affect energy usage?,code refactoring; empirical study; energy usage,"Context: Code refactoring's benefits to understandability, maintainability and extensibility are well known enough that automated support for refactoring is now common in IDEs. However, the decision to apply such transformations is currently performed without regard to the impacts of the refactorings on energy consumption. This is primarily due to a lack of information and tools to provide such relevant information to developers. Unfortunately, concerns about energy efficiency are rapidly becoming a high priority concern in many environments, including embedded systems, laptops, mobile devices, and data centers. Goal: We aim to address the lack of information about the energy efficiency impacts of code refactorings. Method: We conducted an empirical study to investigate the energy impacts of 197 applications of 6 commonly-used refactorings. Results: We found that refactorings can not only impact energy usage but can also increase and decrease the amount of energy used by an application. In addition, we also show that metrics commonly believed to correlate with energy usage are unlikely to be able to fully predict the impact of applying a refactoring. Conclusion: The results from this and similar studies could be used to augment IDEs to help software developers build more energy efficient software. {\copyright} 2014 ACM.",0,"How do code refactorings affect energy usage?. Context: Code refactoring's benefits to understandability, maintainability and extensibility are well known enough that automated support for refactoring is now common in IDEs. However, the decision to apply such transformations is currently performed without regard to the impacts of the refactorings on energy consumption. This is primarily due to a lack of information and tools to provide such relevant information to developers. Unfortunately, concerns about energy efficiency are rapidly becoming a high priority concern in many environments, including embedded systems, laptops, mobile devices, and data centers. Goal: We aim to address the lack of information about the energy efficiency impacts of code refactorings. Method: We conducted an empirical study to investigate the energy impacts of 197 applications of 6 commonly-used refactorings. Results: We found that refactorings can not only impact energy usage but can also increase and decrease the amount of energy used by an application. In addition, we also show that metrics commonly believed to correlate with energy usage are unlikely to be able to fully predict the impact of applying a refactoring. Conclusion: The results from this and similar studies could be used to augment IDEs to help software developers build more energy efficient software. {\copyright} 2014 ACM.",code refactorings affect energy usage context code refactoring benefits understandability maintainability extensibility well known enough automated support refactoring common ides however decision apply transformations currently performed without regard impacts refactorings energy consumption primarily due lack information tools provide relevant information developers unfortunately concerns energy efficiency rapidly becoming high priority concern many environments including embedded systems laptops mobile devices data centers goal aim address lack information energy efficiency impacts code refactorings method conducted empirical study investigate energy impacts 197 applications 6 commonly used refactorings results found refactorings impact energy usage also increase decrease amount energy used application addition also show metrics commonly believed correlate energy usage unlikely able fully predict impact applying refactoring conclusion results similar studies could used augment ides help software developers build energy efficient software copyright 2014 acm,1,2,0,1,0,2
68,A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection,code-smells; distributed evolutionary algorithms; Search-based software engineering; software quality,"We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells. {\copyright} 2014 IEEE.",0,"A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection. We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells. {\copyright} 2014 IEEE.",cooperative parallel search based software engineering approach code smells detection propose paper consider code smells detection distributed optimization problem idea different methods combined parallel optimization process find consensus regarding detection code smells end used parallel evolutionary algorithms p ea many evolutionary algorithms different adaptations fitness functions solution representations change operators executed parallel cooperative manner solve common goal detection code smells empirical evaluation compare implementation cooperative p ea approach random search two single population based approaches two code smells detection techniques based meta heuristics search statistical analysis obtained results provides evidence support claim cooperative p ea efficient effective state art detection approaches based benchmark nine large open source systems 85 percent precision recall scores obtained variety eight different types code smells copyright 2014 ieee,0,2,0,0,3,1
69,Real-Time Systems Design and Analysis: Tools for the Practitioner,,"The leading text in the field explains step by step how to write software that responds in real time From power plants to medicine to avionics, the world increasingly depends on computer systems that can compute and respond to various excitations in real time. The Fourth Edition of Real-Time Systems Design and Analysis gives software designers the knowledge and the tools needed to create real-time software using a holistic, systems-based approach. The text covers computer architecture and organization, operating systems, software engineering, programming languages, and compiler theory, all from the perspective of real-time systems design. The Fourth Edition of this renowned text brings it thoroughly up to date with the latest technological advances and applications. This fully updated edition includes coverage of the following concepts: Multidisciplinary design challenges Time-triggered architectures Architectural advancements Automatic code generation Peripheral interfacing Life-cycle processes The final chapter of the text offers an expert perspective on the future of real-time systems and their applications. The text is self-contained, enabling instructors and readers to focus on the material that is most important to their needs and interests. Suggestions for additional readings guide readers to more in-depth discussions on each individual topic. In addition, each chapter features exercises ranging from simple to challenging to help readers progressively build and fine-tune their ability to design their own real-time software programs. Now fully up to date with the latest technological advances and applications in the field, Real-Time Systems Design and Analysis remains the top choice for students and software engineers who want to design better and faster real-time systems at minimum cost. {\copyright} 2012 the Institute of Electrical and Electronics Engineers, Inc.",0,"Real-Time Systems Design and Analysis: Tools for the Practitioner. The leading text in the field explains step by step how to write software that responds in real time From power plants to medicine to avionics, the world increasingly depends on computer systems that can compute and respond to various excitations in real time. The Fourth Edition of Real-Time Systems Design and Analysis gives software designers the knowledge and the tools needed to create real-time software using a holistic, systems-based approach. The text covers computer architecture and organization, operating systems, software engineering, programming languages, and compiler theory, all from the perspective of real-time systems design. The Fourth Edition of this renowned text brings it thoroughly up to date with the latest technological advances and applications. This fully updated edition includes coverage of the following concepts: Multidisciplinary design challenges Time-triggered architectures Architectural advancements Automatic code generation Peripheral interfacing Life-cycle processes The final chapter of the text offers an expert perspective on the future of real-time systems and their applications. The text is self-contained, enabling instructors and readers to focus on the material that is most important to their needs and interests. Suggestions for additional readings guide readers to more in-depth discussions on each individual topic. In addition, each chapter features exercises ranging from simple to challenging to help readers progressively build and fine-tune their ability to design their own real-time software programs. Now fully up to date with the latest technological advances and applications in the field, Real-Time Systems Design and Analysis remains the top choice for students and software engineers who want to design better and faster real-time systems at minimum cost. {\copyright} 2012 the Institute of Electrical and Electronics Engineers, Inc.",real time systems design analysis tools practitioner leading text field explains step step write software responds real time power plants medicine avionics world increasingly depends computer systems compute respond various excitations real time fourth edition real time systems design analysis gives software designers knowledge tools needed create real time software using holistic systems based approach text covers computer architecture organization operating systems software engineering programming languages compiler theory perspective real time systems design fourth edition renowned text brings thoroughly date latest technological advances applications fully updated edition includes coverage following concepts multidisciplinary design challenges time triggered architectures architectural advancements automatic code generation peripheral interfacing life cycle processes final chapter text offers expert perspective future real time systems applications text self contained enabling instructors readers focus material important needs interests suggestions additional readings guide readers depth discussions individual topic addition chapter features exercises ranging simple challenging help readers progressively build fine tune ability design real time software programs fully date latest technological advances applications field real time systems design analysis remains top choice students software engineers want design better faster real time systems minimum cost copyright 2012 institute electrical electronics engineers inc,2,0,1,2,2,3
70,Network versus code metrics to predict defects: A replication study,Code metrics; Defect prediction; Network metrics; Open-source; Replication study,"Several defect prediction models have been proposed to identify which entities in a software system are likely to have defects before its release. This paper presents a replication of one such study conducted by Zimmermann and Nagappan [1] on Windows Server 2003 where the authors leveraged dependency relationships between software entities captured using social network metrics to predict whether they are likely to have defects. They found that network metrics perform significantly better than source code metrics at predicting defects. In order to corroborate the generality of their findings, we replicate their study on three open source Java projects, viz., JRuby, ArgoUML, and Eclipse. Our results are in agreement with the original study by Zimmermann and Nagappan when using a similar experimental setup as them (random sampling). However, when we evaluated the metrics using setups more suited for industrial use - forward-release and cross-project prediction - we found network metrics to offer no vantage over code metrics. Moreover, code metrics may be preferable to network metrics considering the data is easier to collect and we used only 8 code metrics compared to approximately 58 network metrics. {\copyright} 2011 IEEE.",0,"Network versus code metrics to predict defects: A replication study. Several defect prediction models have been proposed to identify which entities in a software system are likely to have defects before its release. This paper presents a replication of one such study conducted by Zimmermann and Nagappan [1] on Windows Server 2003 where the authors leveraged dependency relationships between software entities captured using social network metrics to predict whether they are likely to have defects. They found that network metrics perform significantly better than source code metrics at predicting defects. In order to corroborate the generality of their findings, we replicate their study on three open source Java projects, viz., JRuby, ArgoUML, and Eclipse. Our results are in agreement with the original study by Zimmermann and Nagappan when using a similar experimental setup as them (random sampling). However, when we evaluated the metrics using setups more suited for industrial use - forward-release and cross-project prediction - we found network metrics to offer no vantage over code metrics. Moreover, code metrics may be preferable to network metrics considering the data is easier to collect and we used only 8 code metrics compared to approximately 58 network metrics. {\copyright} 2011 IEEE.",network versus code metrics predict defects replication study several defect prediction models proposed identify entities software system likely defects release paper presents replication one study conducted zimmermann nagappan 1 windows server 2003 authors leveraged dependency relationships software entities captured using social network metrics predict whether likely defects found network metrics perform significantly better source code metrics predicting defects order corroborate generality findings replicate study three open source java projects viz jruby argouml eclipse results agreement original study zimmermann nagappan using similar experimental setup random sampling however evaluated metrics using setups suited industrial use forward release cross project prediction found network metrics offer vantage code metrics moreover code metrics may preferable network metrics considering data easier collect used 8 code metrics compared approximately 58 network metrics copyright 2011 ieee,1,2,0,1,3,2
71,A survey on the use of topic models when mining software repositories,LDA; LSI; Survey; Topic modeling,"Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task. {\copyright} 2015, Springer Science+Business Media New York.",0,"A survey on the use of topic models when mining software repositories. Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task. {\copyright} 2015, Springer Science+Business Media New York.",survey use topic models mining software repositories researchers software engineering attempted improve software development mining analyzing software repositories since majority software engineering data unstructured researchers applied information retrieval ir techniques help software development recent advances ir especially statistical topic models helped make sense unstructured data software repositories even however even though hundreds studies applying topic models software repositories study shows models used software engineering research community software engineering tasks supported topic models moreover since performance topic models directly related model parameters usage knowing researchers use topic models may also help future studies make optimal use models thus surveyed 167 articles software engineering literature make use topic models find studies centre around limited number software engineering tasks ii studies use basic topic models iii researchers usually treat topic models black boxes without fully exploring underlying assumptions parameter values paper provides starting point new researchers interested using topic models may help new researchers practitioners determine best apply topic models particular software engineering task copyright 2015 springer science business media new york,1,2,0,1,0,2
72,Optical-fiber arrays for vapor sensing,Artificial nose; Fluorescence; Microarray; Optical fiber; Vapor sensing,This paper reviews the use of optical fibers as a platform to fabricate vapor sensitive arrays that have been developed in the authors' laboratory in recent years. Two types of fluorescence-based optical-fiber array systems are described: polymer-coated single core optical fibers and imaging fiber-optic bundles incorporating functionalized microspheres. Each system responds to an analyte vapor in a cross-reactive manner generating a multi-dimensional signal that can be used to train a pattern recognition program to identify subsequent exposures of the array to learned vapors. Several applications are described including explosives and nerve agent detection as well as the detection of several volatile organic compounds and ignitable liquids. {\copyright} 2009 Elsevier B.V. All rights reserved.,0,Optical-fiber arrays for vapor sensing. This paper reviews the use of optical fibers as a platform to fabricate vapor sensitive arrays that have been developed in the authors' laboratory in recent years. Two types of fluorescence-based optical-fiber array systems are described: polymer-coated single core optical fibers and imaging fiber-optic bundles incorporating functionalized microspheres. Each system responds to an analyte vapor in a cross-reactive manner generating a multi-dimensional signal that can be used to train a pattern recognition program to identify subsequent exposures of the array to learned vapors. Several applications are described including explosives and nerve agent detection as well as the detection of several volatile organic compounds and ignitable liquids. {\copyright} 2009 Elsevier B.V. All rights reserved.,optical fiber arrays vapor sensing paper reviews use optical fibers platform fabricate vapor sensitive arrays developed authors laboratory recent years two types fluorescence based optical fiber array systems described polymer coated single core optical fibers imaging fiber optic bundles incorporating functionalized microspheres system responds analyte vapor cross reactive manner generating multi dimensional signal used train pattern recognition program identify subsequent exposures array learned vapors several applications described including explosives nerve agent detection well detection several volatile organic compounds ignitable liquids copyright 2009 elsevier b v rights reserved,0,2,1,2,2,3
73,"Is software ""green""? Application development environments and energy efficiency in open source applications",Green IT; Software development application environment; Software energy efficiency,"Context: The energy efficiency of IT systems, also referred to as Green IT, is attracting more and more attention. While several researchers have focused on the energy efficiency of hardware and embedded systems, the role of application software in IT energy consumption still needs investigation. Objective: This paper aims to define a methodology for measuring software energy efficiency and to understand the consequences of abstraction layers and application development environments for the energy efficiency of software applications. Method: We first develop a measure of energy efficiency that is appropriate for software applications. We then examine how the use of application development environments relates to this measure of energy efficiency for a sample of 63 open source software applications. Results: Our findings indicate that a greater use of application development environments - specifically, frameworks and external libraries - is more detrimental in terms of energy efficiency for larger applications than for smaller applications. We also find that different functional application types have distinctly different levels of energy efficiency, with text and image editing and gaming applications being the most energy inefficient due to their intense use of the processor. Conclusion: We conclude that different designs can have a significant impact on the energy efficiency of software applications. We have related the use of software application development environments to software energy efficiency suggesting that there may be a trade-off between development efficiency and energy efficiency. We propose new research to further investigate this topic. {\copyright} 2011 Elsevier B.V. All rights reserved.",0,"Is software ""green""? Application development environments and energy efficiency in open source applications. Context: The energy efficiency of IT systems, also referred to as Green IT, is attracting more and more attention. While several researchers have focused on the energy efficiency of hardware and embedded systems, the role of application software in IT energy consumption still needs investigation. Objective: This paper aims to define a methodology for measuring software energy efficiency and to understand the consequences of abstraction layers and application development environments for the energy efficiency of software applications. Method: We first develop a measure of energy efficiency that is appropriate for software applications. We then examine how the use of application development environments relates to this measure of energy efficiency for a sample of 63 open source software applications. Results: Our findings indicate that a greater use of application development environments - specifically, frameworks and external libraries - is more detrimental in terms of energy efficiency for larger applications than for smaller applications. We also find that different functional application types have distinctly different levels of energy efficiency, with text and image editing and gaming applications being the most energy inefficient due to their intense use of the processor. Conclusion: We conclude that different designs can have a significant impact on the energy efficiency of software applications. We have related the use of software application development environments to software energy efficiency suggesting that there may be a trade-off between development efficiency and energy efficiency. We propose new research to further investigate this topic. {\copyright} 2011 Elsevier B.V. All rights reserved.",software green application development environments energy efficiency open source applications context energy efficiency systems also referred green attracting attention several researchers focused energy efficiency hardware embedded systems role application software energy consumption still needs investigation objective paper aims define methodology measuring software energy efficiency understand consequences abstraction layers application development environments energy efficiency software applications method first develop measure energy efficiency appropriate software applications examine use application development environments relates measure energy efficiency sample 63 open source software applications results findings indicate greater use application development environments specifically frameworks external libraries detrimental terms energy efficiency larger applications smaller applications also find different functional application types distinctly different levels energy efficiency text image editing gaming applications energy inefficient due intense use processor conclusion conclude different designs significant impact energy efficiency software applications related use software application development environments software energy efficiency suggesting may trade development efficiency energy efficiency propose new research investigate topic copyright 2011 elsevier b v rights reserved,2,0,2,2,2,2
74,A procedure to detect problems of processes in software development projects using Bayesian networks,Bayesian networks; Software development project; Software process management; Software process simulation modeling,"There are several software process models and methodologies such as waterfall, spiral and agile. Even so, the rate of successful software development projects is low. Since software is the major output of software processes, increasing software process management quality should increase the project's chances of success. Organizations have invested to adapt software processes to their environments and the characteristics of projects to improve the productivity and quality of the products. In this paper, we present a procedure to detect problems of processes in software development projects using Bayesian networks. The procedure was successfully applied to Scrum-based software development projects. The research results should encourage the usage of Bayesian networks to manage software processes and increase the rate of successful software development projects. {\copyright} 2014 Elsevier Ltd. All rights reserved.",0,"A procedure to detect problems of processes in software development projects using Bayesian networks. There are several software process models and methodologies such as waterfall, spiral and agile. Even so, the rate of successful software development projects is low. Since software is the major output of software processes, increasing software process management quality should increase the project's chances of success. Organizations have invested to adapt software processes to their environments and the characteristics of projects to improve the productivity and quality of the products. In this paper, we present a procedure to detect problems of processes in software development projects using Bayesian networks. The procedure was successfully applied to Scrum-based software development projects. The research results should encourage the usage of Bayesian networks to manage software processes and increase the rate of successful software development projects. {\copyright} 2014 Elsevier Ltd. All rights reserved.",procedure detect problems processes software development projects using bayesian networks several software process models methodologies waterfall spiral agile even rate successful software development projects low since software major output software processes increasing software process management quality increase project chances success organizations invested adapt software processes environments characteristics projects improve productivity quality products paper present procedure detect problems processes software development projects using bayesian networks procedure successfully applied scrum based software development projects research results encourage usage bayesian networks manage software processes increase rate successful software development projects copyright 2014 elsevier ltd rights reserved,1,2,2,1,0,2
75,Mining software evolution to predict refactoring,,"Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, propositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects. {\copyright} 2007 IEEE.",0,"Mining software evolution to predict refactoring. Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, propositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects. {\copyright} 2007 IEEE.",mining software evolution predict refactoring predict locations future refactoring based development history empirical study open source projects found attributes software evolution data used predict need refactoring following two months development information systems utilized software projects provide broad range data decision support versioning systems log activity development use extract data mining features growth measures relationships classes number authors working particular piece code etc use information input classification algorithms create prediction models future refactoring activities different state art classifiers investigated decision trees logistic model trees propositional rule learners nearest neighbor algorithms high precision high recall assess refactoring proneness object oriented systems although investigate different domains discovered critical factors within development life cycle leading refactoring common among studied projects copyright 2007 ieee,1,2,0,1,0,2
76,A review-based comparative study of bad smell detection tools,Bad smells; Comparative study; Detection tools; Systematic literature review,"Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools. {\copyright} 2016 ACM.",0,"A review-based comparative study of bad smell detection tools. Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools. {\copyright} 2016 ACM.",review based comparative study bad smell detection tools bad smells symptoms something may wrong system design code many bad smells defined literature detecting far trivial therefore several tools proposed automate bad smell detection aiming improve software maintainability however lack detailed study summarizing comparing wide range available tools paper first present findings systematic literature review bad smell detection tools results review found 84 tools 29 available online download altogether tools aim detect 61 bad smells relying least six different detection techniques also target different programming languages java c c c following systematic review present comparative study four detection tools respect two bad smells large class long method study relies two software systems three metrics comparison agreement recall precision findings support tools provide redundant detection results bad smell based quantitative qualitative data also discuss relevant usability issues propose guidelines developers detection tools copyright 2016 acm,0,2,2,0,3,1
77,Empirical evidence on the link between object-oriented measures and external quality attributes: A systematic literature review,Object-oriented system; Software metrics; Software quality; Source code analysis; Source code measures; Static analysis; Systematic literature review,"There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The least investigated attributes were: effectiveness and functionality. Measures from the C&K measurement suite were the most popular across studies. Votecounting results suggest that complexity, cohesion, size and coupling measures have a better link with reliability and maintainability than inheritance measures. However, inheritance measures should not be overlooked during quality assessment initiatives; their link with reliability and maintainability could be context dependent. There were too few studies traced to effectiveness and functionality attributes; thus a meaningful vote-counting analysis could not be conducted for these attributes. Thus, there is a need for diversification of quality attributes investigated in empirical studies. This would help with identifying useful measures during quality assessment initiatives, and not just for reliability and maintainability aspects. {\copyright} Springer Science+Business Media New York 2014.",0,"Empirical evidence on the link between object-oriented measures and external quality attributes: A systematic literature review. There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The least investigated attributes were: effectiveness and functionality. Measures from the C&K measurement suite were the most popular across studies. Votecounting results suggest that complexity, cohesion, size and coupling measures have a better link with reliability and maintainability than inheritance measures. However, inheritance measures should not be overlooked during quality assessment initiatives; their link with reliability and maintainability could be context dependent. There were too few studies traced to effectiveness and functionality attributes; thus a meaningful vote-counting analysis could not be conducted for these attributes. Thus, there is a need for diversification of quality attributes investigated in empirical studies. This would help with identifying useful measures during quality assessment initiatives, and not just for reliability and maintainability aspects. {\copyright} Springer Science+Business Media New York 2014.",empirical evidence link object oriented measures external quality attributes systematic literature review plethora studies investigating object oriented measures link external quality attributes usefulness measures may differ across empirical studies study aims aggregate identify useful object oriented measures specifically obtainable source code object oriented systems gone empirical evaluation conducting systematic literature review 99 primary studies identified traced four external quality attributes reliability maintainability effectiveness functionality vote counting approach used investigate link object oriented measures attributes also assess consistency relation reported across empirical studies studies investigate links object oriented measures proxies reliability attributes followed proxies maintainability least investigated attributes effectiveness functionality measures c k measurement suite popular across studies votecounting results suggest complexity cohesion size coupling measures better link reliability maintainability inheritance measures however inheritance measures overlooked quality assessment initiatives link reliability maintainability could context dependent studies traced effectiveness functionality attributes thus meaningful vote counting analysis could conducted attributes thus need diversification quality attributes investigated empirical studies would help identifying useful measures quality assessment initiatives reliability maintainability aspects copyright springer science business media new york 2014,1,1,1,0,0,2
78,Statistical learning approach for mining API usage mappings for code migration,API mappings; API usages; Code migration; Statistical learning,"The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C#. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy. {\copyright} 2014 ACM.",0,"Statistical learning approach for mining API usage mappings for code migration. The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C#. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy. {\copyright} 2014 ACM.",statistical learning approach mining api usage mappings code migration software product nowadays could appear multiple platforms devices address business needs software companies develop software product programming language migrate another one support process semi automatic migration tools proposed however require users manually define mappings respective apis libraries used two languages reduce manual effort introduce staminer novel data driven approach statistically learns mappings apis corpus corresponding client code apis two languages java c instead using heuristics textual structural similarity apis two languages map api methods classes existing mining approaches staminer based statistical model learns mappings corpus provides mappings apis possible arities empirical evaluation several projects shows staminer detect api usage mappings higher accuracy state art approach resulting api mappings mined staminer java2csharp existing migration tool could achieve higher level accuracy copyright 2014 acm,0,2,0,1,0,2
79,A review of code smell mining techniques,code quality; code smells; design flaws; detection techniques; literature review,"Over the past 15 years, researchers presented numerous techniques and tools for mining code smells. It is imperative to classify, compare, and evaluate existing techniques and tools used for the detection of code smells because of their varying features and outcomes. This paper presents an up-to-date review on the state-of-the-art techniques and tools used for mining code smells from the source code of different software applications. We classify selected code smell detection techniques and tools based on their detection methods and analyze the results of the selected techniques. We present our observations and recommendations after our critical analysis of existing code smell techniques and tools. Our recommendations may be used by existing and new tool developers working in the field of code smell detection. The scope of this review is limited to research publications in the area of code smells that focus on detection of code smells as compared with previous reviews that cover all aspects of code smells. Copyright {\copyright} 2015 John Wiley & Sons, Ltd.",0,"A review of code smell mining techniques. Over the past 15 years, researchers presented numerous techniques and tools for mining code smells. It is imperative to classify, compare, and evaluate existing techniques and tools used for the detection of code smells because of their varying features and outcomes. This paper presents an up-to-date review on the state-of-the-art techniques and tools used for mining code smells from the source code of different software applications. We classify selected code smell detection techniques and tools based on their detection methods and analyze the results of the selected techniques. We present our observations and recommendations after our critical analysis of existing code smell techniques and tools. Our recommendations may be used by existing and new tool developers working in the field of code smell detection. The scope of this review is limited to research publications in the area of code smells that focus on detection of code smells as compared with previous reviews that cover all aspects of code smells. Copyright {\copyright} 2015 John Wiley & Sons, Ltd.",review code smell mining techniques past 15 years researchers presented numerous techniques tools mining code smells imperative classify compare evaluate existing techniques tools used detection code smells varying features outcomes paper presents date review state art techniques tools used mining code smells source code different software applications classify selected code smell detection techniques tools based detection methods analyze results selected techniques present observations recommendations critical analysis existing code smell techniques tools recommendations may used existing new tool developers working field code smell detection scope review limited research publications area code smells focus detection code smells compared previous reviews cover aspects code smells copyright copyright 2015 john wiley sons ltd,0,2,2,0,3,1
80,Software mutational robustness,Genetic programming; Mutation testing; Mutational robustness; N-version programming; Neutral landscapes; Proactive diversity,"Neutral landscapes and mutational robustness are believed to be important enablers of evolvability in biology. We apply these concepts to software, defining mutational robustness to be the fraction of random mutations to program code that leave a program's behavior unchanged. Test cases are used to measure program behavior and mutation operators are taken from earlier work on genetic programming. Although software is often viewed as brittle, with small changes leading to catastrophic changes in behavior, our results show surprising robustness in the face of random software mutations. The paper describes empirical studies of the mutational robustness of 22 programs, including 14 production software projects, the Siemens benchmarks, and four specially constructed programs. We find that over 30 % of random mutations are neutral with respect to their test suite. The results hold across all classes of programs, for mutations at both the source code and assembly instruction levels, across various programming languages, and bear only a limited relation to test suite coverage. We conclude that mutational robustness is an inherent property of software, and that neutral variants (i.e., those that pass the test suite) often fulfill the program's original purpose or specification. Based on these results, we conjecture that neutral mutations can be leveraged as a mechanism for generating software diversity. We demonstrate this idea by generating a population of neutral program variants and showing that the variants automatically repair latent bugs. Neutral landscapes also provide a partial explanation for recent results that use evolutionary computation to automatically repair software bugs. {\copyright} 2013 Springer Science+Business Media New York.",0,"Software mutational robustness. Neutral landscapes and mutational robustness are believed to be important enablers of evolvability in biology. We apply these concepts to software, defining mutational robustness to be the fraction of random mutations to program code that leave a program's behavior unchanged. Test cases are used to measure program behavior and mutation operators are taken from earlier work on genetic programming. Although software is often viewed as brittle, with small changes leading to catastrophic changes in behavior, our results show surprising robustness in the face of random software mutations. The paper describes empirical studies of the mutational robustness of 22 programs, including 14 production software projects, the Siemens benchmarks, and four specially constructed programs. We find that over 30 % of random mutations are neutral with respect to their test suite. The results hold across all classes of programs, for mutations at both the source code and assembly instruction levels, across various programming languages, and bear only a limited relation to test suite coverage. We conclude that mutational robustness is an inherent property of software, and that neutral variants (i.e., those that pass the test suite) often fulfill the program's original purpose or specification. Based on these results, we conjecture that neutral mutations can be leveraged as a mechanism for generating software diversity. We demonstrate this idea by generating a population of neutral program variants and showing that the variants automatically repair latent bugs. Neutral landscapes also provide a partial explanation for recent results that use evolutionary computation to automatically repair software bugs. {\copyright} 2013 Springer Science+Business Media New York.",software mutational robustness neutral landscapes mutational robustness believed important enablers evolvability biology apply concepts software defining mutational robustness fraction random mutations program code leave program behavior unchanged test cases used measure program behavior mutation operators taken earlier work genetic programming although software often viewed brittle small changes leading catastrophic changes behavior results show surprising robustness face random software mutations paper describes empirical studies mutational robustness 22 programs including 14 production software projects siemens benchmarks four specially constructed programs find 30 random mutations neutral respect test suite results hold across classes programs mutations source code assembly instruction levels across various programming languages bear limited relation test suite coverage conclude mutational robustness inherent property software neutral variants e pass test suite often fulfill program original purpose specification based results conjecture neutral mutations leveraged mechanism generating software diversity demonstrate idea generating population neutral program variants showing variants automatically repair latent bugs neutral landscapes also provide partial explanation recent results use evolutionary computation automatically repair software bugs copyright 2013 springer science business media new york,0,1,1,1,3,2
81,Support vector machines for anti-pattern detection,Anti-pattern; Empirical software engineering; Program comprehension; Program maintenance,"Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and-or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a is important to ease the maintenance of software. Detecting anti-patterns could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique- support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX. Copyright 2012 ACM.",0,"Support vector machines for anti-pattern detection. Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and-or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a is important to ease the maintenance of software. Detecting anti-patterns could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique- support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX. Copyright 2012 ACM.",support vector machines anti pattern detection developers may introduce anti patterns software systems time pressure lack understanding communication skills anti patterns impede development maintenance activities making source code difficult understand detecting anti patterns important ease maintenance software detecting anti patterns could reduce costs effort resources researchers proposed approaches detect occurrences anti patterns approaches currently limitations require extensive knowledge anti patterns limited precision recall applied subsets systems overcome limitations introduce svmdetect novel approach detect anti patterns based machine learning technique support vector machines indeed empirical study involving three subject systems four anti patterns showed accuracy svmdetect greater detex detecting anti patterns occurrences set classes concerning whole system svmdetect able find anti patterns occurrences detex copyright 2012 acm,1,2,0,1,3,1
82,A bayesian network based approach for change coupling prediction,,"Source code coupling and change history are two important data sources for change coupling analysis. The popularity of public open source projects in recent years makes both sources available. Based on our previous research, in this paper, we inspect different dimensions of software changes including change significance or source code dependency levels, extract a set of features from the two sources and propose a bayesian network-based approach for change coupling prediction. By combining the features from the co-changed entities and their dependency relation, the approach can model the underlying uncertainty. The empirical case study on two medium-sized open source projects demonstrates the feasibility and effectiveness of our approach compared to previous work. {\copyright} 2008 IEEE.",0,"A bayesian network based approach for change coupling prediction. Source code coupling and change history are two important data sources for change coupling analysis. The popularity of public open source projects in recent years makes both sources available. Based on our previous research, in this paper, we inspect different dimensions of software changes including change significance or source code dependency levels, extract a set of features from the two sources and propose a bayesian network-based approach for change coupling prediction. By combining the features from the co-changed entities and their dependency relation, the approach can model the underlying uncertainty. The empirical case study on two medium-sized open source projects demonstrates the feasibility and effectiveness of our approach compared to previous work. {\copyright} 2008 IEEE.",bayesian network based approach change coupling prediction source code coupling change history two important data sources change coupling analysis popularity public open source projects recent years makes sources available based previous research paper inspect different dimensions software changes including change significance source code dependency levels extract set features two sources propose bayesian network based approach change coupling prediction combining features co changed entities dependency relation approach model underlying uncertainty empirical case study two medium sized open source projects demonstrates feasibility effectiveness approach compared previous work copyright 2008 ieee,1,2,2,1,0,2
83,Taste quality decoding parallels taste sensations,,"In most species, the sense of taste is key in the distinction of potentially nutritious and harmful food constituents and thereby in the acceptance (or rejection) of food. Taste quality is encoded by specialized receptors on the tongue, which detect chemicals corresponding to each of the basic tastes (sweet, salty, sour, bitter, and savory [1]), before taste quality information is transmitted via segregated neuronal fibers [2], distributed coding across neuronal fibers [3], or dynamic firing patterns [4] to the gustatory cortex in the insula. In rodents, both hardwired coding by labeled lines [2] and flexible, learning-dependent representations [5] and broadly tuned neurons [6] seem to coexist. It is currently unknown how, when, and where taste quality representations are established in the cortex and whether these representations are used for perceptual decisions. Here, we show that neuronal response patterns allow to decode which of four tastants (salty, sweet, sour, and bitter) participants tasted in a given trial by using time-resolved multivariate pattern analyses of large-scale electrophysiological brain responses. The onset of this prediction coincided with the earliest taste-evoked responses originating from the insula and opercular cortices, indicating that quality is among the first attributes of a taste represented in the central gustatory system. These response patterns correlated with perceptual decisions of taste quality: tastes that participants discriminated less accurately also evoked less discriminated brain response patterns. The results therefore provide the first evidence for a link between taste-related decision-making and the predictive value of these brain response patterns. {\copyright}2015 Elsevier Ltd All rights reserved.",0,"Taste quality decoding parallels taste sensations. In most species, the sense of taste is key in the distinction of potentially nutritious and harmful food constituents and thereby in the acceptance (or rejection) of food. Taste quality is encoded by specialized receptors on the tongue, which detect chemicals corresponding to each of the basic tastes (sweet, salty, sour, bitter, and savory [1]), before taste quality information is transmitted via segregated neuronal fibers [2], distributed coding across neuronal fibers [3], or dynamic firing patterns [4] to the gustatory cortex in the insula. In rodents, both hardwired coding by labeled lines [2] and flexible, learning-dependent representations [5] and broadly tuned neurons [6] seem to coexist. It is currently unknown how, when, and where taste quality representations are established in the cortex and whether these representations are used for perceptual decisions. Here, we show that neuronal response patterns allow to decode which of four tastants (salty, sweet, sour, and bitter) participants tasted in a given trial by using time-resolved multivariate pattern analyses of large-scale electrophysiological brain responses. The onset of this prediction coincided with the earliest taste-evoked responses originating from the insula and opercular cortices, indicating that quality is among the first attributes of a taste represented in the central gustatory system. These response patterns correlated with perceptual decisions of taste quality: tastes that participants discriminated less accurately also evoked less discriminated brain response patterns. The results therefore provide the first evidence for a link between taste-related decision-making and the predictive value of these brain response patterns. {\copyright}2015 Elsevier Ltd All rights reserved.",taste quality decoding parallels taste sensations species sense taste key distinction potentially nutritious harmful food constituents thereby acceptance rejection food taste quality encoded specialized receptors tongue detect chemicals corresponding basic tastes sweet salty sour bitter savory 1 taste quality information transmitted via segregated neuronal fibers 2 distributed coding across neuronal fibers 3 dynamic firing patterns 4 gustatory cortex insula rodents hardwired coding labeled lines 2 flexible learning dependent representations 5 broadly tuned neurons 6 seem coexist currently unknown taste quality representations established cortex whether representations used perceptual decisions show neuronal response patterns allow decode four tastants salty sweet sour bitter participants tasted given trial using time resolved multivariate pattern analyses large scale electrophysiological brain responses onset prediction coincided earliest taste evoked responses originating insula opercular cortices indicating quality among first attributes taste represented central gustatory system response patterns correlated perceptual decisions taste quality tastes participants discriminated less accurately also evoked less discriminated brain response patterns results therefore provide first evidence link taste related decision making predictive value brain response patterns copyright 2015 elsevier ltd rights reserved,1,1,1,0,1,3
84,How changes affect software entropy: An empirical study,Mining software repositories; Software complexity; Software entropy,"Context Software systems continuously change for various reasons, such as adding new features, fixing bugs, or refactoring. Changes may either increase the source code complexity and disorganization, or help to reducing it. Aim This paper empirically investigates the relationship of source code complexity and disorganization - measured using source code change entropy - with four factors, namely the presence of refactoring activities, the number of developers working on a source code file, the participation of classes in design patterns, and the different kinds of changes occurring on the system, classified in terms of their topics extracted from commit notes. Method We carried out an exploratory study on an interval of the life-time span of four open source systems, namely ArgoUML, Eclipse-JDT, Mozilla, and Samba, with the aim of analyzing the relationship between the source code change entropy and four factors: refactoring activities, number of contributors for a file, participation of classes in design patterns, and change topics. Results The study shows that (i) the change entropy decreases after refactoring, (ii) files changed by a higher number of developers tend to exhibit a higher change entropy than others, (iii) classes participating in certain design patterns exhibit a higher change entropy than others, and (iv) changes related to different topics exhibit different change entropy, for example bug fixings exhibit a limited change entropy while changes introducing new features exhibit a high change entropy. Conclusions Results provided in this paper indicate that the nature of changes (in particular changes related to refactorings), the software design, and the number of active developers are factors related to change entropy. Our findings contribute to understand the software aging phenomenon and are preliminary to identifying better ways to contrast it. {\copyright} 2012 Springer Science+Business Media, LLC.",0,"How changes affect software entropy: An empirical study. Context Software systems continuously change for various reasons, such as adding new features, fixing bugs, or refactoring. Changes may either increase the source code complexity and disorganization, or help to reducing it. Aim This paper empirically investigates the relationship of source code complexity and disorganization - measured using source code change entropy - with four factors, namely the presence of refactoring activities, the number of developers working on a source code file, the participation of classes in design patterns, and the different kinds of changes occurring on the system, classified in terms of their topics extracted from commit notes. Method We carried out an exploratory study on an interval of the life-time span of four open source systems, namely ArgoUML, Eclipse-JDT, Mozilla, and Samba, with the aim of analyzing the relationship between the source code change entropy and four factors: refactoring activities, number of contributors for a file, participation of classes in design patterns, and change topics. Results The study shows that (i) the change entropy decreases after refactoring, (ii) files changed by a higher number of developers tend to exhibit a higher change entropy than others, (iii) classes participating in certain design patterns exhibit a higher change entropy than others, and (iv) changes related to different topics exhibit different change entropy, for example bug fixings exhibit a limited change entropy while changes introducing new features exhibit a high change entropy. Conclusions Results provided in this paper indicate that the nature of changes (in particular changes related to refactorings), the software design, and the number of active developers are factors related to change entropy. Our findings contribute to understand the software aging phenomenon and are preliminary to identifying better ways to contrast it. {\copyright} 2012 Springer Science+Business Media, LLC.",changes affect software entropy empirical study context software systems continuously change various reasons adding new features fixing bugs refactoring changes may either increase source code complexity disorganization help reducing aim paper empirically investigates relationship source code complexity disorganization measured using source code change entropy four factors namely presence refactoring activities number developers working source code file participation classes design patterns different kinds changes occurring system classified terms topics extracted commit notes method carried exploratory study interval life time span four open source systems namely argouml eclipse jdt mozilla samba aim analyzing relationship source code change entropy four factors refactoring activities number contributors file participation classes design patterns change topics results study shows change entropy decreases refactoring ii files changed higher number developers tend exhibit higher change entropy others iii classes participating certain design patterns exhibit higher change entropy others iv changes related different topics exhibit different change entropy example bug fixings exhibit limited change entropy changes introducing new features exhibit high change entropy conclusions results provided paper indicate nature changes particular changes related refactorings software design number active developers factors related change entropy findings contribute understand software aging phenomenon preliminary identifying better ways contrast copyright 2012 springer science business media llc,1,1,2,1,3,2
85,Code-smell detection as a bilevel problem,,"Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the out performance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems. {\copyright} 2014 ACM.",0,"Code-smell detection as a bilevel problem. Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the out performance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems. {\copyright} 2014 ACM.",code smell detection bilevel problem code smells represent design situations affect maintenance evolution software make system difficult evolve code smells detected general using quality metrics represent symptoms however selection suitable quality metrics challenging due absence consensus identifying code smells based set symptoms also high calibration effort determining manually threshold value metric article propose treating generation code smell detection rules bilevel optimization problem bilevel optimization problems represent class challenging optimization problems contain two levels optimization tasks problems optimal solutions lower level problem become possible feasible candidates upper level problem sense code smell detection problem treated bilevel optimization problem due lack suitable solution techniques attempted solved single level optimization problem past adaptation upper level problem generates set detection rules combination quality metrics maximizes coverage base code smell examples artificial code smells generated lower level lower level maximizes number generated artificial code smells detected rules produced upper level main advantage bilevel formulation generation detection rules limited code smell examples identified manually developers difficult collect allows prediction new code smell behavior different base examples statistical analysis experiments 31 runs nine open source systems one industrial project shows seven types code smells detected average 86 terms precision recall results confirm performance bilevel proposal compared state art code smell detection techniques evaluation performed software engineers also confirms relevance detected code smells improve quality software systems copyright 2014 acm,0,1,0,0,3,1
86,Code anomalies flock together,,"Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies-popularly known as code smells-may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to ""flock together"" to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%. {\copyright} 2016 ACM.",0,"Code anomalies flock together. Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies-popularly known as code smells-may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to ""flock together"" to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%. {\copyright} 2016 ACM.",code anomalies flock together design problems affect every software system diverse software systems discontinued reengineered due design problems design documentation often informal nonexistent design problems need located source code main difficulty identify design problem implementation stems fact problem often scattered several program elements previous work assumed code anomalies popularly known code smells may provide sufficient hints location design problem however code anomaly alone may represent partial embodiment design problem paper hypothesize code anomalies tend flock together realize design problem analyze extent groups inter related code anomalies named agglomerations suffice locate design problems analyze 2200 agglomerations found seven software systems different sizes different domains analysis indicates certain forms agglomerations consistent indicators congenital evolutionary design problems accuracy often higher 80 copyright 2016 acm,0,2,0,1,3,2
87,Identifying and summarizing systematic code changes via rule inference,Logic-based program representation; Program differencing; Rule learning; Software evolution,"Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company. {\copyright} 2013 IEEE.",0,"Identifying and summarizing systematic code changes via rule inference. Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company. {\copyright} 2013 IEEE.",identifying summarizing systematic code changes via rule inference programmers often need reason program evolved two program versions reasoning program changes challenging significant gap programmers think changes existing program differencing tools represent changes example even though modification locking protocol conceptually simple systematic code level diff extracts scattered text additions deletions per file enable programmers reason program differences high level paper proposes rule based program differencing approach automatically discovers represents systematic changes logic rules demonstrate viability approach instantiated approach two different abstraction levels java first level application programming interface api names signatures second level code elements e g types methods fields structural dependences e g method calls field accesses subtyping relationships benefit approach demonstrated application several open source projects well focus group study professional software engineers large e commerce company copyright 2013 ieee,0,2,1,1,3,2
88,Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt,empirical study; natural language processing; source code comments; Technical debt,"The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-Admitted technical debt), and that the most common types of self-Admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-Admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-Admitted technical debt, significantly outperforming the current state-of-The-Art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-Admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-Admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset. {\copyright} 1976-2012 IEEE.",0,"Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt. The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-Admitted technical debt), and that the most common types of self-Admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-Admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-Admitted technical debt, significantly outperforming the current state-of-The-Art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-Admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-Admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset. {\copyright} 1976-2012 IEEE.",using natural language processing automatically detect self admitted technical debt metaphor technical debt introduced express trade productivity quality e developers take shortcuts perform quick hacks recently work shown possible detect technical debt using source code comments e self admitted technical debt common types self admitted technical debt design requirement debt however approaches thus far heavily depend manual classification source code comments paper present approach automatically identify design requirement self admitted technical debt using natural language processing nlp study 10 open source projects ant argouml columba emf hibernate jedit jfreechart jmeter jruby squirrel sql find 1 able accurately identify self admitted technical debt significantly outperforming current state art based fixed keywords phrases 2 words related sloppy code mediocre source code quality best indicators design debt whereas words related need complete partially implemented requirement future best indicators requirement debt 3 achieve 90 percent best classification performance using little 23 percent comments design requirement self admitted technical debt 80 percent best performance using little 9 5 percent comments design requirement self admitted technical debt respectively last finding shows proposed approach achieve good accuracy even relatively small training dataset copyright 1976 2012 ieee,0,1,0,1,3,2
89,Metaphor wars: Conceptual metaphors in human life,,"The study of metaphor is now firmly established as a central topic within cognitive science and the humanities. We marvel at the creative dexterity of gifted speakers and writers for their special talents in both thinking about certain ideas in new ways, and communicating these thoughts in vivid, poetic forms. Yet metaphors may not only be special communicative devices, but a fundamental part of everyday cognition in the form of 'conceptual metaphors'. An enormous body of empirical evidence from cognitive linguistics and related disciplines has emerged detailing how conceptual metaphors underlie significant aspects of language, thought, cultural and expressive action. Despite its influence and popularity, there have been major criticisms of conceptual metaphor. This book offers an evaluation of the arguments and empirical evidence for and against conceptual metaphors, much of which scholars on both sides of the wars fail to properly acknowledge. {\copyright} Raymond W. Gibbs, Jr. 2017. All rights reserved.",0,"Metaphor wars: Conceptual metaphors in human life. The study of metaphor is now firmly established as a central topic within cognitive science and the humanities. We marvel at the creative dexterity of gifted speakers and writers for their special talents in both thinking about certain ideas in new ways, and communicating these thoughts in vivid, poetic forms. Yet metaphors may not only be special communicative devices, but a fundamental part of everyday cognition in the form of 'conceptual metaphors'. An enormous body of empirical evidence from cognitive linguistics and related disciplines has emerged detailing how conceptual metaphors underlie significant aspects of language, thought, cultural and expressive action. Despite its influence and popularity, there have been major criticisms of conceptual metaphor. This book offers an evaluation of the arguments and empirical evidence for and against conceptual metaphors, much of which scholars on both sides of the wars fail to properly acknowledge. {\copyright} Raymond W. Gibbs, Jr. 2017. All rights reserved.",metaphor wars conceptual metaphors human life study metaphor firmly established central topic within cognitive science humanities marvel creative dexterity gifted speakers writers special talents thinking certain ideas new ways communicating thoughts vivid poetic forms yet metaphors may special communicative devices fundamental part everyday cognition form conceptual metaphors enormous body empirical evidence cognitive linguistics related disciplines emerged detailing conceptual metaphors underlie significant aspects language thought cultural expressive action despite influence popularity major criticisms conceptual metaphor book offers evaluation arguments empirical evidence conceptual metaphors much scholars sides wars fail properly acknowledge copyright raymond w gibbs jr 2017 rights reserved,2,0,1,2,1,3
90,Adaptive detection of design flaws,Code smell; Design flaw; Machine learning; Object-oriented design; Program analysis; Refactoring; Software quality,"Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid ""bad"" - incomprehensible, hardly expandable and changeable - program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study. {\copyright} 2005 Elsevier B.V. All rights reserved.",1,"Adaptive detection of design flaws. Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid ""bad"" - incomprehensible, hardly expandable and changeable - program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study. {\copyright} 2005 Elsevier B.V. All rights reserved.",adaptive detection design flaws criteria software quality measurement depend application area large software systems criteria like maintainability comprehensibility extensibility play important role aim identify design flaws software systems automatically thus avoid bad incomprehensible hardly expandable changeable program structures depending perception experience searching engineer design flaws interpreted different way propose combine known methods finding design flaws basis metrics machine learning mechanisms design flaw detection adaptable different views paper presents underlying method describes analysis tool java programs shows results initial case study copyright 2005 elsevier b v rights reserved,0,2,2,1,3,2
91,Community detection in networks: A multidisciplinary review,Anomaly detection; Clustering algorithms; Community detection; Modularity; Online social networks,"The modern science of networks has made significant advancement in the modeling of complex real-world systems. One of the most important features in these networks is the existence of community structure. In recent years, many community detection algorithms have been proposed to unveil the structural properties and dynamic behaviors of networks. In this study, we attempt a contemporary survey on the methods of community detection and its applications in the various domains of real life. Besides highlighting the strengths and weaknesses of each community detection approach, different aspects of algorithmic performance comparison and their testing on standard benchmarks are discussed. The challenges faced by community detection algorithms, open issues and future trends related to community detection are also postulated. The main goal of this paper is to put forth a review of prevailing community detection algorithms that range from traditional algorithms to state of the art algorithms for overlapping community detection. Algorithms based on dimensionality reduction techniques such as non-negative matrix factorization (NMF) and principal component analysis (PCA) are also focused. This study will serve as an up-to-date report on the evolution of community detection and its potential applications in various domains from real world networks. {\copyright} 2018 Elsevier Ltd",0,"Community detection in networks: A multidisciplinary review. The modern science of networks has made significant advancement in the modeling of complex real-world systems. One of the most important features in these networks is the existence of community structure. In recent years, many community detection algorithms have been proposed to unveil the structural properties and dynamic behaviors of networks. In this study, we attempt a contemporary survey on the methods of community detection and its applications in the various domains of real life. Besides highlighting the strengths and weaknesses of each community detection approach, different aspects of algorithmic performance comparison and their testing on standard benchmarks are discussed. The challenges faced by community detection algorithms, open issues and future trends related to community detection are also postulated. The main goal of this paper is to put forth a review of prevailing community detection algorithms that range from traditional algorithms to state of the art algorithms for overlapping community detection. Algorithms based on dimensionality reduction techniques such as non-negative matrix factorization (NMF) and principal component analysis (PCA) are also focused. This study will serve as an up-to-date report on the evolution of community detection and its potential applications in various domains from real world networks. {\copyright} 2018 Elsevier Ltd",community detection networks multidisciplinary review modern science networks made significant advancement modeling complex real world systems one important features networks existence community structure recent years many community detection algorithms proposed unveil structural properties dynamic behaviors networks study attempt contemporary survey methods community detection applications various domains real life besides highlighting strengths weaknesses community detection approach different aspects algorithmic performance comparison testing standard benchmarks discussed challenges faced community detection algorithms open issues future trends related community detection also postulated main goal paper put forth review prevailing community detection algorithms range traditional algorithms state art algorithms overlapping community detection algorithms based dimensionality reduction techniques non negative matrix factorization nmf principal component analysis pca also focused study serve date report evolution community detection potential applications various domains real world networks copyright 2018 elsevier ltd,0,2,0,0,2,2
92,Detecting software design defects using relational association rule mining,Association rule mining; Data mining; Defect detection; Machine learning; Software design,"In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. {\copyright} 2014, Springer-Verlag London.",0,"Detecting software design defects using relational association rule mining. In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. {\copyright} 2014, Springer-Verlag London.",detecting software design defects using relational association rule mining paper approaching machine learning perspective problem automatically detecting defective software entities classes methods existing software systems problem major importance software maintenance evolution order improve internal quality software system identifying faulty entities classes modules methods essential software developers defective software entities hard identify machine learning based classification models still developed approach problem detecting software design defects proposing novel method based relational association rule mining detecting faulty entities existing software systems relational association rules particular type association rules describe numerical orderings attributes commonly occur dataset method based discovery relational association rules identifying design defects software experiments open source software conducted order detect defective classes object oriented software systems comparison approach similar existing approaches provided obtained results show method effective software design defect detection confirms potential proposal copyright 2014 springer verlag london,1,2,2,1,3,0
93,Bayesian networks for evidence-based decision-making in software engineering,Bayesian networks; Bayesian statistics; Evidence-based decision-making; post-release defects; software metrics; software reliability,"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making. {\copyright} 2014 IEEE.",0,"Bayesian networks for evidence-based decision-making in software engineering. Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making. {\copyright} 2014 IEEE.",bayesian networks evidence based decision making software engineering recommendation systems software engineering se designed integrate evidence practitioners experience bayesian networks bns provide natural statistical framework evidence based decision making incorporating integrated summary available evidence associated uncertainty consequences study follow lead computational biology healthcare decision making investigate applications bns se terms 1 main software engineering challenges addressed 2 techniques used learn causal relationships among variables 3 techniques used infer parameters 4 variable types used bn nodes conduct systematic mapping study investigate four facets compare current usage bns se two domains subsequently highlight main limitations usage bns se propose hybrid bn improve evidence based decision making se two industrial cases build sample hybrid bns evaluate performance results empirical analyses show hybrid bns powerful frameworks combine expert knowledge quantitative data researchers se become aware underlying dynamics bns proposed models also advance naturally contribute evidence based decision making copyright 2014 ieee,1,2,0,2,2,2
94,Studying evolving software ecosystems based on ecological models,,"Research on software evolution is very active, but evolutionary principles, models and theories that properly explain why and how software systems evolve over time are still lacking. Similarly, more empirical research is needed to understand how different software projects co-exist and co-evolve, and how contributors collaborate within their encompassing software ecosystem. In this chapter, we explore the differences and analogies between natural ecosystems and biological evolution on the one hand, and software ecosystems and software evolution on the other hand. The aim is to learn from research in ecology to advance the understanding of evolving software ecosystems. Ultimately, we wish to use such knowledge to derive diagnostic tools aiming to predict survival of software projects within their ecosystem, to analyse and optimise the fitness of software projcts in their environment, and to help software project communities in managing their projects better. {\copyright} Springer-Verlag Berlin Heidelberg 2014.",0,"Studying evolving software ecosystems based on ecological models. Research on software evolution is very active, but evolutionary principles, models and theories that properly explain why and how software systems evolve over time are still lacking. Similarly, more empirical research is needed to understand how different software projects co-exist and co-evolve, and how contributors collaborate within their encompassing software ecosystem. In this chapter, we explore the differences and analogies between natural ecosystems and biological evolution on the one hand, and software ecosystems and software evolution on the other hand. The aim is to learn from research in ecology to advance the understanding of evolving software ecosystems. Ultimately, we wish to use such knowledge to derive diagnostic tools aiming to predict survival of software projects within their ecosystem, to analyse and optimise the fitness of software projcts in their environment, and to help software project communities in managing their projects better. {\copyright} Springer-Verlag Berlin Heidelberg 2014.",studying evolving software ecosystems based ecological models research software evolution active evolutionary principles models theories properly explain software systems evolve time still lacking similarly empirical research needed understand different software projects co exist co evolve contributors collaborate within encompassing software ecosystem chapter explore differences analogies natural ecosystems biological evolution one hand software ecosystems software evolution hand aim learn research ecology advance understanding evolving software ecosystems ultimately wish use knowledge derive diagnostic tools aiming predict survival software projects within ecosystem analyse optimise fitness software projcts environment help software project communities managing projects better copyright springer verlag berlin heidelberg 2014,2,0,2,2,2,2
95,A survey on software smells,Antipatterns; Code smells; Maintainability; Smell detection tools; Software quality; Software smells; Technical debt,"Context: Smells in software systems impair software quality and make them hard to maintain and evolve. The software engineering community has explored various dimensions concerning smells and produced extensive research related to smells. The plethora of information poses challenges to the community to comprehend the state-of-the-art tools and techniques. Objective: We aim to present the current knowledge related to software smells and identify challenges as well as opportunities in the current practices. Method: We explore the definitions of smells, their causes as well as effects, and their detection mechanisms presented in the current literature. We studied 445 primary studies in detail, synthesized the information, and documented our observations. Results: The study reveals five possible defining characteristics of smells --- indicator, poor solution, violates best-practices, impacts quality, and recurrence. We curate ten common factors that cause smells to occur including lack of skill or awareness and priority to features over quality. We classify existing smell detection methods into five groups --- metrics, rules/heuristics, history, machine learning, and optimization-based detection. Challenges in the smells detection include the tools' proneness to false-positives and poor coverage of smells detectable by existing tools. {\copyright} 2017",0,"A survey on software smells. Context: Smells in software systems impair software quality and make them hard to maintain and evolve. The software engineering community has explored various dimensions concerning smells and produced extensive research related to smells. The plethora of information poses challenges to the community to comprehend the state-of-the-art tools and techniques. Objective: We aim to present the current knowledge related to software smells and identify challenges as well as opportunities in the current practices. Method: We explore the definitions of smells, their causes as well as effects, and their detection mechanisms presented in the current literature. We studied 445 primary studies in detail, synthesized the information, and documented our observations. Results: The study reveals five possible defining characteristics of smells --- indicator, poor solution, violates best-practices, impacts quality, and recurrence. We curate ten common factors that cause smells to occur including lack of skill or awareness and priority to features over quality. We classify existing smell detection methods into five groups --- metrics, rules/heuristics, history, machine learning, and optimization-based detection. Challenges in the smells detection include the tools' proneness to false-positives and poor coverage of smells detectable by existing tools. {\copyright} 2017",survey software smells context smells software systems impair software quality make hard maintain evolve software engineering community explored various dimensions concerning smells produced extensive research related smells plethora information poses challenges community comprehend state art tools techniques objective aim present current knowledge related software smells identify challenges well opportunities current practices method explore definitions smells causes well effects detection mechanisms presented current literature studied 445 primary studies detail synthesized information documented observations results study reveals five possible defining characteristics smells indicator poor solution violates best practices impacts quality recurrence curate ten common factors cause smells occur including lack skill awareness priority features quality classify existing smell detection methods five groups metrics rules heuristics history machine learning optimization based detection challenges smells detection include tools proneness false positives poor coverage smells detectable existing tools copyright 2017,0,2,2,0,3,1
96,Olfactory receptor subgenomes linked with broad ecological adaptations in sauropsida,adaptation; birds; olfactory receptors; selection,"Olfactory receptors (ORs) govern a prime sensory function. Extant birds have distinct olfactory abilities, but the molecular mechanisms underlining diversification and specialization remain mostly unknown. We explored OR diversity in 48 phylogenetic and ecologically diverse birds and 2 reptiles (alligator and green sea turtle). OR subgenomes showed species- and lineage-specific variation related with ecological requirements. Overall 1,953 OR genes were identified in reptiles and 16,503 in birds. The two reptiles had larger OR gene repertoires (989 and 964 genes, respectively) than birds (182-688 genes). Overall, birds had more pseudogenes (7,855) than intact genes (1,944). The alligator had significantly more functional genes than sea turtle, likely because of distinct foraging habits. We found rapid species-specific expansion and positive selection in OR14 (detects hydrophobic compounds) in birds and in OR51 and OR52 (detect hydrophilic compounds) in sea turtle, suggestive of terrestrial and aquatic adaptations, respectively. Ecological partitioning among birds of prey, water birds, land birds, and vocal learners showed that diverse ecological factors determined olfactory ability and influenced corresponding olfactory-receptor subgenome. OR5/8/9 was expanded in predatory birds and alligator, suggesting adaptive specialization for carnivory. OR families 2/13, 51, and 52 were correlated with aquatic adaptations (water birds), OR families 6 and 10 were more pronounced in vocal-learning birds, whereas most specialized land birds had an expanded OR family 14. Olfactory bulb ratio (OBR) and OR gene repertoire were correlated. Birds that forage for prey (carnivores/piscivores) had relatively complex OBR and OR gene repertoires compared with modern birds, including passerines, perhaps due to highly developed cognitive capacities facilitating foraging innovations. {\copyright} The Author 2015. Published by Oxford University Press on behalf of the Society for Molecular Biology and Evolution.",0,"Olfactory receptor subgenomes linked with broad ecological adaptations in sauropsida. Olfactory receptors (ORs) govern a prime sensory function. Extant birds have distinct olfactory abilities, but the molecular mechanisms underlining diversification and specialization remain mostly unknown. We explored OR diversity in 48 phylogenetic and ecologically diverse birds and 2 reptiles (alligator and green sea turtle). OR subgenomes showed species- and lineage-specific variation related with ecological requirements. Overall 1,953 OR genes were identified in reptiles and 16,503 in birds. The two reptiles had larger OR gene repertoires (989 and 964 genes, respectively) than birds (182-688 genes). Overall, birds had more pseudogenes (7,855) than intact genes (1,944). The alligator had significantly more functional genes than sea turtle, likely because of distinct foraging habits. We found rapid species-specific expansion and positive selection in OR14 (detects hydrophobic compounds) in birds and in OR51 and OR52 (detect hydrophilic compounds) in sea turtle, suggestive of terrestrial and aquatic adaptations, respectively. Ecological partitioning among birds of prey, water birds, land birds, and vocal learners showed that diverse ecological factors determined olfactory ability and influenced corresponding olfactory-receptor subgenome. OR5/8/9 was expanded in predatory birds and alligator, suggesting adaptive specialization for carnivory. OR families 2/13, 51, and 52 were correlated with aquatic adaptations (water birds), OR families 6 and 10 were more pronounced in vocal-learning birds, whereas most specialized land birds had an expanded OR family 14. Olfactory bulb ratio (OBR) and OR gene repertoire were correlated. Birds that forage for prey (carnivores/piscivores) had relatively complex OBR and OR gene repertoires compared with modern birds, including passerines, perhaps due to highly developed cognitive capacities facilitating foraging innovations. {\copyright} The Author 2015. Published by Oxford University Press on behalf of the Society for Molecular Biology and Evolution.",olfactory receptor subgenomes linked broad ecological adaptations sauropsida olfactory receptors ors govern prime sensory function extant birds distinct olfactory abilities molecular mechanisms underlining diversification specialization remain mostly unknown explored diversity 48 phylogenetic ecologically diverse birds 2 reptiles alligator green sea turtle subgenomes showed species lineage specific variation related ecological requirements overall 1 953 genes identified reptiles 16 503 birds two reptiles larger gene repertoires 989 964 genes respectively birds 182 688 genes overall birds pseudogenes 7 855 intact genes 1 944 alligator significantly functional genes sea turtle likely distinct foraging habits found rapid species specific expansion positive selection or14 detects hydrophobic compounds birds or51 or52 detect hydrophilic compounds sea turtle suggestive terrestrial aquatic adaptations respectively ecological partitioning among birds prey water birds land birds vocal learners showed diverse ecological factors determined olfactory ability influenced corresponding olfactory receptor subgenome or5 8 9 expanded predatory birds alligator suggesting adaptive specialization carnivory families 2 13 51 52 correlated aquatic adaptations water birds families 6 10 pronounced vocal learning birds whereas specialized land birds expanded family 14 olfactory bulb ratio obr gene repertoire correlated birds forage prey carnivores piscivores relatively complex obr gene repertoires compared modern birds including passerines perhaps due highly developed cognitive capacities facilitating foraging innovations copyright author 2015 published oxford university press behalf society molecular biology evolution,2,1,1,2,2,3
97,Evolving software systems,,"During the last few years, software evolution research has explored new domains such as the study of socio-technical aspects and collaboration between different individuals contributing to a software system, theuse of search-based techniques and meta-heuristics, the mining of unstructured software repositories, the evolution of software requirements, and the dynamic adaptation of software systems at runtime. Also more and more attention is being paid to the evolution of collections of inter-related and inter-dependent software projects, be it in the form of web systems, software product families, software ecosystems or systems of systems. Withthis book, the editors present insightful contributions on these and other domains currently being intensively explored, written by renowned researchers in the respective fields of software evolution. Each chapter presents the state of the art in a particular topic, as well as the current research, available tool support and remaining challenges. The book is complemented by a glossary of important terms used in the community, a reference list of nearly 1,000 papers and books and tips on additional resources that may be useful to the reader(reference books, journals, standards and major scientific events in the domain of software evolution and datasets). This book is intended for all those interested in software engineering, and more particularly, software maintenance and evolution. Researchers and software practitioners alike will find in the contributed chapters an overview of the most recent findings, covering a broad spectrum of software evolution topics. In addition, it can also serve as the basis of graduate or postgraduate courses on e.g., software evolution, requirements engineering, model-driven software development or social informatics. {\copyright} Springer-Verlag Berlin Heidelberg 2014.",0,"Evolving software systems. During the last few years, software evolution research has explored new domains such as the study of socio-technical aspects and collaboration between different individuals contributing to a software system, theuse of search-based techniques and meta-heuristics, the mining of unstructured software repositories, the evolution of software requirements, and the dynamic adaptation of software systems at runtime. Also more and more attention is being paid to the evolution of collections of inter-related and inter-dependent software projects, be it in the form of web systems, software product families, software ecosystems or systems of systems. Withthis book, the editors present insightful contributions on these and other domains currently being intensively explored, written by renowned researchers in the respective fields of software evolution. Each chapter presents the state of the art in a particular topic, as well as the current research, available tool support and remaining challenges. The book is complemented by a glossary of important terms used in the community, a reference list of nearly 1,000 papers and books and tips on additional resources that may be useful to the reader(reference books, journals, standards and major scientific events in the domain of software evolution and datasets). This book is intended for all those interested in software engineering, and more particularly, software maintenance and evolution. Researchers and software practitioners alike will find in the contributed chapters an overview of the most recent findings, covering a broad spectrum of software evolution topics. In addition, it can also serve as the basis of graduate or postgraduate courses on e.g., software evolution, requirements engineering, model-driven software development or social informatics. {\copyright} Springer-Verlag Berlin Heidelberg 2014.",evolving software systems last years software evolution research explored new domains study socio technical aspects collaboration different individuals contributing software system theuse search based techniques meta heuristics mining unstructured software repositories evolution software requirements dynamic adaptation software systems runtime also attention paid evolution collections inter related inter dependent software projects form web systems software product families software ecosystems systems systems withthis book editors present insightful contributions domains currently intensively explored written renowned researchers respective fields software evolution chapter presents state art particular topic well current research available tool support remaining challenges book complemented glossary important terms used community reference list nearly 1 000 papers books tips additional resources may useful reader reference books journals standards major scientific events domain software evolution datasets book intended interested software engineering particularly software maintenance evolution researchers software practitioners alike find contributed chapters overview recent findings covering broad spectrum software evolution topics addition also serve basis graduate postgraduate courses e g software evolution requirements engineering model driven software development social informatics copyright springer verlag berlin heidelberg 2014,2,0,0,2,2,2
98,Fungal and bacterial volatile organic compounds: An overview and their role as ecological signaling agents,,"Both fungi and bacteria emit many volatile organic compounds (VOCs) as mixtures of low molecular mass alcohols, aldehydes, esters, terpenoids, thiols, and other small molecules that easily volatilize. Most determination (separation and identification) of VOCs now relies on gas chromatography-mass spectrometry (GC-MS) but developments in ``electronic nose'' technology promise to revolutionize the field. Microbial VOC profiles are both complex and dynamic: the compounds produced and their abundance vary with the producing species, the age of the colony, water availability, the substrate, the temperature, and other environmental parameters. The single most commonly reported volatile from fungi is 1-octen-3-ol which is a breakdown product of linoleic acid. It functions as a hormone within many fungal species, serves as both an attractant and deterrent for certain species of arthropods, and exhibits toxicity at relatively low concentrations in model systems. Bacterial and fungal VOCs have been studied by scientists from a broad range of subdisciplines in both theoretical and applied contexts. VOCs are exploited for their food and flavor properties, their use as indirect indicators of microbial growth, their ability to stimulate plant growth, and their ability to attract insect pests. Because these compounds can diffuse a long way from their point of origin, they are excellent chemical signaling molecules (semiochemicals) in non-aqueous habitats and facilitate the ability of microbes to engage in ``chemical conversations.'' The physiological effects of bacterial and fungal VOCs in host-pathogen relationships and in mediating interspecific associations in natural ecosystem functioning is an emerging frontier for future research. {\copyright} Springer-Verlag Berlin Heidelberg 2012.",0,"Fungal and bacterial volatile organic compounds: An overview and their role as ecological signaling agents. Both fungi and bacteria emit many volatile organic compounds (VOCs) as mixtures of low molecular mass alcohols, aldehydes, esters, terpenoids, thiols, and other small molecules that easily volatilize. Most determination (separation and identification) of VOCs now relies on gas chromatography-mass spectrometry (GC-MS) but developments in ``electronic nose'' technology promise to revolutionize the field. Microbial VOC profiles are both complex and dynamic: the compounds produced and their abundance vary with the producing species, the age of the colony, water availability, the substrate, the temperature, and other environmental parameters. The single most commonly reported volatile from fungi is 1-octen-3-ol which is a breakdown product of linoleic acid. It functions as a hormone within many fungal species, serves as both an attractant and deterrent for certain species of arthropods, and exhibits toxicity at relatively low concentrations in model systems. Bacterial and fungal VOCs have been studied by scientists from a broad range of subdisciplines in both theoretical and applied contexts. VOCs are exploited for their food and flavor properties, their use as indirect indicators of microbial growth, their ability to stimulate plant growth, and their ability to attract insect pests. Because these compounds can diffuse a long way from their point of origin, they are excellent chemical signaling molecules (semiochemicals) in non-aqueous habitats and facilitate the ability of microbes to engage in ``chemical conversations.'' The physiological effects of bacterial and fungal VOCs in host-pathogen relationships and in mediating interspecific associations in natural ecosystem functioning is an emerging frontier for future research. {\copyright} Springer-Verlag Berlin Heidelberg 2012.",fungal bacterial volatile organic compounds overview role ecological signaling agents fungi bacteria emit many volatile organic compounds vocs mixtures low molecular mass alcohols aldehydes esters terpenoids thiols small molecules easily volatilize determination separation identification vocs relies gas chromatography mass spectrometry gc ms developments electronic nose technology promise revolutionize field microbial voc profiles complex dynamic compounds produced abundance vary producing species age colony water availability substrate temperature environmental parameters single commonly reported volatile fungi 1 octen 3 ol breakdown product linoleic acid functions hormone within many fungal species serves attractant deterrent certain species arthropods exhibits toxicity relatively low concentrations model systems bacterial fungal vocs studied scientists broad range subdisciplines theoretical applied contexts vocs exploited food flavor properties use indirect indicators microbial growth ability stimulate plant growth ability attract insect pests compounds diffuse long way point origin excellent chemical signaling molecules semiochemicals non aqueous habitats facilitate ability microbes engage chemical conversations physiological effects bacterial fungal vocs host pathogen relationships mediating interspecific associations natural ecosystem functioning emerging frontier future research copyright springer verlag berlin heidelberg 2012,2,0,1,2,2,3
99,Introductory programming: A systematic literature review,CS1; Introductory programming; ITiCSE working group; Literature review; Novice programming; Overview; Review; SLR; Systematic literature review; Systematic review,"As computing becomes a mainstream discipline embedded in the school curriculum and acts as an enabler for an increasing range of academic disciplines in higher education, the literature on introductory programming is growing. Although there have been several reviews that focus on specific aspects of introductory programming, there has been no broad overview of the literature exploring recent trends across the breadth of introductory programming. This paper is the report of an ITiCSE working group that conducted a systematic review in order to gain an overview of the introductory programming literature. Partitioning the literature into papers addressing the student, teaching, the curriculum, and assessment, we explore trends, highlight advances in knowledge over the past 15 years, and indicate possible directions for future research. {\copyright} 2018 Association for Computing Machinery.",0,"Introductory programming: A systematic literature review. As computing becomes a mainstream discipline embedded in the school curriculum and acts as an enabler for an increasing range of academic disciplines in higher education, the literature on introductory programming is growing. Although there have been several reviews that focus on specific aspects of introductory programming, there has been no broad overview of the literature exploring recent trends across the breadth of introductory programming. This paper is the report of an ITiCSE working group that conducted a systematic review in order to gain an overview of the introductory programming literature. Partitioning the literature into papers addressing the student, teaching, the curriculum, and assessment, we explore trends, highlight advances in knowledge over the past 15 years, and indicate possible directions for future research. {\copyright} 2018 Association for Computing Machinery.",introductory programming systematic literature review computing becomes mainstream discipline embedded school curriculum acts enabler increasing range academic disciplines higher education literature introductory programming growing although several reviews focus specific aspects introductory programming broad overview literature exploring recent trends across breadth introductory programming paper report iticse working group conducted systematic review order gain overview introductory programming literature partitioning literature papers addressing student teaching curriculum assessment explore trends highlight advances knowledge past 15 years indicate possible directions future research copyright 2018 association computing machinery,2,0,1,2,2,3
100,Search-based inference of polynomial metamorphic relations,Invariant inference; Metamorphic testing; Particle swarm optimization,"Metamorphic testing (MT) is an effective methodology for testing those so-called ""non-testable"" programs (e.g., scientific programs), where it is sometimes very difficult for testers to know whether the outputs are correct. In metamorphic testing, metamorphic relations (MRs) (which specify how particular changes to the input of the program under test would change the output) play an essential role. However, testers may typically have to obtain MRs manually. In this paper, we propose a search-based approach to automatic inference of polynomial MRs for a program under test. In particular, we use a set of parameters to represent a particular class of MRs, which we refer to as polynomial MRs, and turn the problem of inferring MRs into a problem of searching for suitable values of the parameters. We then dynamically analyze multiple executions of the program, and use particle swarm optimization to solve the search problem. To improve the quality of inferred MRs, we further use MR filtering to remove some inferred MRs. We also conducted three empirical studies to evaluate our approach using four scientific libraries (including 189 scientific functions). From our empirical results, our approach is able to infer many high-quality MRs in acceptable time (i.e., from 9.87 seconds to 1231.16 seconds), which are effective in detecting faults with no false detection. {\copyright} 2014 ACM.",0,"Search-based inference of polynomial metamorphic relations. Metamorphic testing (MT) is an effective methodology for testing those so-called ""non-testable"" programs (e.g., scientific programs), where it is sometimes very difficult for testers to know whether the outputs are correct. In metamorphic testing, metamorphic relations (MRs) (which specify how particular changes to the input of the program under test would change the output) play an essential role. However, testers may typically have to obtain MRs manually. In this paper, we propose a search-based approach to automatic inference of polynomial MRs for a program under test. In particular, we use a set of parameters to represent a particular class of MRs, which we refer to as polynomial MRs, and turn the problem of inferring MRs into a problem of searching for suitable values of the parameters. We then dynamically analyze multiple executions of the program, and use particle swarm optimization to solve the search problem. To improve the quality of inferred MRs, we further use MR filtering to remove some inferred MRs. We also conducted three empirical studies to evaluate our approach using four scientific libraries (including 189 scientific functions). From our empirical results, our approach is able to infer many high-quality MRs in acceptable time (i.e., from 9.87 seconds to 1231.16 seconds), which are effective in detecting faults with no false detection. {\copyright} 2014 ACM.",search based inference polynomial metamorphic relations metamorphic testing mt effective methodology testing called non testable programs e g scientific programs sometimes difficult testers know whether outputs correct metamorphic testing metamorphic relations mrs specify particular changes input program test would change output play essential role however testers may typically obtain mrs manually paper propose search based approach automatic inference polynomial mrs program test particular use set parameters represent particular class mrs refer polynomial mrs turn problem inferring mrs problem searching suitable values parameters dynamically analyze multiple executions program use particle swarm optimization solve search problem improve quality inferred mrs use mr filtering remove inferred mrs also conducted three empirical studies evaluate approach using four scientific libraries including 189 scientific functions empirical results approach able infer many high quality mrs acceptable time e 9 87 seconds 1231 16 seconds effective detecting faults false detection copyright 2014 acm,0,1,0,1,0,2
101,Effective regression test case selection: A systematic literature review,Cost effectiveness; Coverage; Fault detection ability; Slr; Software testing,"Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results. The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible. There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage. {\copyright} 2017 ACM.",0,"Effective regression test case selection: A systematic literature review. Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results. The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible. There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage. {\copyright} 2017 ACM.",effective regression test case selection systematic literature review regression test case selection techniques attempt increase testing effectiveness based measurement capabilities cost coverage fault detection systematic literature review presents state art research effective regression test case selection techniques examined 47 empirical studies published 2007 2015 selected studies categorized according selection procedure empirical study design adequacy criteria respect effectiveness measurement capability methods used measure validity results results showed mining learning based regression test case selection reported 39 studies unit level testing reported 18 studies object oriented environment java used 26 studies structural faults common target used 55 studies overall 39 studies conducted followed experimental guidelines reproducible 7 different cost measures 13 different coverage types 5 fault detection metrics reported studies also observed 70 studies analyzed used cost effectiveness measure compared 31 used fault detection capability 16 used coverage copyright 2017 acm,1,1,0,0,0,2
102,"Managing agile: Strategy, implementation, organisation and people",,"This book examines agile approaches from a management perspective by focusing on matters of strategy, implementation, organization and people. It examines the turbulence of the marketplace and business environment in order to identify what role agile management has to play in coping with such change and uncertainty. Based on observations, personal experience and extensive research, it clearly identifies the fabric of the agile organization, helping managers to become agile leaders in an uncertain world. The book opens with a broad survey of agile strategies, comparing and contrasting some of the major methodologies selected on the basis of where they lie on a continuum of ceremony and formality, ranging from the minimalist technique-driven and software engineering focused XP, to the pragmatic product-project paradigm that is Scrum and its scaled counterpart SAFe{	extregistered}, to the comparatively project-centric DSDM. Subsequently, the core of the book focuses on DSDM, owing to the method's comprehensive elaboration of program and project management practices. This work will chiefly be of interest to all those with decision-making authority within their organizations (e.g., senior managers, line managers, program, project and risk managers) and for whom topics such as strategy, finance, quality, governance and risk management constitute a daily aspect of their work. It will, however, also be of interest to those readers in advanced management or business administration cou rses (e.g., MBA, MSc), who wish to engage in the management of agile organizations and thus need to adapt their skills and knowledge accordingly. {\copyright} Springer International Publishing Switzerland 2015.",0,"Managing agile: Strategy, implementation, organisation and people. This book examines agile approaches from a management perspective by focusing on matters of strategy, implementation, organization and people. It examines the turbulence of the marketplace and business environment in order to identify what role agile management has to play in coping with such change and uncertainty. Based on observations, personal experience and extensive research, it clearly identifies the fabric of the agile organization, helping managers to become agile leaders in an uncertain world. The book opens with a broad survey of agile strategies, comparing and contrasting some of the major methodologies selected on the basis of where they lie on a continuum of ceremony and formality, ranging from the minimalist technique-driven and software engineering focused XP, to the pragmatic product-project paradigm that is Scrum and its scaled counterpart SAFe{	extregistered}, to the comparatively project-centric DSDM. Subsequently, the core of the book focuses on DSDM, owing to the method's comprehensive elaboration of program and project management practices. This work will chiefly be of interest to all those with decision-making authority within their organizations (e.g., senior managers, line managers, program, project and risk managers) and for whom topics such as strategy, finance, quality, governance and risk management constitute a daily aspect of their work. It will, however, also be of interest to those readers in advanced management or business administration cou rses (e.g., MBA, MSc), who wish to engage in the management of agile organizations and thus need to adapt their skills and knowledge accordingly. {\copyright} Springer International Publishing Switzerland 2015.",managing agile strategy implementation organisation people book examines agile approaches management perspective focusing matters strategy implementation organization people examines turbulence marketplace business environment order identify role agile management play coping change uncertainty based observations personal experience extensive research clearly identifies fabric agile organization helping managers become agile leaders uncertain world book opens broad survey agile strategies comparing contrasting major methodologies selected basis lie continuum ceremony formality ranging minimalist technique driven software engineering focused xp pragmatic product project paradigm scrum scaled counterpart safe extregistered comparatively project centric dsdm subsequently core book focuses dsdm owing method comprehensive elaboration program project management practices work chiefly interest decision making authority within organizations e g senior managers line managers program project risk managers topics strategy finance quality governance risk management constitute daily aspect work however also interest readers advanced management business administration cou rses e g mba msc wish engage management agile organizations thus need adapt skills knowledge accordingly copyright springer international publishing switzerland 2015,2,0,1,2,2,3
103,Measuring architecture quality by structure plus history analysis,change history; fault prediction; measure; software architecture; structure,"This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented. {\copyright} 2013 IEEE.",0,"Measuring architecture quality by structure plus history analysis. This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented. {\copyright} 2013 IEEE.",measuring architecture quality structure plus history analysis case study combines known software structure revision history analysis techniques known new ways predict bug related change frequency uncover architecture related risks agile industrial software development project applied suite structure history measures statistically analyzed correlations detected architecture issues identifying outliers distributions measured values investigating architectural significance associated classes used clustering method identify sets files often change together without structurally close together investigating whether architecture issues among root causes development team confirmed identified clusters reflected significant architectural violations unstable key interfaces important undocumented assumptions shared modules combined structure diagrams history data justified refactoring proposal accepted project manager implemented copyright 2013 ieee,1,2,0,1,3,2
104,Developer Micro Interaction Metrics for Software Defect Prediction,Defect prediction; developer interaction; Mylyn; software metrics; software quality,"To facilitate software quality assurance, defect prediction metrics, such as source code metrics, change churns, and the number of previous defects, have been actively studied. Despite the common understanding that developer behavioral interaction patterns can affect software quality, these widely used defect prediction metrics do not consider developer behavior. We therefore propose micro interaction metrics (MIMs), which are metrics that leverage developer interaction information. The developer interactions, such as file editing and browsing events in task sessions, are captured and stored as information by Mylyn, an Eclipse plug-in. Our experimental evaluation demonstrates that MIMs significantly improve overall defect prediction accuracy when combined with existing software measures, perform well in a cost-effective manner, and provide intuitive feedback that enables developers to recognize their own inefficient behaviors during software development. {\copyright} 2016 IEEE.",0,"Developer Micro Interaction Metrics for Software Defect Prediction. To facilitate software quality assurance, defect prediction metrics, such as source code metrics, change churns, and the number of previous defects, have been actively studied. Despite the common understanding that developer behavioral interaction patterns can affect software quality, these widely used defect prediction metrics do not consider developer behavior. We therefore propose micro interaction metrics (MIMs), which are metrics that leverage developer interaction information. The developer interactions, such as file editing and browsing events in task sessions, are captured and stored as information by Mylyn, an Eclipse plug-in. Our experimental evaluation demonstrates that MIMs significantly improve overall defect prediction accuracy when combined with existing software measures, perform well in a cost-effective manner, and provide intuitive feedback that enables developers to recognize their own inefficient behaviors during software development. {\copyright} 2016 IEEE.",developer micro interaction metrics software defect prediction facilitate software quality assurance defect prediction metrics source code metrics change churns number previous defects actively studied despite common understanding developer behavioral interaction patterns affect software quality widely used defect prediction metrics consider developer behavior therefore propose micro interaction metrics mims metrics leverage developer interaction information developer interactions file editing browsing events task sessions captured stored information mylyn eclipse plug experimental evaluation demonstrates mims significantly improve overall defect prediction accuracy combined existing software measures perform well cost effective manner provide intuitive feedback enables developers recognize inefficient behaviors software development copyright 2016 ieee,1,2,0,1,3,2
105,Mining co-change information to understand when build changes are necessary,Build systems; mining software repositories; software evolution,"As a software project ages, its source code is modified to add new features, restructure existing ones, and fix defects. These source code changes often induce changes in the build system, i.e., the system that specifies how source code is translated into deliverables. However, since developers are often not familiar with the complex and occasionally archaic technologies used to specify build systems, they may not be able to identify when their source code changes require accompanying build system changes. This can cause build breakages that slow development progress and impact other developers, testers, or even users. In this paper, we mine the source and test code changes that required accompanying build changes in order to better understand this co-change relationship. We build random forest classifiers using language-agnostic and language-specific code change characteristics to explain when code-accompanying build changes are necessary based on historical trends. Case studies of the Mozilla C++ system, the Lucene and Eclipse open source Java systems, and the IBM Jazz proprietary Java system indicate that our classifiers can accurately explain when build co-changes are necessary with an AUC of 0.60-0.88. Unsurprisingly, our highly accurate C++ classifiers (AUC of 0.88) derive much of their explanatory power from indicators of structural change (e.g., was a new source file added?). On the other hand, our Java classifiers are less accurate (AUC of 0.60-0.78) because roughly 75% of Java build co-changes do not coincide with changes to the structure of a system, but rather are instigated by concerns related to release engineering, quality assurance, and general build maintenance. {\copyright} 2014 IEEE.",0,"Mining co-change information to understand when build changes are necessary. As a software project ages, its source code is modified to add new features, restructure existing ones, and fix defects. These source code changes often induce changes in the build system, i.e., the system that specifies how source code is translated into deliverables. However, since developers are often not familiar with the complex and occasionally archaic technologies used to specify build systems, they may not be able to identify when their source code changes require accompanying build system changes. This can cause build breakages that slow development progress and impact other developers, testers, or even users. In this paper, we mine the source and test code changes that required accompanying build changes in order to better understand this co-change relationship. We build random forest classifiers using language-agnostic and language-specific code change characteristics to explain when code-accompanying build changes are necessary based on historical trends. Case studies of the Mozilla C++ system, the Lucene and Eclipse open source Java systems, and the IBM Jazz proprietary Java system indicate that our classifiers can accurately explain when build co-changes are necessary with an AUC of 0.60-0.88. Unsurprisingly, our highly accurate C++ classifiers (AUC of 0.88) derive much of their explanatory power from indicators of structural change (e.g., was a new source file added?). On the other hand, our Java classifiers are less accurate (AUC of 0.60-0.78) because roughly 75% of Java build co-changes do not coincide with changes to the structure of a system, but rather are instigated by concerns related to release engineering, quality assurance, and general build maintenance. {\copyright} 2014 IEEE.",mining co change information understand build changes necessary software project ages source code modified add new features restructure existing ones fix defects source code changes often induce changes build system e system specifies source code translated deliverables however since developers often familiar complex occasionally archaic technologies used specify build systems may able identify source code changes require accompanying build system changes cause build breakages slow development progress impact developers testers even users paper mine source test code changes required accompanying build changes order better understand co change relationship build random forest classifiers using language agnostic language specific code change characteristics explain code accompanying build changes necessary based historical trends case studies mozilla c system lucene eclipse open source java systems ibm jazz proprietary java system indicate classifiers accurately explain build co changes necessary auc 0 60 0 88 unsurprisingly highly accurate c classifiers auc 0 88 derive much explanatory power indicators structural change e g new source file added hand java classifiers less accurate auc 0 60 0 78 roughly 75 java build co changes coincide changes structure system rather instigated concerns related release engineering quality assurance general build maintenance copyright 2014 ieee,1,1,0,1,3,2
106,Human-Computer Interaction,,"""Human-Computer Interaction: An Empirical Research Perspective"" is the definitive guide to empirical research in HCI. The book begins with foundational topics including historical context, the human factor, interaction elements, and the fundamentals of science and research. From there, youll progress to learning about the methods for conducting an experiment to evaluate a new computer interface or interaction technique. There are detailed discussions and how-to analyses on models of interaction, focusing on descriptive models and predictive models. Writing and publishing a research paper is explored with helpful tips for success. Throughout the book, youll find hands-on exercises, checklists, and real-world examples. This is your must-have, comprehensive guide to empirical and experimental research in HCI-an essential addition to your HCI library. Master empirical and experimental research with this comprehensive, A-to-Z guide in a concise, hands-on reference Discover the practical and theoretical ins-and-outs of user studies Find exercises, takeaway points, and case studies throughout. {\copyright} 2013 Elsevier Inc. All rights reserved.",0,"Human-Computer Interaction. ""Human-Computer Interaction: An Empirical Research Perspective"" is the definitive guide to empirical research in HCI. The book begins with foundational topics including historical context, the human factor, interaction elements, and the fundamentals of science and research. From there, youll progress to learning about the methods for conducting an experiment to evaluate a new computer interface or interaction technique. There are detailed discussions and how-to analyses on models of interaction, focusing on descriptive models and predictive models. Writing and publishing a research paper is explored with helpful tips for success. Throughout the book, youll find hands-on exercises, checklists, and real-world examples. This is your must-have, comprehensive guide to empirical and experimental research in HCI-an essential addition to your HCI library. Master empirical and experimental research with this comprehensive, A-to-Z guide in a concise, hands-on reference Discover the practical and theoretical ins-and-outs of user studies Find exercises, takeaway points, and case studies throughout. {\copyright} 2013 Elsevier Inc. All rights reserved.",human computer interaction human computer interaction empirical research perspective definitive guide empirical research hci book begins foundational topics including historical context human factor interaction elements fundamentals science research youll progress learning methods conducting experiment evaluate new computer interface interaction technique detailed discussions analyses models interaction focusing descriptive models predictive models writing publishing research paper explored helpful tips success throughout book youll find hands exercises checklists real world examples must comprehensive guide empirical experimental research hci essential addition hci library master empirical experimental research comprehensive z guide concise hands reference discover practical theoretical ins outs user studies find exercises takeaway points case studies throughout copyright 2013 elsevier inc rights reserved,2,0,0,2,2,3
107,Can Lexicon Bad Smells improve fault prediction?,Fault prediction; lexicon bad smells; program understanding; structural metrics,"In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases. {\copyright} 2012 IEEE.",0,"Can Lexicon Bad Smells improve fault prediction?. In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases. {\copyright} 2012 IEEE.",lexicon bad smells improve fault prediction software development early identification fault prone classes save considerable amount resources literature source code structural metrics widely investigated one factors used identify faulty classes structural metrics measure code complexity one aspect source code quality complexity might affect program understanding hence increase likelihood inserting errors class besides structural metrics believe quality identifiers used code may also affect program understanding thus increase likelihood error insertion study measure quality identifiers using number lexicon bad smells lbs contain investigate whether using lbs addition structural metrics improves fault prediction conduct investigation assess prediction capability model using structural metrics ii structural metrics lbs results three open source systems argouml rhino eclipse indicate improvement majority cases copyright 2012 ieee,1,2,2,1,3,2
108,Discovering bug patterns in Javascript,Bug Patterns; Data Mining; Javascript; Node.Js; Static Analysis,"JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. {\copyright} 2016 ACM.",0,"Discovering bug patterns in Javascript. JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. {\copyright} 2016 ACM.",discovering bug patterns javascript javascript become popular language used developers client server side programming language however still lacks proper support form warnings potential bugs code bug findings tools use today cover bug patterns discovered reading best practices developer intuition anecdotal observation still unclear bugs happen frequently practice important developers fixed propose novel semi automatic technique called bugaid discovering prevalent detectable bug patterns bugaid based unsupervised machine learning using languageconstruct based changes distilled ast differencing bug fixes code present large scale study common bug patterns mining 105k commits 134 server side javascript projects discover 219 bug fixing change types discuss 13 pervasive bug patterns occur across multiple projects likely prevented better tool support findings useful improving tools techniques prevent common bugs javascript guiding tool integration ides making developers aware common mistakes involved programming javascript copyright 2016 acm,0,2,1,1,3,2
109,Evolving estimators of the pointwise Holder exponent with Genetic Programming,Genetic Programming; Holder regularity; Local image description,"The regularity of a signal can be numerically expressed using Holder exponents, which characterize the singular structures a signal contains. In particular, within the domains of image processing and image understanding, regularity-based analysis can be used to describe local image shape and appearance. However, estimating the Holder exponent is not a trivial task, and current methods tend to be computationally slow and complex. This work presents an approach to automatically synthesize estimators of the pointwise Holder exponent for digital images. This task is formulated as an optimization problem and Genetic Programming (GP) is used to search for operators that can approximate a traditional estimator, the oscillations method. Experimental results show that GP can generate estimators that achieve a low error and a high correlation with the ground truth estimation. Furthermore, most of the GP estimators are faster than traditional approaches, in some cases their runtime is orders of magnitude smaller. This result allowed us to implement a real-time estimation of the Holder exponent on a live video signal, the first such implementation in current literature. Moreover, the evolved estimators are used to generate local descriptors of salient image regions, a task for which a stable and robust matching is achieved, comparable with state-of-the-art methods. In conclusion, the evolved estimators produced by GP could help expand the application domain of Holder regularity within the fields of image analysis and signal processing. {\copyright} 2012 Elsevier Inc. All rights reserved.",0,"Evolving estimators of the pointwise Holder exponent with Genetic Programming. The regularity of a signal can be numerically expressed using Holder exponents, which characterize the singular structures a signal contains. In particular, within the domains of image processing and image understanding, regularity-based analysis can be used to describe local image shape and appearance. However, estimating the Holder exponent is not a trivial task, and current methods tend to be computationally slow and complex. This work presents an approach to automatically synthesize estimators of the pointwise Holder exponent for digital images. This task is formulated as an optimization problem and Genetic Programming (GP) is used to search for operators that can approximate a traditional estimator, the oscillations method. Experimental results show that GP can generate estimators that achieve a low error and a high correlation with the ground truth estimation. Furthermore, most of the GP estimators are faster than traditional approaches, in some cases their runtime is orders of magnitude smaller. This result allowed us to implement a real-time estimation of the Holder exponent on a live video signal, the first such implementation in current literature. Moreover, the evolved estimators are used to generate local descriptors of salient image regions, a task for which a stable and robust matching is achieved, comparable with state-of-the-art methods. In conclusion, the evolved estimators produced by GP could help expand the application domain of Holder regularity within the fields of image analysis and signal processing. {\copyright} 2012 Elsevier Inc. All rights reserved.",evolving estimators pointwise holder exponent genetic programming regularity signal numerically expressed using holder exponents characterize singular structures signal contains particular within domains image processing image understanding regularity based analysis used describe local image shape appearance however estimating holder exponent trivial task current methods tend computationally slow complex work presents approach automatically synthesize estimators pointwise holder exponent digital images task formulated optimization problem genetic programming gp used search operators approximate traditional estimator oscillations method experimental results show gp generate estimators achieve low error high correlation ground truth estimation furthermore gp estimators faster traditional approaches cases runtime orders magnitude smaller result allowed us implement real time estimation holder exponent live video signal first implementation current literature moreover evolved estimators used generate local descriptors salient image regions task stable robust matching achieved comparable state art methods conclusion evolved estimators produced gp could help expand application domain holder regularity within fields image analysis signal processing copyright 2012 elsevier inc rights reserved,1,1,1,1,0,3
110,Smells like teen spirit: Improving bug prediction performance using the intensity of code smells,,"Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models and comparing the results of the new model against the baseline model. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also evaluate the actual gain provided by the intensity index with respect to the other metrics in the model, including the ones used to compute the code smell intensity. We observe that the intensity index is much more important as compared to other metrics used for predicting the buggyness of smelly classes. {\copyright} 2016 IEEE.",0,"Smells like teen spirit: Improving bug prediction performance using the intensity of code smells. Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models and comparing the results of the new model against the baseline model. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also evaluate the actual gain provided by the intensity index with respect to the other metrics in the model, including the ones used to compute the code smell intensity. We observe that the intensity index is much more important as compared to other metrics used for predicting the buggyness of smelly classes. {\copyright} 2016 IEEE.",smells like teen spirit improving bug prediction performance using intensity code smells code smells symptoms poor design implementation choices previous studies empirically assessed impact smells code quality clearly indicate negative impact maintainability including higher bug proneness components affected code smells paper capture previous findings bug proneness build specialized bug prediction model smelly classes specifically evaluate contribution measure severity code smells e code smell intensity adding existing bug prediction models comparing results new model baseline model results indicate accuracy bug prediction model increases adding code smell intensity predictor also evaluate actual gain provided intensity index respect metrics model including ones used compute code smell intensity observe intensity index much important compared metrics used predicting buggyness smelly classes copyright 2016 ieee,0,2,0,0,3,1
111,An empirical framework for defect prediction using machine learning techniques with Android software,Inter-release validation; Machine-learning; Object-oriented metrics; Software defect proneness; Statistical tests,"Context Software defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. Objective The main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. Method In this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. Results The results show that the area under the curve measure for Na{""\i}ve Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. Conclusion The results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software. {\copyright} 2016 Elsevier B.V.",0,"An empirical framework for defect prediction using machine learning techniques with Android software. Context Software defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. Objective The main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. Method In this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. Results The results show that the area under the curve measure for Na{""\i}ve Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. Conclusion The results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software. {\copyright} 2016 Elsevier B.V.",empirical framework defect prediction using machine learning techniques android software context software defect prediction important identification defect prone parts software defect prediction models developed using software metrics combination defect data predicting defective classes various studies conducted find relationship software metrics defect proneness studies statistically determine effectiveness results objective main objectives study comparison machine learning techniques using data sets obtained popular open source software ii use appropriate performance measures measuring performance defect prediction models iii use statistical tests effective comparison machine learning techniques iv validation models different releases data sets method study use object oriented metrics predicting defective classes using 18 machine learning techniques proposed framework applied seven application packages well known widely used android operating system viz contact mms bluetooth email calendar gallery2 telephony results validated using 10 fold inter release validation methods reliability significance results evaluated using statistical test post hoc analysis results results show area curve measure na bayes logitboost multilayer perceptron 0 7 cases results also depict difference ml techniques statistically significant however also proved support vector machines based techniques support vector machines voted perceptron possess predictive capability predicting defects conclusion results confirm predictive capability various ml techniques developing defect prediction models results also confirm superiority one ml technique ml techniques thus software engineers use results obtained study early phases software development identifying defect prone classes given software copyright 2016 elsevier b v,1,1,2,1,0,0
112,Code ownership and software quality: A replication study,Code ownership; Empirical software engineering; Software quality,"In a traditional sense, ownership determines rights and duties in regard to an object, for example a property. The owner of source code usually refers to the person that invented the code. However, larger code artifacts, such as files, are usually composed by multiple engineers contributing to the entity over time through a series of changes. Frequently, the person with the highest contribution, e.g. The most number of code changes, is defined as the code owner and takes responsibility for it. Thus, code ownership relates to the knowledge engineers have about code. Lacking responsibility and knowledge about code can reduce code quality. In an earlier study, Bird et al. [1] showed that Windows binaries that lacked clear code ownership were more likely to be defect prone. However recommendations for large artifacts such as binaries are usually not actionable. E.g. Changing the concept of binaries and refactoring them to ensure strong ownership would violate system architecture principles. A recent replication study by Foucault et al. [2] on open source software replicate the original results and lead to doubts about the general concept of ownership impacting code quality. In this paper, we replicated and extended the previous two ownership studies [1, 2] and reflect on their findings. Further, we define several new ownership metrics to investigate the dependency between ownership and code quality on file and directory level for 4 major Microsoft products. The results confirm the original findings by Bird et al. [1] that code ownership correlates with code quality. Using new and refined code ownership metrics we were able to classify source files that contained at least one bug with a median precision of 0.74 and a median recall of 0.38. On directory level, we achieve a precision of 0.76 and a recall of 0.60. {\copyright} 2015 IEEE.",0,"Code ownership and software quality: A replication study. In a traditional sense, ownership determines rights and duties in regard to an object, for example a property. The owner of source code usually refers to the person that invented the code. However, larger code artifacts, such as files, are usually composed by multiple engineers contributing to the entity over time through a series of changes. Frequently, the person with the highest contribution, e.g. The most number of code changes, is defined as the code owner and takes responsibility for it. Thus, code ownership relates to the knowledge engineers have about code. Lacking responsibility and knowledge about code can reduce code quality. In an earlier study, Bird et al. [1] showed that Windows binaries that lacked clear code ownership were more likely to be defect prone. However recommendations for large artifacts such as binaries are usually not actionable. E.g. Changing the concept of binaries and refactoring them to ensure strong ownership would violate system architecture principles. A recent replication study by Foucault et al. [2] on open source software replicate the original results and lead to doubts about the general concept of ownership impacting code quality. In this paper, we replicated and extended the previous two ownership studies [1, 2] and reflect on their findings. Further, we define several new ownership metrics to investigate the dependency between ownership and code quality on file and directory level for 4 major Microsoft products. The results confirm the original findings by Bird et al. [1] that code ownership correlates with code quality. Using new and refined code ownership metrics we were able to classify source files that contained at least one bug with a median precision of 0.74 and a median recall of 0.38. On directory level, we achieve a precision of 0.76 and a recall of 0.60. {\copyright} 2015 IEEE.",code ownership software quality replication study traditional sense ownership determines rights duties regard object example property owner source code usually refers person invented code however larger code artifacts files usually composed multiple engineers contributing entity time series changes frequently person highest contribution e g number code changes defined code owner takes responsibility thus code ownership relates knowledge engineers code lacking responsibility knowledge code reduce code quality earlier study bird et al 1 showed windows binaries lacked clear code ownership likely defect prone however recommendations large artifacts binaries usually actionable e g changing concept binaries refactoring ensure strong ownership would violate system architecture principles recent replication study foucault et al 2 open source software replicate original results lead doubts general concept ownership impacting code quality paper replicated extended previous two ownership studies 1 2 reflect findings define several new ownership metrics investigate dependency ownership code quality file directory level 4 major microsoft products results confirm original findings bird et al 1 code ownership correlates code quality using new refined code ownership metrics able classify source files contained least one bug median precision 0 74 median recall 0 38 directory level achieve precision 0 76 recall 0 60 copyright 2015 ieee,1,1,0,1,3,2
113,Bad-smell prediction from software design model using machine learning techniques,Bad-smell; Design Diagram Metrics; Machine Learners; Prediction models; Random Forest; Software Design Model,Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values. {\copyright} 2011 IEEE.,1,Bad-smell prediction from software design model using machine learning techniques. Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values. {\copyright} 2011 IEEE.,bad smell prediction software design model using machine learning techniques bad smell prediction significantly impacts software quality beneficial bad smell prediction performed early possible development life cycle present methodology predicting bad smells software design model collect 7 data sets previous literatures offer 27 design model metrics 7 bad smells learnt tested predict bad smells using seven machine learning algorithms use cross validation assessing performance preventing fitting statistical significance tests used evaluate compare prediction performance conclude methodology proximity actual values copyright 2011 ieee,0,2,2,0,0,0
114,Progress on approaches to software defect prediction,,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction. {\copyright} The Institution of Engineering and Technology 2018.",0,"Progress on approaches to software defect prediction. Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction. {\copyright} The Institution of Engineering and Technology 2018.",progress approaches software defect prediction software defect prediction one popular research topics software engineering aims predict defect prone software modules defects discovered therefore used better prioritise software quality assurance effort recent years especially recent 3 years many new defect prediction studies proposed goal study comprehensively review analyse discuss state art defect prediction authors survey almost 70 representative defect prediction papers recent years january 2014 april 2017 published prominent software engineering journals top conferences selected defect prediction papers summarised four aspects machine learning based prediction algorithms manipulating data effort aware prediction empirical studies research community still facing number challenges building methods many research opportunities exist identified challenges give practical guidelines software engineering researchers practitioners future software defect prediction copyright institution engineering technology 2018,1,2,2,1,0,0
115,The impact of tangled code changes on defect prediction models,Data noise; Defect prediction; Untangling,"When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets---in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes. {\copyright} 2015, Springer Science+Business Media New York.",0,"The impact of tangled code changes on defect prediction models. When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets---in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes. {\copyright} 2015, Springer Science+Business Media New York.",impact tangled code changes defect prediction models interacting source control management system developers often commit unrelated loosely related code changes single transaction analyzing version histories tangled changes make changes modules appear related possibly compromising resulting analyses noise bias investigation five open source java projects found 7 20 bug fixes consist multiple tangled changes using multi predictor approach untangle changes show average least 16 6 source files incorrectly associated bug reports incorrect bug file associations seem significantly impact models classifying source files least one bug bugs experiments show untangling tangled code changes result accurate regression bug prediction models compared models trained tested tangled bug datasets experiments statistically significant accuracy improvements lies 5 200 recommend better change organization limit impact tangled changes copyright 2015 springer science business media new york,1,2,0,1,3,2
116,Combining Software Metrics and Text Features for Vulnerable File Prediction,Machine Learning; Text Mining; Vulnerable File,"In recent years, to help developers reduce time and effort required to build highly secure software, a number of prediction models which are built on different kinds of features have been proposed to identify vulnerable source code files. In this paper, we propose a novel approach VULPREDICTOR to predict vulnerable files, it analyzes software metrics and text mining together to build a composite prediction model. VULPREDICTOR first builds 6 underlying classifiers on a training set of vulnerable and non-vulnerable files represented by their software metrics and text features, and then constructs a meta classifier to process the outputs of the 6 underlying classifiers. We evaluate our solution on datasets from three web applications including Drupal, PHPMyAdmin and Moodle which contain a total of 3,466 files and 223 vulnerabilities. The experiment results show that VULPREDICTOR can achieve F1 and EffectivenessRatio@20% scores of up to 0.683 and 75%, respectively. On average across the 3 projects, VULPREDICTOR improves the F1 and EffectivenessRatio@20% scores of the best performing state-of-the-art approaches proposed by Walden et al. by 46.53% and 14.93%, respectively. {\copyright} 2015 IEEE.",0,"Combining Software Metrics and Text Features for Vulnerable File Prediction. In recent years, to help developers reduce time and effort required to build highly secure software, a number of prediction models which are built on different kinds of features have been proposed to identify vulnerable source code files. In this paper, we propose a novel approach VULPREDICTOR to predict vulnerable files, it analyzes software metrics and text mining together to build a composite prediction model. VULPREDICTOR first builds 6 underlying classifiers on a training set of vulnerable and non-vulnerable files represented by their software metrics and text features, and then constructs a meta classifier to process the outputs of the 6 underlying classifiers. We evaluate our solution on datasets from three web applications including Drupal, PHPMyAdmin and Moodle which contain a total of 3,466 files and 223 vulnerabilities. The experiment results show that VULPREDICTOR can achieve F1 and EffectivenessRatio@20% scores of up to 0.683 and 75%, respectively. On average across the 3 projects, VULPREDICTOR improves the F1 and EffectivenessRatio@20% scores of the best performing state-of-the-art approaches proposed by Walden et al. by 46.53% and 14.93%, respectively. {\copyright} 2015 IEEE.",combining software metrics text features vulnerable file prediction recent years help developers reduce time effort required build highly secure software number prediction models built different kinds features proposed identify vulnerable source code files paper propose novel approach vulpredictor predict vulnerable files analyzes software metrics text mining together build composite prediction model vulpredictor first builds 6 underlying classifiers training set vulnerable non vulnerable files represented software metrics text features constructs meta classifier process outputs 6 underlying classifiers evaluate solution datasets three web applications including drupal phpmyadmin moodle contain total 3 466 files 223 vulnerabilities experiment results show vulpredictor achieve f1 effectivenessratio 20 scores 0 683 75 respectively average across 3 projects vulpredictor improves f1 effectivenessratio 20 scores best performing state art approaches proposed walden et al 46 53 14 93 respectively copyright 2015 ieee,1,1,0,1,0,2
117,Quality: Its definition and measurement as applied to the Medically ill,,"Quality, as exemplified by Quality-of-life (QoL) assessment, is frequently discussed among health care professionals and often invoked as a goal for improvement, but somehow rarely defined, even as it is regularly assessed. It is understood that some medical patients have a better QoL than others, but should the QoL achieved be compared to an ideal state, or is it too personal and subjective to gauge? Can a better understanding of the concept help health care systems deliver services more effectively? Is QoL worth measuring at all? Integrating concepts from psychology, philosophy, neurocognition, and linguistics, this book attempts to answer these complex questions. It also breaks down the cognitive-linguistic components that comprise the judgment of quality, including description, evaluation, and valuations, and applies them to issues specific to individuals with chronic medical illness. In this context, quality/QoL assessment becomes an essential contributor to ethical practice, a critical step towards improving the nature of social interactions. The author considers linear, non-linear, and complexity-based models in analyzing key methodology and content issues in health-related QoL assessment.This book is certain to stimulate debate in the research and scientific communities. Its forward-looking perspective takes great strides toward promoting a common cognitive-linguistic model of how the judgment of quality occurs, thereby contributing important conceptual and empirical tools to its varied applications, including QoL assessment. {\copyright} Springer Science+Business Media, LLC 2012. All rights reserved.",0,"Quality: Its definition and measurement as applied to the Medically ill. Quality, as exemplified by Quality-of-life (QoL) assessment, is frequently discussed among health care professionals and often invoked as a goal for improvement, but somehow rarely defined, even as it is regularly assessed. It is understood that some medical patients have a better QoL than others, but should the QoL achieved be compared to an ideal state, or is it too personal and subjective to gauge? Can a better understanding of the concept help health care systems deliver services more effectively? Is QoL worth measuring at all? Integrating concepts from psychology, philosophy, neurocognition, and linguistics, this book attempts to answer these complex questions. It also breaks down the cognitive-linguistic components that comprise the judgment of quality, including description, evaluation, and valuations, and applies them to issues specific to individuals with chronic medical illness. In this context, quality/QoL assessment becomes an essential contributor to ethical practice, a critical step towards improving the nature of social interactions. The author considers linear, non-linear, and complexity-based models in analyzing key methodology and content issues in health-related QoL assessment.This book is certain to stimulate debate in the research and scientific communities. Its forward-looking perspective takes great strides toward promoting a common cognitive-linguistic model of how the judgment of quality occurs, thereby contributing important conceptual and empirical tools to its varied applications, including QoL assessment. {\copyright} Springer Science+Business Media, LLC 2012. All rights reserved.",quality definition measurement applied medically ill quality exemplified quality life qol assessment frequently discussed among health care professionals often invoked goal improvement somehow rarely defined even regularly assessed understood medical patients better qol others qol achieved compared ideal state personal subjective gauge better understanding concept help health care systems deliver services effectively qol worth measuring integrating concepts psychology philosophy neurocognition linguistics book attempts answer complex questions also breaks cognitive linguistic components comprise judgment quality including description evaluation valuations applies issues specific individuals chronic medical illness context quality qol assessment becomes essential contributor ethical practice critical step towards improving nature social interactions author considers linear non linear complexity based models analyzing key methodology content issues health related qol assessment book certain stimulate debate research scientific communities forward looking perspective takes great strides toward promoting common cognitive linguistic model judgment quality occurs thereby contributing important conceptual empirical tools varied applications including qol assessment copyright springer science business media llc 2012 rights reserved,2,0,1,2,1,3
118,Measuring code quality to improve specification mining,code metrics; machine learning; program understanding; software engineering; Specification mining,"Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code. {\copyright} 2006 IEEE.",0,"Measuring code quality to improve specification mining. Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code. {\copyright} 2006 IEEE.",measuring code quality improve specification mining formal specifications help program testing optimization refactoring documentation importantly debugging repair however difficult write manually automatic mining techniques suffer 90 99 percent false positive rates address problem propose augment temporal property miner incorporating code quality metrics measure code quality extracting additional information software engineering process using information code likely correct well code less likely correct used preprocessing step existing specification miner technique identifies input indicative correct program behavior allows shelf techniques learn number specifications using 45 percent original input novel inference technique approach false positives practice 63 percent balancing precision recall 3 percent focused precision still finding useful specifications e g find many bugs 1 5 million lines code copyright 2006 ieee,0,2,0,1,3,2
119,Enhancing change prediction models using developer-related factors,Change prediction; Empirical study; Mining software repositories,"Continuous changes applied during software maintenance risk to deteriorate the structure of a system and are a threat to its maintainability. In this context, predicting the portions of source code where specific maintenance operations should be focused on may be crucial for developers to prevent maintainability issues. Previous work proposed change prediction models relying on product and process metrics as predictors of change-prone source code classes. However, we believe that existing approaches still miss an important piece of information, i.e., developer-related factors that are able to capture the complexity of the development process under different perspectives. In this paper, we firstly investigate three change prediction models that exploit developer-related factors (e.g., number of developers working on a class) as predictors of change-proneness of classes and then we compare them with existing models. Our findings reveal that these factors improve the capabilities of change prediction models. Moreover, we observed interesting complementarities among the prediction models. For this reason, we devised a novel change prediction model exploiting the combination of developer-related factors and product and evolution metrics. The results show that such a combined model is up to 22% more effective than the single models in the identification of change-prone classes. {\copyright} 2018 Elsevier Inc.",0,"Enhancing change prediction models using developer-related factors. Continuous changes applied during software maintenance risk to deteriorate the structure of a system and are a threat to its maintainability. In this context, predicting the portions of source code where specific maintenance operations should be focused on may be crucial for developers to prevent maintainability issues. Previous work proposed change prediction models relying on product and process metrics as predictors of change-prone source code classes. However, we believe that existing approaches still miss an important piece of information, i.e., developer-related factors that are able to capture the complexity of the development process under different perspectives. In this paper, we firstly investigate three change prediction models that exploit developer-related factors (e.g., number of developers working on a class) as predictors of change-proneness of classes and then we compare them with existing models. Our findings reveal that these factors improve the capabilities of change prediction models. Moreover, we observed interesting complementarities among the prediction models. For this reason, we devised a novel change prediction model exploiting the combination of developer-related factors and product and evolution metrics. The results show that such a combined model is up to 22% more effective than the single models in the identification of change-prone classes. {\copyright} 2018 Elsevier Inc.",enhancing change prediction models using developer related factors continuous changes applied software maintenance risk deteriorate structure system threat maintainability context predicting portions source code specific maintenance operations focused may crucial developers prevent maintainability issues previous work proposed change prediction models relying product process metrics predictors change prone source code classes however believe existing approaches still miss important piece information e developer related factors able capture complexity development process different perspectives paper firstly investigate three change prediction models exploit developer related factors e g number developers working class predictors change proneness classes compare existing models findings reveal factors improve capabilities change prediction models moreover observed interesting complementarities among prediction models reason devised novel change prediction model exploiting combination developer related factors product evolution metrics results show combined model 22 effective single models identification change prone classes copyright 2018 elsevier inc,1,2,2,1,0,0
120,Mutation-aware fault prediction,Empirical study; Mutation testing; Software defect prediction; Software fault prediction; Software metrics,"We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 Different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¤ 0:05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments. {\copyright} 2016 ACM.",0,"Mutation-aware fault prediction. We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 Different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¤ 0:05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments. {\copyright} 2016 ACM.",mutation aware fault prediction introduce mutation aware fault prediction leverages additional guidance metrics constructed terms mutants test cases cover detect report results 12 sets experiments applying 4 different predictive modelling techniques 3 large real world systems open closed source results show proposal significantly p 0 05 improve fault prediction performance moreover mutation based metrics lie top 5 frequently relied upon fault predictors 10 12 sets experiments provide majority top ten fault predictors 9 12 sets experiments copyright 2016 acm,1,2,2,1,0,0
121,Detecting API documentation errors,API documentation error; Outdated documentation,"When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DocRef, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DocRef and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DocRef has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them. Copyright {\copyright} 2013. Copyright {\copyright} 2013 ACM.",0,"Detecting API documentation errors. When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DocRef, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DocRef and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DocRef has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them. Copyright {\copyright} 2013. Copyright {\copyright} 2013 ACM.",detecting api documentation errors programmers encounter unfamiliar api library often need refer documentations tutorials discussions development forums learn proper usage api documents contain valuable information may also mislead programmers may contain errors e g broken code names obsolete code samples although api documents actively maintained updated studies show many new latent errors exist tedious error prone find errors manually api documents enormous thousands pages existing tools ineffective locating documentation errors traditional natural language nl tools understand code names code samples traditional code analysis tools understand nl sentences paper propose first approach docref specifically designed developed detect api documentation errors formulate class inconsistencies indicate potential documentation errors combine nl code analysis techniques detect report inconsistencies implemented docref evaluated effectiveness latest documentations five widely used api libraries docref detected 1 000 new documentation errors reported authors many errors already confirmed fixed reported copyright copyright 2013 copyright copyright 2013 acm,0,2,0,1,3,2
122,Fault-prone module detection using large-scale text features based on spam filtering,Fault-prone module; Large-scale; Software repository; Spam filtering; Text feature; Text mining,"This paper proposes an approach using large-scale text features for faultprone module detection inspired by spam filtering. The number of every text feature in the source code of a module is counted and used as data for training detection models. In this paper, we prepared a naive Bayes classifier and a logistic regression model as detection models. To show the effectiveness of our approaches, we conducted experiments with five open source projects and compared them with a well-known metrics set, thereby achieving higher detection results. The results imply that large-scale text features are useful in constructing practical detection models, and measuring sophisticated metrics is not always necessary for detecting fault-prone modules. {\copyright} Springer Science + Business Media, LLC 2009.",0,"Fault-prone module detection using large-scale text features based on spam filtering. This paper proposes an approach using large-scale text features for faultprone module detection inspired by spam filtering. The number of every text feature in the source code of a module is counted and used as data for training detection models. In this paper, we prepared a naive Bayes classifier and a logistic regression model as detection models. To show the effectiveness of our approaches, we conducted experiments with five open source projects and compared them with a well-known metrics set, thereby achieving higher detection results. The results imply that large-scale text features are useful in constructing practical detection models, and measuring sophisticated metrics is not always necessary for detecting fault-prone modules. {\copyright} Springer Science + Business Media, LLC 2009.",fault prone module detection using large scale text features based spam filtering paper proposes approach using large scale text features faultprone module detection inspired spam filtering number every text feature source code module counted used data training detection models paper prepared naive bayes classifier logistic regression model detection models show effectiveness approaches conducted experiments five open source projects compared well known metrics set thereby achieving higher detection results results imply large scale text features useful constructing practical detection models measuring sophisticated metrics always necessary detecting fault prone modules copyright springer science business media llc 2009,0,2,2,0,0,0
123,Teaching software engineering principles to K-12 students: A MOOC on scratch,code smells; dropout prediction; MOOC; Programming education; Scratch,"In the last few years, many books, online puzzles, apps and games have been created to teach young children programming. However, most of these do not introduce children to broader concepts from software engineering, such as debugging and code quality issues like smells, duplication, refactoring and naming. To address this, we designed and ran an online introductory Scratch programming course in which we teach elementary programming concepts and software engineering concepts simultaneously. In total 2,220 children actively participated in our course in June and July of 2016, most of which (73%) between the ages of 7 and 11. In this paper we describe our course design and analyze the resulting data. More specifically, we investigate whether 1) students find programming concepts more difficult than software engineering concepts, 2) there are age-related differences in their performance, and 3) we can predict successful course completion. Our results show that there is no difference in students' scores between the programming concepts and the software engineering concepts, suggesting that it is indeed possible to teach these concepts to this age group. We also find that students over 12 years of age perform significantly better in questions related to operators and procedures. Finally, we identify the factors from the students' profile and their behaviour in the first week of the course that can be used to predict its successful completion. {\copyright} 2017 IEEE.",0,"Teaching software engineering principles to K-12 students: A MOOC on scratch. In the last few years, many books, online puzzles, apps and games have been created to teach young children programming. However, most of these do not introduce children to broader concepts from software engineering, such as debugging and code quality issues like smells, duplication, refactoring and naming. To address this, we designed and ran an online introductory Scratch programming course in which we teach elementary programming concepts and software engineering concepts simultaneously. In total 2,220 children actively participated in our course in June and July of 2016, most of which (73%) between the ages of 7 and 11. In this paper we describe our course design and analyze the resulting data. More specifically, we investigate whether 1) students find programming concepts more difficult than software engineering concepts, 2) there are age-related differences in their performance, and 3) we can predict successful course completion. Our results show that there is no difference in students' scores between the programming concepts and the software engineering concepts, suggesting that it is indeed possible to teach these concepts to this age group. We also find that students over 12 years of age perform significantly better in questions related to operators and procedures. Finally, we identify the factors from the students' profile and their behaviour in the first week of the course that can be used to predict its successful completion. {\copyright} 2017 IEEE.",teaching software engineering principles k 12 students mooc scratch last years many books online puzzles apps games created teach young children programming however introduce children broader concepts software engineering debugging code quality issues like smells duplication refactoring naming address designed ran online introductory scratch programming course teach elementary programming concepts software engineering concepts simultaneously total 2 220 children actively participated course june july 2016 73 ages 7 11 paper describe course design analyze resulting data specifically investigate whether 1 students find programming concepts difficult software engineering concepts 2 age related differences performance 3 predict successful course completion results show difference students scores programming concepts software engineering concepts suggesting indeed possible teach concepts age group also find students 12 years age perform significantly better questions related operators procedures finally identify factors students profile behaviour first week course used predict successful completion copyright 2017 ieee,2,2,1,2,2,3
124,Multi-objective code-smells detection using good and bad design examples,Search-based software engineering; Software maintenance; Software metrics,"Code-smells are identified, in general, by using a set of detection rules. These rules are manually defined to identify the key symptoms that characterize a code-smell using combinations of mainly quantitative (metrics), structural, and/or lexical information. We propose in this work to consider the problem of code-smell detection as a multi-objective problem where examples of code-smells and well-designed code are used to generate detection rules. To this end, we use multi-objective genetic programming (MOGP) to find the best combination of metrics that maximizes the detection of code-smell examples and minimizes the detection of well-designed code examples. We evaluated our proposal on seven large open-source systems and found that, on average, most of the different five code-smell types were detected with an average of 87 % of precision and 92 % of recall. Statistical analysis of our experiments over 51 runs shows that MOGP performed significantly better than state-of-the-art code-smell detectors. {\copyright} 2016, Springer Science+Business Media New York.",0,"Multi-objective code-smells detection using good and bad design examples. Code-smells are identified, in general, by using a set of detection rules. These rules are manually defined to identify the key symptoms that characterize a code-smell using combinations of mainly quantitative (metrics), structural, and/or lexical information. We propose in this work to consider the problem of code-smell detection as a multi-objective problem where examples of code-smells and well-designed code are used to generate detection rules. To this end, we use multi-objective genetic programming (MOGP) to find the best combination of metrics that maximizes the detection of code-smell examples and minimizes the detection of well-designed code examples. We evaluated our proposal on seven large open-source systems and found that, on average, most of the different five code-smell types were detected with an average of 87 % of precision and 92 % of recall. Statistical analysis of our experiments over 51 runs shows that MOGP performed significantly better than state-of-the-art code-smell detectors. {\copyright} 2016, Springer Science+Business Media New York.",multi objective code smells detection using good bad design examples code smells identified general using set detection rules rules manually defined identify key symptoms characterize code smell using combinations mainly quantitative metrics structural lexical information propose work consider problem code smell detection multi objective problem examples code smells well designed code used generate detection rules end use multi objective genetic programming mogp find best combination metrics maximizes detection code smell examples minimizes detection well designed code examples evaluated proposal seven large open source systems found average different five code smell types detected average 87 precision 92 recall statistical analysis experiments 51 runs shows mogp performed significantly better state art code smell detectors copyright 2016 springer science business media new york,0,1,2,0,3,1
125,Studying the effectiveness of Application Performance Management (APM) tools for detecting performance regressions for web applications: An experience report,,"Performance regressions, such as a higher CPU utilization than in the previous version of an application, are caused by software application updates that negatively affect the performance of an application. Although a plethora of mining software repository research has been done to detect such regressions, research tools are generally not readily available to practitioners. Application Performance Management (APM) tools are commonly used in practice for detecting performance issues in the field by mining operational data. In contrast to performance regression detection tools that assume a changing code base and a stable workload, APM tools mine operational data to detect performance anomalies caused by a changing workload in an otherwise stable code base. Although APM tools are widely used in practice, no research has been done to understand 1) whether APM tools can identify performance regressions caused by code changes and 2) how well these APM tools support diagnosing the root-cause of these regressions. In this paper, we explore if the readily accessible APM tools can help practitioners detect performance regressions. We perform a case study using three commercial (AppDynamics, New Relic and Dynatrace) and one open source (Pinpoint) APM tools. In particular, we examine the effectiveness of leveraging these APM tools in detecting and diagnosing injected performance regressions (excessive memory usage, high CPU utilization and inefficient database queries) in three open source applications. We find that APM tools can detect most of the injected performance regressions, making them good candidates to detect performance regressions in practice. However, there is a gap between mining approaches that are proposed in state-of-theart performance regression detection research and the ones used by APM tools. In addition, APM tools lack the ability to be extended, which makes it hard to enhance them when exploring novel mining approaches for detecting performance regressions. {\copyright} 2016 ACM.",0,"Studying the effectiveness of Application Performance Management (APM) tools for detecting performance regressions for web applications: An experience report. Performance regressions, such as a higher CPU utilization than in the previous version of an application, are caused by software application updates that negatively affect the performance of an application. Although a plethora of mining software repository research has been done to detect such regressions, research tools are generally not readily available to practitioners. Application Performance Management (APM) tools are commonly used in practice for detecting performance issues in the field by mining operational data. In contrast to performance regression detection tools that assume a changing code base and a stable workload, APM tools mine operational data to detect performance anomalies caused by a changing workload in an otherwise stable code base. Although APM tools are widely used in practice, no research has been done to understand 1) whether APM tools can identify performance regressions caused by code changes and 2) how well these APM tools support diagnosing the root-cause of these regressions. In this paper, we explore if the readily accessible APM tools can help practitioners detect performance regressions. We perform a case study using three commercial (AppDynamics, New Relic and Dynatrace) and one open source (Pinpoint) APM tools. In particular, we examine the effectiveness of leveraging these APM tools in detecting and diagnosing injected performance regressions (excessive memory usage, high CPU utilization and inefficient database queries) in three open source applications. We find that APM tools can detect most of the injected performance regressions, making them good candidates to detect performance regressions in practice. However, there is a gap between mining approaches that are proposed in state-of-theart performance regression detection research and the ones used by APM tools. In addition, APM tools lack the ability to be extended, which makes it hard to enhance them when exploring novel mining approaches for detecting performance regressions. {\copyright} 2016 ACM.",studying effectiveness application performance management apm tools detecting performance regressions web applications experience report performance regressions higher cpu utilization previous version application caused software application updates negatively affect performance application although plethora mining software repository research done detect regressions research tools generally readily available practitioners application performance management apm tools commonly used practice detecting performance issues field mining operational data contrast performance regression detection tools assume changing code base stable workload apm tools mine operational data detect performance anomalies caused changing workload otherwise stable code base although apm tools widely used practice research done understand 1 whether apm tools identify performance regressions caused code changes 2 well apm tools support diagnosing root cause regressions paper explore readily accessible apm tools help practitioners detect performance regressions perform case study using three commercial appdynamics new relic dynatrace one open source pinpoint apm tools particular examine effectiveness leveraging apm tools detecting diagnosing injected performance regressions excessive memory usage high cpu utilization inefficient database queries three open source applications find apm tools detect injected performance regressions making good candidates detect performance regressions practice however gap mining approaches proposed state theart performance regression detection research ones used apm tools addition apm tools lack ability extended makes hard enhance exploring novel mining approaches detecting performance regressions copyright 2016 acm,1,1,0,1,0,2
126,Data stream mining for predicting software build outcomes using source code metrics,Concept drift detection; Data stream mining; Hoeffding tree; Jazz; Software metrics; Software repositories,"Context Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process. Objective This paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure. Method This research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool. Results The results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches. Conclusion Overall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure. {\copyright} 2013 Elsevier B.V. All rights reserved.",0,"Data stream mining for predicting software build outcomes using source code metrics. Context Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process. Objective This paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure. Method This research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool. Results The results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches. Conclusion Overall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure. {\copyright} 2013 Elsevier B.V. All rights reserved.",data stream mining predicting software build outcomes using source code metrics context software development projects involve use wide range tools produce software artifact software repositories source control systems become focus emergent research source rich information regarding software development projects mining repositories becoming increasingly common view gaining deeper understanding development process objective paper explores concepts representing software development project process results creation data stream also describes extraction metrics jazz repository application data stream mining techniques identify useful metrics predicting build success failure method research systematic study using hoeffding tree classification method used conjunction adaptive sliding window adwin method detecting concept drift applying massive online analysis moa tool results results indicate relatively small number available measures considered significance predicting outcome build time significant measures identified implication results discussed particularly relative difficulty able predict failed builds hoeffding tree approach shown produce stable robust model traditional data mining approaches conclusion overall prediction accuracies 75 achieved use hoeffding tree classification method despite high overall accuracy greater difficulty predicting failure success emergence stable classification tree limited lack data overall approach shows promise terms informing software development activities order minimize chance failure copyright 2013 elsevier b v rights reserved,1,2,0,1,0,2
127,A fuzzy classifier approach to estimating software quality,Computational intelligence; Fuzzy logic; Pattern classification; Software engineering; Software metric,"With the increasing sophistication of today's software systems, it is often difficult to estimate the overall quality of underlying software components with respect to attributes such as complexity, utility, and extensibility. Many metrics exist in the software engineering literature that attempt to quantify, with varying levels of accuracy, a large swath of qualitative attributes. However, the overall quality of a software object may manifest itself in ways that the simple interpretation of metrics fails to identify. A better strategy is to determine the best, possibly non-linear, subset of many software metrics for accurately estimating software quality. This strategy may be couched in terms of a problem of classification, that is, determine a mapping from a set of software metrics to a set of class labels representing software quality. We implement this strategy using a fuzzy classification approach. The software metrics are automatically computed and presented as features (input) to a classifier, while the class labels (output) are assigned via an expert's (software architect) thorough assessment of the quality of individual software objects. A large collection of classifiers is presented with subsets of the software metric features. Subsets are selected stochastically using a fuzzy logic based sampling method. The classifiers then predict the quality, specifically the class label, of each software object. Fuzzy integration is applied to the results from the most accurate individual classifiers. We empirically evaluate this approach using software objects from a sophisticated algorithm development framework used to develop biomedical data analysis systems. We demonstrate that the sampling method attenuates the effects of confounding features, and the aggregated classification results using fuzzy integration are superior to the predictions from the respective best individual classifiers. {\copyright} 2013 Elsevier Inc. All rights reserved.",0,"A fuzzy classifier approach to estimating software quality. With the increasing sophistication of today's software systems, it is often difficult to estimate the overall quality of underlying software components with respect to attributes such as complexity, utility, and extensibility. Many metrics exist in the software engineering literature that attempt to quantify, with varying levels of accuracy, a large swath of qualitative attributes. However, the overall quality of a software object may manifest itself in ways that the simple interpretation of metrics fails to identify. A better strategy is to determine the best, possibly non-linear, subset of many software metrics for accurately estimating software quality. This strategy may be couched in terms of a problem of classification, that is, determine a mapping from a set of software metrics to a set of class labels representing software quality. We implement this strategy using a fuzzy classification approach. The software metrics are automatically computed and presented as features (input) to a classifier, while the class labels (output) are assigned via an expert's (software architect) thorough assessment of the quality of individual software objects. A large collection of classifiers is presented with subsets of the software metric features. Subsets are selected stochastically using a fuzzy logic based sampling method. The classifiers then predict the quality, specifically the class label, of each software object. Fuzzy integration is applied to the results from the most accurate individual classifiers. We empirically evaluate this approach using software objects from a sophisticated algorithm development framework used to develop biomedical data analysis systems. We demonstrate that the sampling method attenuates the effects of confounding features, and the aggregated classification results using fuzzy integration are superior to the predictions from the respective best individual classifiers. {\copyright} 2013 Elsevier Inc. All rights reserved.",fuzzy classifier approach estimating software quality increasing sophistication today software systems often difficult estimate overall quality underlying software components respect attributes complexity utility extensibility many metrics exist software engineering literature attempt quantify varying levels accuracy large swath qualitative attributes however overall quality software object may manifest ways simple interpretation metrics fails identify better strategy determine best possibly non linear subset many software metrics accurately estimating software quality strategy may couched terms problem classification determine mapping set software metrics set class labels representing software quality implement strategy using fuzzy classification approach software metrics automatically computed presented features input classifier class labels output assigned via expert software architect thorough assessment quality individual software objects large collection classifiers presented subsets software metric features subsets selected stochastically using fuzzy logic based sampling method classifiers predict quality specifically class label software object fuzzy integration applied results accurate individual classifiers empirically evaluate approach using software objects sophisticated algorithm development framework used develop biomedical data analysis systems demonstrate sampling method attenuates effects confounding features aggregated classification results using fuzzy integration superior predictions respective best individual classifiers copyright 2013 elsevier inc rights reserved,1,1,0,1,0,2
128,An expert system for determining candidate software classes for refactoring,Naive Bayes; Refactor prediction; Refactoring; Software metrics,"In the lifetime of a software product, development costs are only the tip of the iceberg. Nearly 90% of the cost is maintenance due to error correction, adaptation and mainly enhancements. As Lehman and Belady [Lehman, M. M., & Belady, L. A. (1985). Program evolution: Processes of software change. Academic Press Professional.] state that software will become increasingly unstructured as it is changed. One way to overcome this problem is refactoring. Refactoring is an approach which reduces the software complexity by incrementally improving internal software quality. Our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity. We propose a machine learning based model to predict classes to be refactored. We use Weighted Na{""\i}ve Bayes with InfoGain heuristic as the learner and we conducted experiments with metric data that we collected from the largest GSM operator in Turkey. Our results showed that we can predict 82% of the classes that need refactoring with 13% of manual inspection effort on the average. {\copyright} 2008 Elsevier Ltd. All rights reserved.",0,"An expert system for determining candidate software classes for refactoring. In the lifetime of a software product, development costs are only the tip of the iceberg. Nearly 90% of the cost is maintenance due to error correction, adaptation and mainly enhancements. As Lehman and Belady [Lehman, M. M., & Belady, L. A. (1985). Program evolution: Processes of software change. Academic Press Professional.] state that software will become increasingly unstructured as it is changed. One way to overcome this problem is refactoring. Refactoring is an approach which reduces the software complexity by incrementally improving internal software quality. Our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity. We propose a machine learning based model to predict classes to be refactored. We use Weighted Na{""\i}ve Bayes with InfoGain heuristic as the learner and we conducted experiments with metric data that we collected from the largest GSM operator in Turkey. Our results showed that we can predict 82% of the classes that need refactoring with 13% of manual inspection effort on the average. {\copyright} 2008 Elsevier Ltd. All rights reserved.",expert system determining candidate software classes refactoring lifetime software product development costs tip iceberg nearly 90 cost maintenance due error correction adaptation mainly enhancements lehman belady lehman belady l 1985 program evolution processes software change academic press professional state software become increasingly unstructured changed one way overcome problem refactoring refactoring approach reduces software complexity incrementally improving internal software quality motivation research detect classes need rafactored analyzing code complexity propose machine learning based model predict classes refactored use weighted na bayes infogain heuristic learner conducted experiments metric data collected largest gsm operator turkey results showed predict 82 classes need refactoring 13 manual inspection effort average copyright 2008 elsevier ltd rights reserved,1,2,0,1,0,2
129,Evolutionary trends of developer coordination: a network approach,Developer coordination; Developer networks; Software evolution,"Software evolution is a fundamental process that transcends the realm of technical artifacts and permeates the entire organizational structure of a software project. By means of a longitudinal empirical study of 18 large open-source projects, we examine and discuss the evolutionary principles that govern the coordination of developers. By applying a network-analytic approach, we found that the implicit and self-organizing structure of developer coordination is ubiquitously described by non-random organizational principles that defy conventional software-engineering wisdom. In particular, we found that: (a) developers form scale-free networks, in which the majority of coordination requirements arise among an extremely small number of developers, (b) developers tend to accumulate coordination requirements with more and more developers over time, presumably limited by an upper bound, and (c) initially developers are hierarchically arranged, but over time, form a hybrid structure, in which core developers are hierarchically arranged and peripheral developers are not. Our results suggest that the organizational structure of large projects is constrained to evolve towards a state that balances the costs and benefits of developer coordination, and the mechanisms used to achieve this state depend on the project's scale. {\copyright} 2016, Springer Science+Business Media New York.",0,"Evolutionary trends of developer coordination: a network approach. Software evolution is a fundamental process that transcends the realm of technical artifacts and permeates the entire organizational structure of a software project. By means of a longitudinal empirical study of 18 large open-source projects, we examine and discuss the evolutionary principles that govern the coordination of developers. By applying a network-analytic approach, we found that the implicit and self-organizing structure of developer coordination is ubiquitously described by non-random organizational principles that defy conventional software-engineering wisdom. In particular, we found that: (a) developers form scale-free networks, in which the majority of coordination requirements arise among an extremely small number of developers, (b) developers tend to accumulate coordination requirements with more and more developers over time, presumably limited by an upper bound, and (c) initially developers are hierarchically arranged, but over time, form a hybrid structure, in which core developers are hierarchically arranged and peripheral developers are not. Our results suggest that the organizational structure of large projects is constrained to evolve towards a state that balances the costs and benefits of developer coordination, and the mechanisms used to achieve this state depend on the project's scale. {\copyright} 2016, Springer Science+Business Media New York.",evolutionary trends developer coordination network approach software evolution fundamental process transcends realm technical artifacts permeates entire organizational structure software project means longitudinal empirical study 18 large open source projects examine discuss evolutionary principles govern coordination developers applying network analytic approach found implicit self organizing structure developer coordination ubiquitously described non random organizational principles defy conventional software engineering wisdom particular found developers form scale free networks majority coordination requirements arise among extremely small number developers b developers tend accumulate coordination requirements developers time presumably limited upper bound c initially developers hierarchically arranged time form hybrid structure core developers hierarchically arranged peripheral developers results suggest organizational structure large projects constrained evolve towards state balances costs benefits developer coordination mechanisms used achieve state depend project scale copyright 2016 springer science business media new york,2,0,1,2,2,3
130,Using Visual Symptoms for Debugging Presentation Failures in Web Applications,image processing; presentation failures; probabilistic model; Root cause analysis; visual symptoms; web applications,"Presentation failures in a website can undermine its success by giving users a negative perception of the trustworthiness of the site and the quality of the services it delivers. Unfortunately, existing techniques for debugging presentation failures do not provide developers with automated and broadly applicable solutions for finding the site's faulty HTML elements and CSS properties. To address this limitation, we propose a novel automated approach for debugging web sites that is based on image processing and probabilistic techniques. Our approach first builds a model that links observable changes in the web site's appearance to faulty elements and styling properties. Then using this model, our approach predicts the elements and styling properties most likely to cause the observed failure for the page under test and reports these to the developer. In evaluation, our approach was more accurate and faster than prior techniques for identifying faulty elements in a website. {\copyright} 2016 IEEE.",0,"Using Visual Symptoms for Debugging Presentation Failures in Web Applications. Presentation failures in a website can undermine its success by giving users a negative perception of the trustworthiness of the site and the quality of the services it delivers. Unfortunately, existing techniques for debugging presentation failures do not provide developers with automated and broadly applicable solutions for finding the site's faulty HTML elements and CSS properties. To address this limitation, we propose a novel automated approach for debugging web sites that is based on image processing and probabilistic techniques. Our approach first builds a model that links observable changes in the web site's appearance to faulty elements and styling properties. Then using this model, our approach predicts the elements and styling properties most likely to cause the observed failure for the page under test and reports these to the developer. In evaluation, our approach was more accurate and faster than prior techniques for identifying faulty elements in a website. {\copyright} 2016 IEEE.",using visual symptoms debugging presentation failures web applications presentation failures website undermine success giving users negative perception trustworthiness site quality services delivers unfortunately existing techniques debugging presentation failures provide developers automated broadly applicable solutions finding site faulty html elements css properties address limitation propose novel automated approach debugging web sites based image processing probabilistic techniques approach first builds model links observable changes web site appearance faulty elements styling properties using model approach predicts elements styling properties likely cause observed failure page test reports developer evaluation approach accurate faster prior techniques identifying faulty elements website copyright 2016 ieee,2,2,0,1,0,2
131,Reflections on self-deception,,"Commentators raised 10 major questions with regard to self-deception: Are dual representations necessary? Does self-deception serve intrapersonal goals? What forces shape self-deception? Are there cultural differences in self-deception? What is the self? Does self-deception have costs? How well do people detect deception? Are self-deceivers lying? Do cognitive processes account for seemingly motivational ones? And how is mental illness tied up with self-deception? We address these questions and conclude that none of them compel major modifications to our theory of self-deception, although many commentators provided helpful suggestions and observations. {\copyright} 2011 Cambridge University Press.",0,"Reflections on self-deception. Commentators raised 10 major questions with regard to self-deception: Are dual representations necessary? Does self-deception serve intrapersonal goals? What forces shape self-deception? Are there cultural differences in self-deception? What is the self? Does self-deception have costs? How well do people detect deception? Are self-deceivers lying? Do cognitive processes account for seemingly motivational ones? And how is mental illness tied up with self-deception? We address these questions and conclude that none of them compel major modifications to our theory of self-deception, although many commentators provided helpful suggestions and observations. {\copyright} 2011 Cambridge University Press.",reflections self deception commentators raised 10 major questions regard self deception dual representations necessary self deception serve intrapersonal goals forces shape self deception cultural differences self deception self self deception costs well people detect deception self deceivers lying cognitive processes account seemingly motivational ones mental illness tied self deception address questions conclude none compel major modifications theory self deception although many commentators provided helpful suggestions observations copyright 2011 cambridge university press,2,0,0,2,1,3
132,A systematic review of requirements change management,Agile; Requirements change management; Systematic review,"Context Software requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement. Objective In this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature. Method We use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work. Results The four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified. Conclusions This paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success. {\copyright} 2017 Elsevier B.V.",0,"A systematic review of requirements change management. Context Software requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement. Objective In this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature. Method We use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work. Results The four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified. Conclusions This paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success. {\copyright} 2017 Elsevier B.V.",systematic review requirements change management context software requirements often set concrete start software development project requirements changes become necessary sometimes inevitable due changes customer requirements changes business rules operating environments hence requirements development includes requirements changes part software process previous work shown failing manage software requirements changes well main contributor project failure given importance subject plethora research work discuss management requirements change various directions ways means examination works suggests room improvement objective paper present systematic review research requirements change management rcm reported literature method use systematic review method answer four key research questions related requirements change management questions 1 causes requirements changes 2 processes used requirements change management 3 techniques used requirements change management 4 organizations make decisions regarding requirements changes questions aimed studying various directions field requirements change management providing suggestions future research work results four questions answered strengths weaknesses existing techniques rcm identified conclusions paper provided information current state art techniques practices rcm research gaps existing work benefits risks difficulties associated rcm also made available software practitioners position making better decisions activities related rcm better decisions lead better planning increase chance project success copyright 2017 elsevier b v,1,0,0,1,0,2
133,Developer-Related Factors in Change Prediction: An Empirical Assessment,Change prediction; Empirical Studies; Mining Software Repositories,"Predicting the areas of the source code having a higher likelihood to change in the future is a crucial activity to allow developers to plan preventive maintenance operations such as refactoring or peer-code reviews. In the past the research community was active in devising change prediction models based on structural metrics extracted from the source code. More recently, Elish et al. showed how evolution metrics can be more efficient for predicting change-prone classes. In this paper, we aim at making a further step ahead by investigating the role of different developer-related factors, which are able to capture the complexity of the development process under different perspectives, in the context of change prediction. We also compared such models with existing change-prediction models based on evolution and code metrics. Our findings reveal the capabilities of developer-based metrics in identifying classes of a software system more likely to be changed in the future. Moreover, we observed interesting complementarities among the experimented prediction models, that may possibly lead to the definition of new combined models exploiting developer-related factors as well as product and evolution metrics. {\copyright} 2017 IEEE.",0,"Developer-Related Factors in Change Prediction: An Empirical Assessment. Predicting the areas of the source code having a higher likelihood to change in the future is a crucial activity to allow developers to plan preventive maintenance operations such as refactoring or peer-code reviews. In the past the research community was active in devising change prediction models based on structural metrics extracted from the source code. More recently, Elish et al. showed how evolution metrics can be more efficient for predicting change-prone classes. In this paper, we aim at making a further step ahead by investigating the role of different developer-related factors, which are able to capture the complexity of the development process under different perspectives, in the context of change prediction. We also compared such models with existing change-prediction models based on evolution and code metrics. Our findings reveal the capabilities of developer-based metrics in identifying classes of a software system more likely to be changed in the future. Moreover, we observed interesting complementarities among the experimented prediction models, that may possibly lead to the definition of new combined models exploiting developer-related factors as well as product and evolution metrics. {\copyright} 2017 IEEE.",developer related factors change prediction empirical assessment predicting areas source code higher likelihood change future crucial activity allow developers plan preventive maintenance operations refactoring peer code reviews past research community active devising change prediction models based structural metrics extracted source code recently elish et al showed evolution metrics efficient predicting change prone classes paper aim making step ahead investigating role different developer related factors able capture complexity development process different perspectives context change prediction also compared models existing change prediction models based evolution code metrics findings reveal capabilities developer based metrics identifying classes software system likely changed future moreover observed interesting complementarities among experimented prediction models may possibly lead definition new combined models exploiting developer related factors well product evolution metrics copyright 2017 ieee,1,2,0,1,0,2
134,Finding and analyzing compiler warning defects,,"Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers. At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting? Our technique is very effective-we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers. {\copyright} 2016 ACM.",0,"Finding and analyzing compiler warning defects. Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers. At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting? Our technique is very effective-we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers. {\copyright} 2016 ACM.",finding analyzing compiler warning defects good compiler diagnostic warnings facilitate software development indicate likely programming mistakes code smells however due compiler bugs warnings may erroneous superfluous missing even mature production compilers like gcc clang paper 1 propose first randomized differential testing technique detect compiler warning defects 2 describe extensive evaluation finding warning defects widely used c compilers high level technique starts generating random programs trigger compilers emit variety compiler warnings aligns warnings different compilers identifies inconsistencies potential bugs develop effective techniques overcome three specific challenges 1 generate random programs 2 align textual warnings 3 reduce test programs bug reporting technique effective found reported 60 bugs gcc 38 confirmed assigned fixed 39 clang 14 confirmed fixed case study demonstrates technique effectiveness also highlights need continue improving compilers warning support essential rather neglected aspect compilers copyright 2016 acm,0,2,1,0,3,2
135,Using (bio)metrics to predict code quality online,,"Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the diffculty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository. In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classi fiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs. {\copyright} 2016 ACM.",0,"Using (bio)metrics to predict code quality online. Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the diffculty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository. In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classi fiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs. {\copyright} 2016 ACM.",using bio metrics predict code quality online finding fixing code quality concerns defects poor understandability code decreases software development evolution costs common industrial practice identify code quality concerns early code reviews code reviews help identify problems early also impose costs development take place code change already completed goal research automatically identify code quality concerns developer making change code using biometrics heart rate variability aim determine diffculty developer experiences working part code well identify help fix code quality concerns even committed repository field study ten professional developers two week period investigated use biometrics determine code quality concerns results show biometrics indeed able predict quality concerns parts code developer working improving upon naive classifier 26 outperforming classi fiers based traditional metrics second study five professional developers different country company found evidence findings initial study replicated overall results presented studies suggest biometrics potential predict code quality concerns online thus lower development evolution costs copyright 2016 acm,1,2,2,1,3,2
136,Brain asymmetry and neural systems: Foundations in clinical neuroscience and neuropsychology,,"The proposed book investigates brain asymmetry from the perspective of functional neural systems theory, a foundational approach for the topic. There is currently no such book available on the market and there is a need for a neuroscience book, with a focus on the functional asymmetry of these two integrated and dynamic brains using historical and modern clinical and experimental research findings with the field. The book provides evidence from multiple methodologies, including clinical lesion studies, brain stimulation, and modern imaging techniques. The author has successfully used the book in doctoral and advances undergraduate courses on neuroscience and neuropsychology. It has also been used to teach a course on the biological basis of behavior and could be used in a variety of contexts and courses. {\copyright} Springer International Publishing Switzerland 2015.",0,"Brain asymmetry and neural systems: Foundations in clinical neuroscience and neuropsychology. The proposed book investigates brain asymmetry from the perspective of functional neural systems theory, a foundational approach for the topic. There is currently no such book available on the market and there is a need for a neuroscience book, with a focus on the functional asymmetry of these two integrated and dynamic brains using historical and modern clinical and experimental research findings with the field. The book provides evidence from multiple methodologies, including clinical lesion studies, brain stimulation, and modern imaging techniques. The author has successfully used the book in doctoral and advances undergraduate courses on neuroscience and neuropsychology. It has also been used to teach a course on the biological basis of behavior and could be used in a variety of contexts and courses. {\copyright} Springer International Publishing Switzerland 2015.",brain asymmetry neural systems foundations clinical neuroscience neuropsychology proposed book investigates brain asymmetry perspective functional neural systems theory foundational approach topic currently book available market need neuroscience book focus functional asymmetry two integrated dynamic brains using historical modern clinical experimental research findings field book provides evidence multiple methodologies including clinical lesion studies brain stimulation modern imaging techniques author successfully used book doctoral advances undergraduate courses neuroscience neuropsychology also used teach course biological basis behavior could used variety contexts courses copyright springer international publishing switzerland 2015,2,0,1,2,1,3
137,Family-based performance measurement,family-based analysis; featurehouse; performance prediction,"Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing. {\copyright} 2013 ACM.",0,"Family-based performance measurement. Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing. {\copyright} 2013 ACM.",family based performance measurement contemporary programs customizable provide many features give rise millions program variants determining feature selection yields optimal performance challenging exponential number variants predicting performance variant based previous measurements proved successful induces trade measurement effort prediction accuracy propose alternative approach family based performance measurement reduce number measurements required identifying feature interactions obtaining accurate predictions key idea create variant simulator translating compile time variability run time variability simulate behavior program variants use measure performance individual methods trace methods features infer feature interactions based call graph evaluate approach means five feature oriented programs average achieve accuracy 98 single measurement per customizable program observations show approach opens avenues future research different domains feature interaction detection testing copyright 2013 acm,0,2,0,1,0,2
138,Mining Sequences of Developer Interactions in Visual Studio for Usage Smells,data mining; IDE usage data; pattern mining; usability analysis,"In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional. {\copyright} 2017 IEEE.",0,"Mining Sequences of Developer Interactions in Visual Studio for Usage Smells. In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional. {\copyright} 2017 IEEE.",mining sequences developer interactions visual studio usage smells paper present semi automatic approach mining large scale dataset ide interactions extract usage smells e inefficient ide usage patterns exhibited developers field approach outlined paper first mines frequent ide usage patterns filtered via set thresholds authors subsequently supported disputed using developer survey order form usage smells contrast conventional mining ide usage data approach identifies time ordered sequences developer actions exhibited many developers field pattern mining workflow resilient ample noise present ide datasets due mix actions events datasets typically contain identify usage patterns smells contribute understanding usability visual studio debugging code search active file navigation broadly understanding developer behavior software development activities among findings discovery developers reluctant use conditional breakpoints debugging due perceived ide performance problems well due lack error checking specifying conditional copyright 2017 ieee,2,2,1,0,3,2
139,Confounds and consequences in geotagged twitter data,,"Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location field in each profile. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and textbased geolocation. GPS-tagging and self-reported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40. {\copyright} 2015 Association for Computational Linguistics.",0,"Confounds and consequences in geotagged twitter data. Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location field in each profile. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and textbased geolocation. GPS-tagging and self-reported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40. {\copyright} 2015 Association for Computational Linguistics.",confounds consequences geotagged twitter data twitter often used quantitative studies identify geographically preferred topics writing styles entities studies rely either gps coordinates attached individual messages user supplied location field profile paper compare data acquisition techniques quantify biases introduce also measure effects linguistic analysis textbased geolocation gps tagging self reported locations yield measurably different corpora linguistic differences partially attributable differences dataset composition age gender using latent variable model induce age gender show demographic variables interact geography affect language use also show accuracy text based geolocation varies population demographics giving best results men age 40 copyright 2015 association computational linguistics,0,2,0,1,0,2
140,Heuristic Search,,"Search has been vital to artificial intelligence from the very beginning as a core technique in problem solving. The authors present a thorough overview of heuristic search with a balance of discussion between theoretical analysis and efficient implementation and application to real-world problems. Current developments in search such as pattern databases and search with efficient use of external memory and parallel processing units on main boards and graphics cards are detailed. Heuristic search as a problem solving tool is demonstrated in applications for puzzle solving, game playing, constraint satisfaction and machine learning. While no previous familiarity with heuristic search is necessary the reader should have a basic knowledge of algorithms, data structures, and calculus. Real-world case studies and chapter ending exercises help to create a full and realized picture of how search fits into the world of artificial intelligence and the one around us. The content is organized into five parts as follows: Search Primer: State-Space Search, Basic Search Algorithms, Dictionary Data Structures, and Automatically Created Heuristics Search under Memory Constraints: Linear-Space Search, Memory-Restricted Search, Symbolic Search, External Search Search Under Time Constraints: Distributed Search, State-Space Pruning, and Real-Time Search Search Variants: Adversary Search, Constraint Satisfaction Search, and Local Search Search Applications: Robotics, Automated System Verification, Action Planning, Vehicle Navigation, and Computational Biology. {\copyright} 2012 Elsevier Inc. All rights reserved.",0,"Heuristic Search. Search has been vital to artificial intelligence from the very beginning as a core technique in problem solving. The authors present a thorough overview of heuristic search with a balance of discussion between theoretical analysis and efficient implementation and application to real-world problems. Current developments in search such as pattern databases and search with efficient use of external memory and parallel processing units on main boards and graphics cards are detailed. Heuristic search as a problem solving tool is demonstrated in applications for puzzle solving, game playing, constraint satisfaction and machine learning. While no previous familiarity with heuristic search is necessary the reader should have a basic knowledge of algorithms, data structures, and calculus. Real-world case studies and chapter ending exercises help to create a full and realized picture of how search fits into the world of artificial intelligence and the one around us. The content is organized into five parts as follows: Search Primer: State-Space Search, Basic Search Algorithms, Dictionary Data Structures, and Automatically Created Heuristics Search under Memory Constraints: Linear-Space Search, Memory-Restricted Search, Symbolic Search, External Search Search Under Time Constraints: Distributed Search, State-Space Pruning, and Real-Time Search Search Variants: Adversary Search, Constraint Satisfaction Search, and Local Search Search Applications: Robotics, Automated System Verification, Action Planning, Vehicle Navigation, and Computational Biology. {\copyright} 2012 Elsevier Inc. All rights reserved.",heuristic search search vital artificial intelligence beginning core technique problem solving authors present thorough overview heuristic search balance discussion theoretical analysis efficient implementation application real world problems current developments search pattern databases search efficient use external memory parallel processing units main boards graphics cards detailed heuristic search problem solving tool demonstrated applications puzzle solving game playing constraint satisfaction machine learning previous familiarity heuristic search necessary reader basic knowledge algorithms data structures calculus real world case studies chapter ending exercises help create full realized picture search fits world artificial intelligence one around us content organized five parts follows search primer state space search basic search algorithms dictionary data structures automatically created heuristics search memory constraints linear space search memory restricted search symbolic search external search search time constraints distributed search state space pruning real time search search variants adversary search constraint satisfaction search local search search applications robotics automated system verification action planning vehicle navigation computational biology copyright 2012 elsevier inc rights reserved,2,0,1,2,2,3
141,"Object oriented design expertise reuse: An approach based on heuristics, design patterns and anti-patterns",,"Object Oriented (OO) languages do not guarantee that a system is flexible enough to absorb future requirements, nor that its components can be reused in other contexts. This paper presents an approach to OO design expertise reuse, which is able to detect certain constructions that compromise future expansion or modification of OO systems, and suggest their replacement by more adequate ones. Both reengineering legacy systems, and systems that are still under development are considered by the approach. A tool (OOPDTool) was developed to support the approach, comprising a knowledge base of good design constructions, that correspond to heuristics and design patterns, as well as problematic constructions (i.e., anti-patterns). {\copyright} Springer-Verlag Berlin Heidelberg 2000.",0,"Object oriented design expertise reuse: An approach based on heuristics, design patterns and anti-patterns. Object Oriented (OO) languages do not guarantee that a system is flexible enough to absorb future requirements, nor that its components can be reused in other contexts. This paper presents an approach to OO design expertise reuse, which is able to detect certain constructions that compromise future expansion or modification of OO systems, and suggest their replacement by more adequate ones. Both reengineering legacy systems, and systems that are still under development are considered by the approach. A tool (OOPDTool) was developed to support the approach, comprising a knowledge base of good design constructions, that correspond to heuristics and design patterns, as well as problematic constructions (i.e., anti-patterns). {\copyright} Springer-Verlag Berlin Heidelberg 2000.",object oriented design expertise reuse approach based heuristics design patterns anti patterns object oriented oo languages guarantee system flexible enough absorb future requirements components reused contexts paper presents approach oo design expertise reuse able detect certain constructions compromise future expansion modification oo systems suggest replacement adequate ones reengineering legacy systems systems still development considered approach tool oopdtool developed support approach comprising knowledge base good design constructions correspond heuristics design patterns well problematic constructions e anti patterns copyright springer verlag berlin heidelberg 2000,0,2,0,2,2,2
142,A comparison of code similarity analysers,Clone detection; Code similarity measurement; Empirical study; Parameter optimisation; Plagiarism detection,"Copying and pasting of source code is a common activity in software engineering. Often, the code is not copied as it is and it may be modified for various purposes; e.g. refactoring, bug fixing, or even software plagiarism. These code modifications could affect the performance of code similarity analysers including code clone and plagiarism detectors to some certain degree. We are interested in two types of code modification in this study: pervasive modifications, i.e. transformations that may have a global effect, and local modifications, i.e. code changes that are contained in a single method or code block. We evaluate 30 code similarity detection techniques and tools using five experimental scenarios for Java source code. These are (1) pervasively modified code, created with tools for source code and bytecode obfuscation, and boiler-plate code, (2) source code normalisation through compilation and decompilation using different decompilers, (3) reuse of optimal configurations over different data sets, (4) tool evaluation using ranked-based measures, and (5) local + global code modifications. Our experimental results show that in the presence of pervasive modifications, some of the general textual similarity measures can offer similar performance to specialised code similarity tools, whilst in the presence of boiler-plate code, highly specialised source code similarity detection techniques and tools outperform textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for three of the tools. Moreover, we demonstrate that optimal configurations are very sensitive to a specific data set. After directly applying optimal configurations derived from one data set to another, the tools perform poorly on the new data set. The code similarity analysers are thoroughly evaluated not only based on several well-known pair-based and query-based error measures but also on each specific type of pervasive code modification. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code. {\copyright} 2017, The Author(s).",0,"A comparison of code similarity analysers. Copying and pasting of source code is a common activity in software engineering. Often, the code is not copied as it is and it may be modified for various purposes; e.g. refactoring, bug fixing, or even software plagiarism. These code modifications could affect the performance of code similarity analysers including code clone and plagiarism detectors to some certain degree. We are interested in two types of code modification in this study: pervasive modifications, i.e. transformations that may have a global effect, and local modifications, i.e. code changes that are contained in a single method or code block. We evaluate 30 code similarity detection techniques and tools using five experimental scenarios for Java source code. These are (1) pervasively modified code, created with tools for source code and bytecode obfuscation, and boiler-plate code, (2) source code normalisation through compilation and decompilation using different decompilers, (3) reuse of optimal configurations over different data sets, (4) tool evaluation using ranked-based measures, and (5) local + global code modifications. Our experimental results show that in the presence of pervasive modifications, some of the general textual similarity measures can offer similar performance to specialised code similarity tools, whilst in the presence of boiler-plate code, highly specialised source code similarity detection techniques and tools outperform textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for three of the tools. Moreover, we demonstrate that optimal configurations are very sensitive to a specific data set. After directly applying optimal configurations derived from one data set to another, the tools perform poorly on the new data set. The code similarity analysers are thoroughly evaluated not only based on several well-known pair-based and query-based error measures but also on each specific type of pervasive code modification. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code. {\copyright} 2017, The Author(s).",comparison code similarity analysers copying pasting source code common activity software engineering often code copied may modified various purposes e g refactoring bug fixing even software plagiarism code modifications could affect performance code similarity analysers including code clone plagiarism detectors certain degree interested two types code modification study pervasive modifications e transformations may global effect local modifications e code changes contained single method code block evaluate 30 code similarity detection techniques tools using five experimental scenarios java source code 1 pervasively modified code created tools source code bytecode obfuscation boiler plate code 2 source code normalisation compilation decompilation using different decompilers 3 reuse optimal configurations different data sets 4 tool evaluation using ranked based measures 5 local global code modifications experimental results show presence pervasive modifications general textual similarity measures offer similar performance specialised code similarity tools whilst presence boiler plate code highly specialised source code similarity detection techniques tools outperform textual similarity measures study strongly validates use compilation decompilation normalisation technique use reduced false classifications zero three tools moreover demonstrate optimal configurations sensitive specific data set directly applying optimal configurations derived one data set another tools perform poorly new data set code similarity analysers thoroughly evaluated based several well known pair based query based error measures also specific type pervasive code modification broad thorough study largest existence potentially invaluable guide future users similarity detection source code copyright 2017 author,0,1,0,1,3,2
143,On the use of design defect examples to detect model refactoring opportunities,Design defects; Detection by example; Genetic algorithm; Search-based software engineering,"Design defects are symptoms of design decay, which can lead to several maintenance problems. To detect these defects, most of existing research is based on the definition of rules that represent a combination of software metrics. These rules are sometimes not enough to detect design defects since it is difficult to find the best threshold values; the rules do not take into consideration the programming context, and it is challenging to find the best combination of metrics. As an alternative, we propose in this paper to identify design defects using a genetic algorithm based on the similarity/distance between the system under study and a set of defect examples without the need to define detection rules. We tested our approach on four open-source systems to identify three potential design defects. The results of our experiments confirm the effectiveness of the proposed approach. {\copyright} 2015, Springer Science+Business Media New York.",0,"On the use of design defect examples to detect model refactoring opportunities. Design defects are symptoms of design decay, which can lead to several maintenance problems. To detect these defects, most of existing research is based on the definition of rules that represent a combination of software metrics. These rules are sometimes not enough to detect design defects since it is difficult to find the best threshold values; the rules do not take into consideration the programming context, and it is challenging to find the best combination of metrics. As an alternative, we propose in this paper to identify design defects using a genetic algorithm based on the similarity/distance between the system under study and a set of defect examples without the need to define detection rules. We tested our approach on four open-source systems to identify three potential design defects. The results of our experiments confirm the effectiveness of the proposed approach. {\copyright} 2015, Springer Science+Business Media New York.",use design defect examples detect model refactoring opportunities design defects symptoms design decay lead several maintenance problems detect defects existing research based definition rules represent combination software metrics rules sometimes enough detect design defects since difficult find best threshold values rules take consideration programming context challenging find best combination metrics alternative propose paper identify design defects using genetic algorithm based similarity distance system study set defect examples without need define detection rules tested approach four open source systems identify three potential design defects results experiments confirm effectiveness proposed approach copyright 2015 springer science business media new york,0,2,2,1,3,0
144,A combined ant colony optimization and simulated annealing algorithm to assess stability and fault-proneness of classes based on internal software quality attributes,Ant colony optimization; C4.5; Metric; Prediction; Rule sets; Search-based software engineering; Simulated annealing; Software quality,"Several machine learning algorithms have been used to assess external quality attributes of software systems. Given a set of metrics that describe internal software attributes (cohesion, complexity, size, etc.), the purpose is to construct a model that can be used to assess external quality attributes (stability, reliability, maintainability, etc.) based on the internal ones. Most of these algorithms result in assessment models that are hard to generalize. As a result, they show a degradation in their assessment performance when used to estimate quality of new software modules. This paper presents a hybrid heuristic to construct software quality estimation models that can be used to predict software quality attributes of new unseen systems prior to re-using them or purchasing them. The technique relies on two heuristics: simulated annealing and ant colony optimization. It learns from the data available in a particular domain guidelines and rules to achieve a particular external software quality. These guidelines are presented as rule-based logical models. We validate our technique on two software quality attributes namely stability and fault-proneness - a subattribute of maintainability. We compare our technique to two state-of-the-art algorithms: Neural Networks (NN) and C4.5 as well as to a previously published Ant Colony Optimization algorithm. Results show that our hybrid technique out-performs both C4.5 and ACO in most of the cases. Compared to NN, our algorithm preserves the white-box nature of the predictive models hence, giving not only the classification of a particular module but also guidelines for software engineers to follow in order to reach a particular external quality attribute. Our algorithm gives promising results and is generic enough to apply to any software quality attribute. {\copyright} 2016 [International Journal of Artificial Intelligence].",0,"A combined ant colony optimization and simulated annealing algorithm to assess stability and fault-proneness of classes based on internal software quality attributes. Several machine learning algorithms have been used to assess external quality attributes of software systems. Given a set of metrics that describe internal software attributes (cohesion, complexity, size, etc.), the purpose is to construct a model that can be used to assess external quality attributes (stability, reliability, maintainability, etc.) based on the internal ones. Most of these algorithms result in assessment models that are hard to generalize. As a result, they show a degradation in their assessment performance when used to estimate quality of new software modules. This paper presents a hybrid heuristic to construct software quality estimation models that can be used to predict software quality attributes of new unseen systems prior to re-using them or purchasing them. The technique relies on two heuristics: simulated annealing and ant colony optimization. It learns from the data available in a particular domain guidelines and rules to achieve a particular external software quality. These guidelines are presented as rule-based logical models. We validate our technique on two software quality attributes namely stability and fault-proneness - a subattribute of maintainability. We compare our technique to two state-of-the-art algorithms: Neural Networks (NN) and C4.5 as well as to a previously published Ant Colony Optimization algorithm. Results show that our hybrid technique out-performs both C4.5 and ACO in most of the cases. Compared to NN, our algorithm preserves the white-box nature of the predictive models hence, giving not only the classification of a particular module but also guidelines for software engineers to follow in order to reach a particular external quality attribute. Our algorithm gives promising results and is generic enough to apply to any software quality attribute. {\copyright} 2016 [International Journal of Artificial Intelligence].",combined ant colony optimization simulated annealing algorithm assess stability fault proneness classes based internal software quality attributes several machine learning algorithms used assess external quality attributes software systems given set metrics describe internal software attributes cohesion complexity size etc purpose construct model used assess external quality attributes stability reliability maintainability etc based internal ones algorithms result assessment models hard generalize result show degradation assessment performance used estimate quality new software modules paper presents hybrid heuristic construct software quality estimation models used predict software quality attributes new unseen systems prior using purchasing technique relies two heuristics simulated annealing ant colony optimization learns data available particular domain guidelines rules achieve particular external software quality guidelines presented rule based logical models validate technique two software quality attributes namely stability fault proneness subattribute maintainability compare technique two state art algorithms neural networks nn c4 5 well previously published ant colony optimization algorithm results show hybrid technique performs c4 5 aco cases compared nn algorithm preserves white box nature predictive models hence giving classification particular module also guidelines software engineers follow order reach particular external quality attribute algorithm gives promising results generic enough apply software quality attribute copyright 2016 international journal artificial intelligence,1,1,0,1,0,2
145,Automatic clustering of code changes,Clustering; Code changes; Software repositories,"Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. {\copyright} 2016 ACM.",0,"Automatic clustering of code changes. Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. {\copyright} 2016 ACM.",automatic clustering code changes several research tools projects require groups similar code changes input examples recommendation bug finding tools provide valuable information developers based data help similar code changes simplify application bug fixes code changes multiple locations project despite benefit practical value existing tools limited users need manually specify input data e groups similar code changes overcome drawback paper presents evaluates two syntactical similarity metrics one specifically designed run fast combination two carefully selected self tuning clustering algorithms automatically detect groups similar code changes evaluate combinations metrics clustering algorithms applying several open source projects also publish detected groups similar code changes online reference dataset automatically detected groups similar code changes work well used input lase recommendation system code changes copyright 2016 acm,0,2,0,1,3,2
146,Experience report: Evaluating the effectiveness of decision trees for detecting code smells,Code Smells; Decision Tree; Genetic Algorithm; Software Quality,"Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use. {\copyright} 2015 IEEE.",1,"Experience report: Evaluating the effectiveness of decision trees for detecting code smells. Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use. {\copyright} 2015 IEEE.",experience report evaluating effectiveness decision trees detecting code smells developers continuously maintain software systems adapt new requirements fix bugs due complexity maintenance tasks time market developers make poor implementation choices also known code smells studies indicate code smells hinder comprehensibility possibly increase change fault proneness therefore must identified enable application corrections challenge inaccurate definitions code smells make developers disagree whether piece code smell consequently making difficult creation universal detection solution able recognize smells different software projects several works proposed identify code smells still report inaccurate results use techniques present developers comprehensive explanation results obtained experimental report study effectiveness decision tree algorithm recognize code smells applied dataset containing 4 open source projects results compared manual oracle existing detection approaches machine learning algorithms results showed approach able effectively learn rules detection code smells studied results even better genetic algorithms used pre select metrics use copyright 2015 ieee,0,2,0,0,3,1
147,Code Bad Smell Detection through Evolutionary Data Mining,bad smell detection; data mining; software evolutionary history,"The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure. {\copyright} 2015 IEEE.",0,"Code Bad Smell Detection through Evolutionary Data Mining. The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure. {\copyright} 2015 IEEE.",code bad smell detection evolutionary data mining existence code bad smell severe impact software quality numerous researches show ignoring code bad smells lead failure software system thus detection bad smells drawn attention many researchers practitioners quite approaches proposed detect code bad smells approaches solely based structural information extracted source code however observed code bad smells evolutionary property thus propose novel approach detect three code bad smells mining software evolutionary data duplicated code shotgun surgery divergent change exploits association rules mined change history software systems upon define heuristic algorithms detect three bad smells experimental results five open source projects demonstrate proposed approach achieves higher precision recall f measure copyright 2015 ieee,0,2,2,0,3,1
148,The ancient origins of consciousness: How the brain created experience,,"How is consciousness created? When did it first appear on Earth, and how did it evolve? What constitutes consciousness, and which animals can be said to be sentient? In this book, Todd Feinberg and Jon Mallatt draw on recent scientific findings to answer these questions -- and to tackle the most fundamental question about the nature of consciousness: how does the material brain create subjective experience? After assembling a list of the biological and neurobiological features that seem responsible for consciousness, and considering the fossil record of evolution, Feinberg and Mallatt argue that consciousness appeared much earlier in evolutionary history than is commonly assumed. About 520 to 560 million years ago, they explain, the great ""Cambrian explosion"" of animal diversity produced the first complex brains, which were accompanied by the first appearance of consciousness; simple reflexive behaviors evolved into a unified inner world of subjective experiences. From this they deduce that all vertebrates are and have always been conscious -- not just humans and other mammals, but also every fish, reptile, amphibian, and bird. Considering invertebrates, they find that arthropods (including insects and probably crustaceans) and cephalopods (including the octopus) meet many of the criteria for consciousness. The obvious and conventional wisdom--shattering implication is that consciousness evolved simultaneously but independently in the first vertebrates and possibly arthropods more than half a billion years ago. Combining evolutionary, neurobiological, and philosophical approaches allows Feinberg and Mallatt to offer an original solution to the ""hard problem"" of consciousness. {\copyright} 2016 Massachusetts Institute of Technology. All rights reserved.",0,"The ancient origins of consciousness: How the brain created experience. How is consciousness created? When did it first appear on Earth, and how did it evolve? What constitutes consciousness, and which animals can be said to be sentient? In this book, Todd Feinberg and Jon Mallatt draw on recent scientific findings to answer these questions -- and to tackle the most fundamental question about the nature of consciousness: how does the material brain create subjective experience? After assembling a list of the biological and neurobiological features that seem responsible for consciousness, and considering the fossil record of evolution, Feinberg and Mallatt argue that consciousness appeared much earlier in evolutionary history than is commonly assumed. About 520 to 560 million years ago, they explain, the great ""Cambrian explosion"" of animal diversity produced the first complex brains, which were accompanied by the first appearance of consciousness; simple reflexive behaviors evolved into a unified inner world of subjective experiences. From this they deduce that all vertebrates are and have always been conscious -- not just humans and other mammals, but also every fish, reptile, amphibian, and bird. Considering invertebrates, they find that arthropods (including insects and probably crustaceans) and cephalopods (including the octopus) meet many of the criteria for consciousness. The obvious and conventional wisdom--shattering implication is that consciousness evolved simultaneously but independently in the first vertebrates and possibly arthropods more than half a billion years ago. Combining evolutionary, neurobiological, and philosophical approaches allows Feinberg and Mallatt to offer an original solution to the ""hard problem"" of consciousness. {\copyright} 2016 Massachusetts Institute of Technology. All rights reserved.",ancient origins consciousness brain created experience consciousness created first appear earth evolve constitutes consciousness animals said sentient book todd feinberg jon mallatt draw recent scientific findings answer questions tackle fundamental question nature consciousness material brain create subjective experience assembling list biological neurobiological features seem responsible consciousness considering fossil record evolution feinberg mallatt argue consciousness appeared much earlier evolutionary history commonly assumed 520 560 million years ago explain great cambrian explosion animal diversity produced first complex brains accompanied first appearance consciousness simple reflexive behaviors evolved unified inner world subjective experiences deduce vertebrates always conscious humans mammals also every fish reptile amphibian bird considering invertebrates find arthropods including insects probably crustaceans cephalopods including octopus meet many criteria consciousness obvious conventional wisdom shattering implication consciousness evolved simultaneously independently first vertebrates possibly arthropods half billion years ago combining evolutionary neurobiological philosophical approaches allows feinberg mallatt offer original solution hard problem consciousness copyright 2016 massachusetts institute technology rights reserved,2,0,1,2,1,3
149,Protein- and Peptide-Based Biosensors in Artificial Olfaction,artificial olfaction; biosensor; odorant-binding protein; olfactory receptor; peptide; Volatile organic compounds,"Animals' olfactory systems rely on proteins, olfactory receptors (ORs) and odorant-binding proteins (OBPs), as their native sensing units to detect odours. Recent advances demonstrate that these proteins can also be employed as molecular recognition units in gas-phase biosensors. In addition, the interactions between odorant molecules and ORs or OBPs are a source of inspiration for designing peptides with tunable odorant selectivity. We review recent progress in gas biosensors employing biological units (ORs, OBPs, and peptides) in light of future developments in artificial olfaction, emphasizing examples where biological components have been employed to detect gas-phase analytes. {\copyright} 2018 The Authors",0,"Protein- and Peptide-Based Biosensors in Artificial Olfaction. Animals' olfactory systems rely on proteins, olfactory receptors (ORs) and odorant-binding proteins (OBPs), as their native sensing units to detect odours. Recent advances demonstrate that these proteins can also be employed as molecular recognition units in gas-phase biosensors. In addition, the interactions between odorant molecules and ORs or OBPs are a source of inspiration for designing peptides with tunable odorant selectivity. We review recent progress in gas biosensors employing biological units (ORs, OBPs, and peptides) in light of future developments in artificial olfaction, emphasizing examples where biological components have been employed to detect gas-phase analytes. {\copyright} 2018 The Authors",protein peptide based biosensors artificial olfaction animals olfactory systems rely proteins olfactory receptors ors odorant binding proteins obps native sensing units detect odours recent advances demonstrate proteins also employed molecular recognition units gas phase biosensors addition interactions odorant molecules ors obps source inspiration designing peptides tunable odorant selectivity review recent progress gas biosensors employing biological units ors obps peptides light future developments artificial olfaction emphasizing examples biological components employed detect gas phase analytes copyright 2018 authors,2,0,0,2,2,3
150,An empirical study on the removal of Self-Admitted Technical Debt,Mining software repositories; Self-Admitted Technical Debt; Source code quality,"Technical debt refers to the phenomena of taking shortcuts to achieve short term gain at the cost of higher maintenance efforts in the future. Recently, approaches were developed to detect technical debt through code comments, referred to as Self-Admitted Technical Debt (SATD). Due to its importance, several studies have focused on the detection of SATD and examined its impact on software quality. However, preliminary findings showed that in some cases SATD may live in a project for a long time, i.e., more than 10 years. These findings clearly show that not all SATD may be regarded as 'bad' and some SATD needs to be removed, while other SATD may be fine to take on. Therefore, in this paper, we study the removal of SATD. In an empirical study on five open source projects, we examine how much SATD is removed and who removes SATD? We also investigate for how long SATD lives in a project and what activities lead to the removal of SATD? Our findings indicate that the majority of SATD is removed and that the majority is self-removed (i.e., removed by the same person that introduced it). Moreover, we find that SATD can last between approx. 18-172 days, on median. Finally, through a developer survey, we find that developers mostly use SATD to track future bugs and areas of the code that need improvements. Also, developers mostly remove SATD when they are fixing bugs or adding new features. Our findings contribute to the body of empirical evidence on SATD, in particular evidence pertaining to its removal. {\copyright} 2017 IEEE.",0,"An empirical study on the removal of Self-Admitted Technical Debt. Technical debt refers to the phenomena of taking shortcuts to achieve short term gain at the cost of higher maintenance efforts in the future. Recently, approaches were developed to detect technical debt through code comments, referred to as Self-Admitted Technical Debt (SATD). Due to its importance, several studies have focused on the detection of SATD and examined its impact on software quality. However, preliminary findings showed that in some cases SATD may live in a project for a long time, i.e., more than 10 years. These findings clearly show that not all SATD may be regarded as 'bad' and some SATD needs to be removed, while other SATD may be fine to take on. Therefore, in this paper, we study the removal of SATD. In an empirical study on five open source projects, we examine how much SATD is removed and who removes SATD? We also investigate for how long SATD lives in a project and what activities lead to the removal of SATD? Our findings indicate that the majority of SATD is removed and that the majority is self-removed (i.e., removed by the same person that introduced it). Moreover, we find that SATD can last between approx. 18-172 days, on median. Finally, through a developer survey, we find that developers mostly use SATD to track future bugs and areas of the code that need improvements. Also, developers mostly remove SATD when they are fixing bugs or adding new features. Our findings contribute to the body of empirical evidence on SATD, in particular evidence pertaining to its removal. {\copyright} 2017 IEEE.",empirical study removal self admitted technical debt technical debt refers phenomena taking shortcuts achieve short term gain cost higher maintenance efforts future recently approaches developed detect technical debt code comments referred self admitted technical debt satd due importance several studies focused detection satd examined impact software quality however preliminary findings showed cases satd may live project long time e 10 years findings clearly show satd may regarded bad satd needs removed satd may fine take therefore paper study removal satd empirical study five open source projects examine much satd removed removes satd also investigate long satd lives project activities lead removal satd findings indicate majority satd removed majority self removed e removed person introduced moreover find satd last approx 18 172 days median finally developer survey find developers mostly use satd track future bugs areas code need improvements also developers mostly remove satd fixing bugs adding new features findings contribute body empirical evidence satd particular evidence pertaining removal copyright 2017 ieee,2,1,0,1,0,2
151,Predicting query quality for applications of text retrieval to software engineering tasks,Artifact traceability; Concept location; Text retrieval,"Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself. Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery. Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average. Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR. {\copyright} 2017 ACM.",0,"Predicting query quality for applications of text retrieval to software engineering tasks. Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself. Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery. Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average. Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR. {\copyright} 2017 ACM.",predicting query quality applications text retrieval software engineering tasks context since mid 2000s numerous recommendation systems based text retrieval tr proposed support software engineering se tasks concept location traceability link recovery code reuse impact analysis success tr based solutions highly depends query submitted either formulated developer automatically extracted software artifacts aim aim predicting quality queries submitted tr based approaches se lead benefits developers quality software systems alike example knowing query poorly formulated save developers time frustration analyzing irrelevant search results instead could focus reformulating query also knowing artifact used query leads irrelevant search results may uncover underlying problems query artifact method introduce automatic query quality prediction approach software artifact retrieval adapting nl inspired solutions use software data present two applications evaluations approach context concept location traceability link recovery tr applied often se concept location use approach determine list retrieved code elements likely contain code relevant particular change request case queries good candidates reformulation traceability link recovery queries represent software artifacts case use query quality prediction approach identify artifacts hard trace artifacts may therefore low intrinsic quality tr based traceability link recovery results concept location evaluation shows approach able correctly predict quality queries 82 cases average using little training data case traceability recovery proposed approach able detect hard trace artifacts 74 cases average conclusions results evaluation applications concept location traceability link recovery indicate approach used predict results tr based approach assessing quality text query lead saved effort time well identification software artifacts may difficult trace using tr copyright 2017 acm,1,1,0,1,0,2
152,Predicting delays in software projects using networked classification,Machine Learning; Networked classification; Risk management; Software analytics,"Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) - as done in previous work - but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46% - 97% precision (49% improved), 46% - 97% recall (28% improved), 56% - 75% F-measure (39% improved), and 78% - 95% Area Under the ROC Curve (16% improved). {\copyright} 2015 IEEE.",0,"Predicting delays in software projects using networked classification. Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) - as done in previous work - but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46% - 97% precision (49% improved), 46% - 97% recall (28% improved), 56% - 75% F-measure (39% improved), and 78% - 95% Area Under the ROC Curve (16% improved). {\copyright} 2015 IEEE.",predicting delays software projects using networked classification software projects high risk cost schedule overruns source concern software engineering community long time one challenges software project management make reliable prediction delays context constant rapid changes inherent software projects paper presents novel approach providing automated support project managers decision makers predicting whether subset software tasks among hundreds thousands ongoing tasks software project risk delayed approach makes use features specific individual software tasks e local data done previous work also relationships e networked data addition using collective classification approach simultaneously predict degree delay group related tasks evaluation results show significant improvement traditional approaches perform classification task independently achieving 46 97 precision 49 improved 46 97 recall 28 improved 56 75 f measure 39 improved 78 95 area roc curve 16 improved copyright 2015 ieee,1,1,0,1,0,2
