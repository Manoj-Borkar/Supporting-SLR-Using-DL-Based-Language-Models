,title,keywords,abstract,label,freq,docID
0,Emotion Explained,Brain; Consciousness; Decision-making; Emotion; Hunger; Pleasure; Reward; Sexual behaviour,"The book links the analysis of the brain mechanisms of emotion and motivation to the wider context of what emotions are, what their functions are, how emotions evolved, and the larger issue of why emotional and motivational feelings and consciousness might arise in a system organized like the brain. The topics in motivation covered are hunger, thirst, sexual behaviour, brain-stimulation reward, and addiction. The book proposes a theory of what emotions are, and an evolutionary, Darwinian, theory of the adaptive value of emotion, and then describes the brain mechanisms of emotion. The book examines how cognitive states can influence emotions, and in turn, how emotions can influence cognitive states. The book also examines emotion and decision-making, with links to the burgeoning field of neuroeconomics. The book describes the brain mechanisms that underlie both emotion and motivation in a scientific form that can be used by both students and scientists in the fields of neuroscience, psychology, cognitive neuroscience, biology, physiology, psychiatry, and medicine. {\copyright} Edmund T Rolls, 2005. All rights reserved.",0,141,1
1,Assessing the bug-prediction with re-usability based package organization for object oriented software systems,Fault-proneness prediction; Package reuse; Software quality,"Packages are re-usable components for faster and effective software maintenance. To promote the re-use in object-oriented systems and maintenance tasks easier, packages should be organized to depict compact design. Therefore, understanding and assessing package organization is primordial for maintenance tasks like Re-usability and Changeability. We believe that additional investigations of prevalent basic design principles such as defined by R.C. Martin are required to explore different aspects of package organization. In this study, we propose package-organization framework based on reachable components that measures re-usability index. Package re-usability index measures common effect of change taking place over dependent elements of a package in an object-oriented design paradigm. A detailed quality assessment on different versions of open source software systems is presented which evaluates capability of the proposed package re-usability index and other traditional package-level metrics to predict fault-proneness in software. The experimental study shows that proposed index captures different aspects of package-design which can be practically integrated with best practices of software development. Furthermore, the results provide insights on organization of feasible software design to counter potential faults appearing due to complex package dependencies. {\copyright} 2017 The Institute of Electronics, Information and Communication Engineers.",0,141,2
2,Prediction of change-prone classes using machine learning and statistical techniques,,"For software development, availability of resources is limited, thereby necessitating efficient and effective utilization of resources. This can be achieved through prediction of key attributes, which affect software quality such as fault proneness, change proneness, effort, maintainability, etc. The primary aim of this chapter is to investigate the relationship between object-oriented metrics and change proneness. Predicting the classes that are prone to changes can help in maintenance and testing. Developers can focus on the classes that are more change prone by appropriately allocating resources. This will help in reducing costs associated with software maintenance activities. The authors have constructed models to predict change proneness using various machine-learning methods and one statistical method. They have evaluated and compared the performance of these methods. The proposed models are validated using open source software, Frinika, and the results are evaluated using Receiver Operating Characteristic (ROC) analysis. The study shows that machine-learning methods are more efficient than regression techniques. Among the machine-learning methods, boosting technique (i.e. Logitboost) outperformed all the other models. Thus, the authors conclude that the developed models can be used to predict the change proneness of classes, leading to improved software quality. {\copyright} 2017 by IGI Global. All rights reserved.",0,141,3
3,Primate behavioral ecology: Fifth edition,,"This comprehensive introductory text integrates evolutionary, ecological, and demographic perspectives with new results from field studies and contemporary noninvasive molecular and hormonal techniques to understand how different primates behave and the significance of these insights for primate conservation. Each chapter is organized around the major research themes in the field, with Strier emphasizing the interplay between theory, observations, and conservation issues. Examples are drawn from the ""classic"" primate field studies as well as more recent studies on previously neglected species, illustrating the vast behavioral variation that exists across the primate order. Primate Behavioral Ecology 5th Edition also examines how anthropogenic activities are negatively impacting primate populations, including a thorough analysis of behavioural plasticity and its implications. This fully updated new edition incorporates exciting new discoveries and the most up-to-date approaches in the field to provide an invaluable overview of the field of primate behavioral ecology and its applications to primate conservation. It is considered to be a ""must read"" for all students interested in primates. {\copyright} 2017, 2011, 2007, 2003, 2000 K. B. Strier. All rights reserved.",0,141,4
4,Feature subset selection for instance filtering methods on cross-project defect prediction,Code metrics; Cross-project defect prediction; Feature selection; Instance filtering; Network metrics; Software quality assurance,"The defect prediction models can be a good tool on organizing the project's test resources. However, not all companies maintain an appropriate set of historical defect data. In this case, the companies can build an appropriate dataset from known external projects. This approach, called Cross-project Defect Prediction (CPDP), solves the lack of defect data, although introduces heterogeneity on data. This heterogeneity can compromises the performance of CPDP models. Recently, filtering methods were proposed in order to decrease the heterogeneity of data by selecting the most similar instances from the training dataset. This similarity between instances is calculated based on the available features of the dataset. On this study, we propose that using only the most relevant features on this calculation can result in more accurate filtered datasets and better prediction performances. We present an empirical evaluation of different methods used for selecting the most relevant features. We evaluate different configurations of four Feature Selection (FS) methods and two metric subsets. We used 36 versions of 11 open source projects on experiments. The results indicate that the defect prediction performance can be improved by using the evaluated approach. In addition, we investigated which methods present better performances. The results do not indicate a method with general better performances, i.e., the most appropriate method for a project can vary according to the project characteristics.",0,141,5
5,Social Science Methods for Psychodynamic Inquiry: The Unconscious on the World Scene,,"This book explains, with case examples, a variety of social science research methods suitable for studying the unconscious components of such irrational social and political actions in world affairs. {\copyright} William R. Meyers 2015. All rights reserved.",0,141,6
6,The prediction of code clone quality based on bayesian network,Bayesian network; Code clone; Prediction; Quality model; Reconstruct,"This paper researched on the quality of code clone in the software, evaluated the code clone quality of the current versions. Then using Bayesian network to train the existing sample data to get the prediction model of code clone that is able to predict the quality. The prediction results are able to help developers decide which code clone should be reconstructed or efficiently reused. The experiment shows that the method can be used to predict the quality of code clone in software more accurately. {\copyright} 2016 SERSC.",0,141,7
7,A novel framework for integrating data mining techniques to software development phases,Code optimization; Defect prediction and classification; Software development process; Software effort estimation; Software reuse,"In software development process, phases such as development effort estimation, code optimization, source code defect detection and software reuse are very important in order to improve the productivity and quality of the software. Software repository data produced in each phases have increased as component of software development process and new data analysis techniques have emerged in order to optimize the software development process. There is a gap between the software project management practices and the need of valuable data from software repository. To overcome this gap, a novel integrated framework is proposed, which integrates data mining techniques to extract valuable information from software repository and software metrics are used in different phases of software development process. Integrated framework can be used by software development project managers to improve quality of software and reduce time in predicting effort estimation, optimizing source code, defect detection and classification. {\copyright} Springer India 2016.",0,141,8
8,Measuring the utility of functional-based software using centroid-adjusted class labelling,Functional programming; Java lambda; Pattern classification; Software engineering; Software metrics; Software utility,"The functional programming paradigm involves stateless computation on immutable data constructs. While this paradigm's historical context dates back to the early twentieth century with lambda calculus and a formal study of computability and function definition, there has been a resurgence in functional programming, especially in the area of predictive analytics. New, purely functional, languages have recently emerged, and functional extensions have been added to several popular programming languages. It is sometimes difficult to estimate the overall utility and extensibility of functional programming software components. At the same time, many software metrics exist that attempt to quantify various qualitative attributes of software components. Here, we use a computational intelligence strategy that uses a set of software metrics to predict the qualitative utility of a software system's underlying components. Centroid-adjusted class labelling is a pattern classification preprocessing method that compensates for the possible imprecision of an established external reference test (gold standard) by adjusting, when necessary, design pattern class labels while maintaining the reference test's discriminatory power. The adjusted design labels incorporate within-class centroid information using robust measures of location and dispersion. This method is applied to a biomedical data analysis software system written in a functional programming style. It is shown that significant improvement to the discriminatory power of the classifier is obtained when using this preprocessing method. {\copyright} Springer International Publishing Switzerland 2016.",0,141,9
9,Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension,association mining; logical coupling; particle swarm optimization; program comprehension; source code visualization,"By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration. {\copyright} 2015 The Authors.",0,141,10
10,Programming biology: Expanding the toolset for the engineering of transcription,,"Transcription is a complex and dynamic process representing the first step in gene expression that can be readily controlled through current tools in molecular biology. Elucidating and subsequently controlling transcriptional processes in various prokaryotic and eukaryotic organisms have been a key element in translational research, yielding a variety of new opportunities for scientists and engineers. This chapter aims to give an overview of how the fields of molecular and synthetic biology have contributed both historically and presently to the state of the art in transcriptional engineering. The described tools and techniques, as well as the emerging genetic circuit engineering discipline, open the door to new advances in the fields of medical and industrial biotechnology. {\copyright} Springer International Publishing Switzerland 2016.",0,141,11
11,Prediction of changeability for object oriented classes and packages by mining change history,change history; changeability prediction; data mining; object oriented software component; software measurements,"As a software system evolves, the classes are changed due to some development or maintenance activity, which is inevitable in software life cycle. These class changes can produce ripple effect or can lead to subsequent changes to other classes in the same package. In this paper, the changeability predictors for the classes and packages are proposed based on their past change pattern i.e. change history. Association learning method has been applied for discovering change-coupling pattern between the classes. In present work, the framework for the computation of the proposed changeability predictors is demonstrated and results are evaluated for java application. The results show that, association mining based machine learning technique can be useful to classify the classes as per their change-readiness. For doing changes in future, the proposed changeability predictors will be helpful for development team to predict the changeability of the classes. {\copyright} 2014 IEEE.",0,141,12
12,Analysing the style of textual labels in i* models,,"An important quality aspect for conceptual models (such as i* models) is the quality of textual labels. Naming conventions are aimed to make sure that labels are used in a consistent manner. We present a tool that checks automatically whether a textual label in an i* model adheres to a set of naming conventions. This does not only help to enforce the use of a consistent labelling style, it also helps to detect modelling errors such as goals in i* models that should be softgoals (or vice versa).",0,141,13
13,Design pattern recognition by using adaptive neuro fuzzy inference system,ANFIS; machine learning; pattern recognition; Software design patterns,"Software design patterns describe recurring design problems and provide the essence of best practice solutions. It is useful and important, for various software engineering tasks, to know which design pattern is implemented where in a software design. However, this information is often lost due to poor or absent documentation, and so accurate recognition tools are required. The problem is that design patterns, given their abstract and vague nature, have a level of resistance to be automatically and accurately recognized. Although this vagueness or fuzziness can be captured and modelled by the fuzzy inference system, it has not yet been applied to solve this problem. This paper fills this gap by proposing an approach for design pattern recognition based on Adaptive Neuro Fuzzy Inference System. Our approach consists of two phases: space reduction phase and design pattern recognition phase. Both phases are implemented by ANFIS. We evaluate the approach by an experiment conducted to recognize six design patterns in an open source application. The results show that the approach is viable and promising. {\copyright} 2013 IEEE.",0,141,14
14,Feature-oriented software product lines: Concepts and implementation,,"While standardization has empowered the software industry to substantially scale software development and to provide affordable software to a broad market, it often does not address smaller market segments, nor the needs and wishes of individual customers. Software product lines reconcile mass production and standardization with mass customization in software engineering. Ideally, based on a set of reusable parts, a software manufacturer can generate a software product based on the requirements of its customer. The concept of features is central to achieving this level of automation, because features bridge the gap between the requirements the customer has and the functionality a product provides. Thus features are a central concept in all phases of product-line development. The authors take a developer's viewpoint, focus on the development, maintenance, and implementation of product-line variability, and especially concentrate on automated product derivation based on a user's feature selection. The book consists of three parts. Part I provides a general introduction to feature-oriented software product lines, describing the product-line approach and introducing the product-line development process with its two elements of domain and application engineering. The pivotal part II covers a wide variety of implementation techniques including design patterns, frameworks, components, feature-oriented programming, and aspect-oriented programming, as well as tool-based approaches including preprocessors, build systems, version-control systems, and virtual separation of concerns. Finally, part III is devoted to advanced topics related to feature-oriented product lines like refactoring, feature interaction, and analysis tools specific to product lines. In addition, an appendix lists various helpful tools for software product-line development, along with a description of how they relate to the topics covered in this book. To tie the book together, the authors use two running examples that are well documented in the product-line literature: data management for embedded systems, and variations of graph data structures. They start every chapter by explicitly stating the respective learning goals and finish it with a set of exercises; additional teaching material is also available online. All these features make the book ideally suited for teaching - both for academic classes and for professionals interested in self-study. {\copyright} Springer-Verlag Berlin Heidelberg 2013. All rights are reserved.",0,141,15
15,Unifying and refactoring DMF to support concurrent Jini and JMS DMS in GIPSY,distributed demand-driven computing; GIPSY; Jini; JMS,"The General Intensional Programming System (GIPSY) is a framework for the compilation and distributed demand-driven evaluation of context-aware declarative programs. Its distributed run-time system includes a demand migration framework that has up to now been instantiated with two different communication technologies, namely Jini and JMS. However, the different nature, APIs, and requirements of these two solutions have resulted in artifact implementation divergence from the original demand migration framework (DMF) specifications as well as flaws in the DMF itself. This also inhibited the concurrent consistent use of the technologies within the same GIPSY network instance. Thus, in this paper we report on our re-engineering effort and results to refactor and unify the two somewhat disjoint Java distributed middleware technologies - Jini and JMS - used in the implementation of the corresponding Demand Migration Systems (DMS). In doing so, we refactor their parent Demand Migration Framework (DMF), within the General Intensional Programming System (GIPSY) and realign the Jini and JMS implementation remedying the flaws, improving interoperability, and allowing for scalability testing with various real applications and comparative studies. {\copyright} 2012 ACM.",0,141,16
16,"Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering, MALETS'11 - Associated with 26th IEEE/ACM International Conference on Automated Software Engineering",,The proceedings contain 8 papers. The topics discussed include: learning system abstractions for human operators; software fault localization using feature selection; the inductive software engineering manifesto: principles for industrial data mining; do better IR tools improve the accuracy of engineers' traceability recovery?; evolution of legacy system comprehensibility through automated refactoring; ProbPoly - a probabilistic inductive logic programming framework with application in model checking; towards learning to detect meaningful changes in software; and software analytics as a learning case in practice: approaches and experiences.,0,141,17
17,Mobile phone-enabled control of medical care and handicapped assistance,actuation; control; healthcare; mobile medicine; mobile phone; pervasive medicine; telemedicine,"Mobile phones are now playing an ever more crucial role in peoples daily lives. They are serving not only as a way of talking and delivering messages, but also for exchanging various information. Nevertheless, the functional limit of the phone is still far from being reached. Among the many promising applications, using mobile phones as an actuating element to control data or devices is useful in quite a few emerging medical care and handicapped assistance settings owing to its wireless communication feature. In this article, selected progresses of mobile phone-enabled controlling have been summarized, with more focus on evaluating its emerging roles in medical care. Several typical applications in the area are illustrated and some potential technical challenges and key issues worthy of pursuit are outlined. The intent of the article is to provide an elementary knowledge for people with different backgrounds, such as electrical or biomedical engineers, as well as people who are working on interdisciplinary areas. It is expected that medical care at any time and anywhere will be possible via the actuation platform provided by the mobile phone and mobile medicine will be pushed forward to a new height in the coming years. {\copyright} 2011 Expert Reviews Ltd.",0,141,18
18,Searching for Molecular Solutions: Empirical Discovery and Its Future,,"A comprehensive look at empirical approaches to molecular discovery, their relationships with rational design, and the future of both Empirical methods of discovery, along with serendipitous and rational design approaches, have played an important role in human history. Searching for Molecular Solutions compares empirical discovery strategies for biologically useful molecules with serendipitous discovery and rational design, while also considering the strengths and limitations of empirical pathways to molecular discovery. Logically arranged, this text examines the different modes of molecular discovery, empha-sizing the historical and ongoing importance of empirical strategies. Along with a broad overview of the subject matter, Searching for Molecular Solutions explores: The differing modes of molecular discovery Biological precedents for evolutionary approaches Directed evolutionary methods and related areas Enzyme evolution and design Functional nucleic acid discovery Antibodies and other recognition molecules General aspects of molecular recognition Small molecule discovery approaches Rational molecular design The interplay between empirical and rational strategies and their ongoing roles in the future of molecular discovery Searching for Molecular Solutions covers several major areas of modern research, development, and practical applications of molecular sciences. This text offers empirical-rational principles of broad relevance to scientists, professionals, and students interested in general aspectsof molecular discovery, as well as the thought processes behind experimental approaches. {\copyright} 2010 John Wiley & Sons, Inc.",0,141,19
19,A unified granular fuzzy-neuro framework for predicting and understanding software quality,Approximation of Min-Max relational equations; Fuzzy sequence; Hybrid granular fuzzy-neuro possibilistic model; If-then fuzzy weighted rules; Level of stability; Possibility theory; Software quality prediction and understanding,"We propose herein a novel unified framework that uses a developed hybrid fuzzy-neuro system in order to evaluate the impact of inheritance aspects on the evolvability of a class library, and to study the relevance of using inheritance as indicator of class interface stability with respect to version change. To this goal, we propose a novel computational granular unified framework that is cognitively motivated for learning if-then fuzzy weighted rules by using a hybrid neuro-fuzzy or fuzzy-neuro possibilistic model appropriately crafted as a means to automatically extract or learn software fuzzy prediction rules from only input-output examples by integrating some useful concepts from the human cognitive processes and adding some interesting granular functionalities. This learning scheme uses an exhaustive search over the fuzzy partitions of involved variables, automatic fuzzy hypotheses generation, formulation and testing, and approximation procedure of Min-Max relational equations. The main idea is to start learning from coarse fuzzy partitions of the involved metrics variables (both input and output) and proceed progressively toward fine-grained partitions until finding the appropriate partitions that fit the data. According to the complexity of the problem at hand, it learns the whole structure of the fuzzy system, i.e. conjointly appropriate fuzzy partitions, appropriate fuzzy rules, their number and their associated membership functions.",0,141,20
20,"3rd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2008",,"The proceedings contain 61 papers. The special focus in this conference is on Leveraging Applications of Formal Methods, Verification and Validation. The topics include: Architecture based specification and verification of embedded software systems; information system engineering supporting observation, orientation, decision, and compliant action; modelling coordination and compensation; animating event B models by formal data models; automated formal testing of C API using T2C framework; tailoring and optimising software for automotive multicore system; timing validation of automotive software; towards using reo for compliance-aware business process modeling; a use-case driven approach to formal service-oriented modelling; safety and response-time analysis of an automotive accident assistance service; a framework for analyzing and testing the performance of software services; assuring the satisfiability of sequential extended regular expressions; computing must and may alias to detect null pointer dereference; program verification by reduction to semi-algebraic systems solving; debugging statecharts via model-code traceability; formal use of design patterns and refactoring; a component-based access control monitor; navigating the requirements jungle; non-functional avionics requirements; measurement-based timing analysis; weaving a formal methods education with problem-based learning; encouraging the uptake of formal methods training in an industrial context; computer-supported collaborative learning with mind-maps; thinking in user-centric models; specialization and instantiation aspects of a standard process for developing educational modules; contexts and context awareness in view of the diagram predicate framework; the use of adaptive semantic hypermedia for ubiquitous collaboration systems; the use of formal ontology to specify context in ubiquitous computing; high service availability in MaTRICS for the OCS; the ASK system and the challenge of distributed knowledge discovery; requirements for ontology based design project assessment; organizing the worlds machine learning information; workflow testing; directed generation of test data for static semantics checker; optimizing the system observability level for diagnosability; weaving authentication and authorization requirements into the functional model of a system using Z promotion; simple gedanken experiments in leveraging applications of formal methods and composition of web services using wrappers.",0,141,21
21,Intelligent Java analyzer,,"This paper presents a software metric working prototype to evaluate Java programmer's profiles. In order to automatically detect source code patterns, a Multi Layer Perceptron neural network is applied. Features determined from such patterns constitute the basis for system's programmer profiling. Results presented here show that the proposed prototype is a confident approach for support in the software quality assurance process. {\copyright} 2008 IEEE.",0,141,22
22,"Morphology, processing, and integrating of information from large source code warehouses for decision support",,"Source code occurs in diverse programming languages with documentation using miscellaneous standards, comments in individual styles, extracted metrics or associated test cases that are hard to exploit through information retrieval or knowledge-discovery techniques. Typically, the information about object-oriented source code for a software system is distributed across several different sources, which makes processing complex. In this chapter we describe the morphology of object-oriented source code and how we (pre-) process, integrate and use it for knowledge discovery in software engineering in order to support decision-making regarding the refactoring, reengineering and reuse of software systems. {\copyright} 2006, Idea Group Inc.",0,141,23
23,Genetic algorithm based restructuring of object-oriented designs using metrics,Genetic algorithm; Metrics; Object-oriented design; Software restructuring,"Software with design flaws increases maintenance costs, decreases component reuse, and reduces software life. Even well-designed software tends to deteriorate with time as it undergoes maintenance. Work on restructuring object-oriented designs involves estimating the quality of the designs using metrics, and automating transformations that preserve the behavior of the designs. However, these factors have been treated almost independently of each other. A long-term goal is to define transformations preserving the behavior of object-oriented designs, and automate the transformations using metrics. In this paper, we describe a genetic algorithm based restructuring approach using metrics to automatically modify object-oriented designs. Cohesion and coupling metrics based on abstract models are defined to quantify designs and provide criteria for comparing alternative designs. The abstract models include a call-use graph and a class-association graph that represent methods, attributes, classes, and their relationships. The metrics include cohesion, inheritance coupling, and interaction coupling based on the behavioral similarity between methods extracted from the models. We define restructuring operations, and show that the operations preserve the behavior of object-oriented designs. We also devise a fitness function using cohesion and coupling metrics, and automatically restructure object-oriented designs by applying a genetic algorithm using the fitness function.",0,141,24
24,Detecting code smells using machine learning techniques: Are we there yet?,Code Smells; Empirical Studies; Machine Learning; Replication Study,"Code smells are symptoms of poor design and implementation choices weighing heavily on the quality of produced source code. During the last decades several code smell detection tools have been proposed. However, the literature shows that the results of these tools can be subjective and are intrinsically tied to the nature and approach of the detection. In a recent work the use of Machine-Learning (ML) techniques for code smell detection has been proposed, possibly solving the issue of tool subjectivity giving to a learner the ability to discern between smelly and non-smelly source code elements. While this work opened a new perspective for code smell detection, it only considered the case where instances affected by a single type smell are contained in each dataset used to train and test the machine learners. In this work we replicate the study with a different dataset configuration containing instances of more than one type of smell. The results reveal that with this configuration the machine learning techniques reveal critical limitations in the state of the art which deserve further research. {\copyright} 2018 IEEE.",1,12,25
25,Code smell detection: Towards a machine learning-based approach,Code smells detection; Machine learning techniques,"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Usually the detection techniques are based on the computation of different kinds of metrics, and other aspects related to the domain of the system under analysis, its size and other design features are not taken into account. In this paper we propose an approach we are studying based on machine learning techniques. We outline some common problems faced for smells detection and we describe the different steps of our approach and the algorithms we use for the classification. {\copyright} 2013 IEEE.",1,12,26
26,SMURF: A SVM-based incremental anti-pattern detection approach,Anti-pattern; empirical software engineering; program comprehension; program maintenance,"In current, typical software development projects, hundreds of developers work asynchronously in space and time and may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and - or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns incrementally and on subsets of a system could reduce costs, effort, and resources by allowing practitioners to identify and take into account occurrences of anti-patterns as they find them during their development and maintenance activities. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently four limitations: (1) they require extensive knowledge of anti-patterns, (2) they have limited precision and recall, (3) they are not incremental, and (4) they cannot be applied on subsets of systems. To overcome these limitations, we introduce SMURF, a novel approach to detect anti-patterns, based on a machine learning technique - support vector machines - and taking into account practitioners' feedback. Indeed, through an empirical study involving three systems and four anti-patterns, we showed that the accuracy of SMURF is greater than that of DETEX and BDTEX when detecting anti-patterns occurrences. We also showed that SMURF can be applied in both intra-system and inter-system configurations. Finally, we reported that SMURF accuracy improves when using practitioners' feedback. {\copyright} 2012 IEEE.",1,12,27
27,Reducing subjectivity in code smells detection: Experimenting with the Long Method,Binary Logistic Regression; Code smells; Long Method; Refactoring process,"Guidelines for refactoring are meant to improve software systems internal quality and are widely acknowledged as among software's best practices. However, such guidelines remain mostly qualitative in nature. As a result, judgments on how to conduct refactoring processes remain mostly subjective and therefore non-automatable, prone to errors and unrepeatable. The detection of the Long Method code smell is an example. To address this problem, this paper proposes a technique to detect Long Method objectively and automatically, using a Binary Logistic Regression model calibrated by expert's knowledge. The results of an experiment illustrating the use of this technique are reported. {\copyright} 2010 IEEE.",1,12,28
28,Classification model for code clones based on machine learning,Classify; Code clone detector; Filtering; Machine learning,"Results from code clone detectors may contain plentiful useless code clones, but judging whether each code clone is useful varies from user to user based on a user's purpose for the clone. In this research, we propose a classification model that applies machine learning to the judgments of each individual user regarding the code clones. To evaluate the proposed model, 32 participants completed an online survey to test its usability and accuracy. The result showed several important observations on the characteristics of the true positives of code clones for the users. Our classification model showed more than 70 % accuracy on average and more than 90 % accuracy for some particular users and projects. {\copyright} 2014, The Author(s).",1,12,29
29,IDS: An immune-inspired approach for the detection of software design smells,Antipatterns; Artificial immune systems; Code smells; Reverse engineering; System design,"We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly-available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state-of-the-art approaches. {\copyright} 2010 IEEE.",1,12,30
30,Tracking design smells: Lessons from a study of God classes,Design smells; Empirical study; Software evolution,"""God class"" is a term used to describe a certain type of large classes which ""know too much or do too much"". Often a God class (GC) is created by accident as functionalities are incrementally added to a central class over the course of its evolution. GCs are generally thought to be examples of bad code that should be detected and removed to ensure software quality. However, in some cases, a GC is created by design as the best solution to a particular problem because, for example, the problem is not easily decomposable or strong requirements on efficiency exist. In this paper, we study in two open-source systems the ""life cycle"" of GCs: how they arise, how prevalent they are, and whether they remain or they are removed as the systems evolve over time, through a number of versions. We show how to detect the degree of ""godliness"" of classes automatically. Then, we show that by identifying the evolution of ""godliness"", we can distinguish between those classes that are so by design (good code) from those that occurred by accident (bad code). This methodology can guide software quality teams in their efforts to implement prevention and correction mechanisms. {\copyright} 2009 IEEE.",1,12,31
31,The evolution and psychology of self-deception,deception; evolutionary psychology; motivated cognition; self-deception; social psychology,"In this article we argue that self-deception evolved to facilitate interpersonal deception by allowing people to avoid the cues to conscious deception that might reveal deceptive intent. Self-deception has two additional advantages: It eliminates the costly cognitive load that is typically associated with deceiving, and it can minimize retribution if the deception is discovered. Beyond its role in specific acts of deception, self-deceptive self-enhancement also allows people to display more confidence than is warranted, which has a host of social advantages. The question then arises of how the self can be both deceiver and deceived. We propose that this is achieved through dissociations of mental processes, including conscious versus unconscious memories, conscious versus unconscious attitudes, and automatic versus controlled processes. Given the variety of methods for deceiving others, it should come as no surprise that self-deception manifests itself in a number of different psychological processes, and we discuss various types of self-deception. We then discuss the interpersonal versus intrapersonal nature of self-deception before considering the levels of consciousness at which the self can be deceived. Finally, we contrast our evolutionary approach to self-deception with current theories and debates in psychology and consider some of the costs associated with self-deception. {\copyright} 2011 Cambridge University Press.",0,141,32
32,A survey on software fault localization,execution trace; program debugging; Software fault localization; software testing; survey; suspicious code,"Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole. {\copyright} 1976-2012 IEEE.",0,141,33
33,CP-Miner: A Tool for Finding Copy-paste and Related Bugs in Operating System Code,,"Copy-pasted code is very common in large software because programmers prefer reusing code via copy-paste in order to reduce programming effort. Recent studies show that copy-paste is prone to introducing bugs and a significant portion of operating system bugs concentrate in copy-pasted code. Unfortunately, it is challenging to efficiently identify copy-pasted code in large software. Existing copy-paste detection tools are either not scalable to large software, or cannot handle small modifications in copy-pasted code. Furthermore, few tools are available to detect copy-paste related bugs. In this paper we propose a tool, CP-Miner, that uses data mining techniques to efficiently identify copy-pasted code in large software including operating systems, and detects copy-paste related bugs. Specifically, it takes less than 20 minutes for CP-Miner to identify 190,000 copy-pasted segments in Linux and 150,000 in FreeBSD. Moreover, CP-Miner has detected 28 copy-paste related bugs in the latest version of Linux and 23 in FreeBSD. In addition, we analyze some interesting characteristics of copy-paste in Linux and FreeBSD, including the distribution of copy-pasted code across different length, granularity, modules, degrees of modification, and various software versions.",0,141,34
34,A systematic review of machine learning techniques for software fault prediction,Machine learning; Software fault proneness; Systematic literature review,"Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work. {\copyright} 2014 Elsevier B.V. All rights reserved.",0,141,35
35,Foundations of Voice Studies: An Interdisciplinary Approach to Voice Production and Perception,,"Foundations of Voice Studies provides a comprehensive description and analysis of the multifaceted role that voice quality plays in human existence. Offers a unique interdisciplinary perspective on all facets of voice perception, illustrating why listeners hear what they do and how they reach conclusions based on voice quality. Integrates voice literature from a multitude of sources and disciplines. Supplemented with practical and approachable examples, including a companion website with sound files at www.wiley.com/go/voicestudies. Explores the choice of various voices in advertising and broadcasting, and voice perception in singing voices and forensic applications. Provides a straightforward and thorough overview of vocal physiology and control. {\copyright} 2011 Jody Kreiman and Diana Sidtis.",0,141,36
36,What science offers the humanities: Integrating body and culture,,"What Science Offers the Humanities examines some of the deep problems facing current approaches to the study of culture. It focuses on the excesses of postmodernism, but also acknowledges serious problems with postmodernism's harshest critics. In short, Edward Slingerland argues that in order for the humanities to progress, its scholars need to take seriously contributions from the natural sciences - and particular research on human cognition - which demonstrate that any separation of the mind and the body is entirely untenable. The author provides suggestions for how humanists might begin to utilize these scientific discoveries without conceding that science has the last word on morality, religion, art, and literature. Calling into question such deeply entrenched dogmas as the `blank slate' theory of nature, strong social constructivism, and the ideal of disembodied reason, What Science Offers the Humanities replaces the human-sciences divide with a more integrated approach to the study of culture. {\copyright} Edward Slingerland 2008.",0,141,37
37,"Memory, Attention, and Decision-Making: A Unifying Computational Neuroscience Approach",Atention; Computational neuroscience; Decision-making; Emotion; Perception; Short-term memory; Smell; Taste; Vision; Visual object recognition,"This book presents a unified approach to understanding memory, attention, and decision-making. It shows how these fundamental functions for cognitive neuroscience can be understood in a common and unifying computational neuroscience framework. This framework links empirical research on brain function from neurophysiology, functional neuroimaging, and the effects of brain damage, to a description of how neural networks in the brain implement these functions using a set of common principles. The book describes the principles of operation of these networks, and how they could implement such important functions as memory, attention, and decision-making. The book discusses the hippocampus and memory, reward- and punishment-related learning, emotion and motivation, invariant visual object recognition learning, short-term memory, attention, biased competition, probabilistic decision-making, action selection, and decision-making. {\copyright} Edmund T Rolls, 2008. All rights reserved.",0,141,38
38,Software evolution,,"Software has become omnipresent and vital in our information-based society, so all software producers should assume responsibility for its reliability. While ""reliable"" originally assumed implementations that were effective and mainly error-free, additional issues like adaptability and maintainability have gained equal importance recently. For example, the 2004 ACM/IEEE Software Engineering Curriculum Guidelines list software evolution as one of ten key areas of software engineering education.Mens and Demeyer, both international authorities in the field of software evolution, together with the invited contributors, focus on novel trends in software evolution research and its relations with other emerging disciplines such as model-driven software engineering, service-oriented software development, and aspect-oriented software development. They do not restrict themselves to the evolution of source code but also address the evolution of other, equally important software artifacts such as databases and database schemas, design models, software architectures, and process management. The contributing authors provide broad overviews of related work, and they also contribute to a comprehensive glossary, a list of acronyms, and a list of books, journals, websites, standards and conferences that together represent the community's body of knowledge. Combining all these features, this book is the indispensable source for researchers and professionals looking for an introduction and comprehensive overview of the state of the art. In addition, it is an ideal basis for an advanced course on software evolution. {\copyright} Springer-Verlag Berlin Heidelberg 2008.",0,141,39
39,Revisiting the impact of classification techniques on the performance of defect prediction models,,"Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others. {\copyright} 2015 IEEE.",0,141,40
40,Learning NATURAL coding conventions,Coding conventions; Naturalness of software,"Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project's coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94% accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted. Copyright 2014 ACM.",0,141,41
41,Evolution of software in automated production systems: Challenges and research directions,Automated production systems; Automation; Evolution; Software engineering,"Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. {\copyright} 2015 Elsevier Inc. All rights reserved.",0,141,42
42,"User requirements notation: The first ten years, the next ten years",Goal-oriented requirement language (GRL); Goals; Modeling; Review; Scenarios; Tools; Use case maps (UCM); User requirements notation (URN),"The User Requirements Notation (URN), standardized by the International Telecommunication Union in 2008, is used to model and analyze requirements with goals and scenarios. This paper describes the first ten years of development of URN, and discusses ongoing efforts targeting the next ten years. We did a study inspired by the systematic literature review approach, querying five major search engines and using the existing URN Virtual Library. Based on the 281 scientific publications related to URN we collected and analyzed, we observe a shift from a more conventional use of URN for telecommunications and reactive systems to business process management and aspect-oriented modeling, with relevant extensions to the language being proposed. URN also benefits from a global and active research community, although industrial contributions are still sparse. URN is now a leading language for goal-driven and scenario-oriented modeling with a promising future for many application domains. {\copyright} 2011 ACADEMY PUBLISHER.",0,141,43
43,Discovering and representing systematic code changes,,"Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes. {\copyright} 2009 IEEE.",0,141,44
44,Stepping into virtual reality,,"The fruit of many years experience on the creation of synthetic worlds and virtual realities, this book is based on the considerable expertise of the authors, who share their knowledge of mastering the complexities behind the creation of Virtual Reality (VR) applications. The first part of the book reviews the basic theoretical and practical concepts involved in the visual aspect of virtual environments. Part 2 provides more details on the components, structure and types of virtual worlds that can be created, including detailed explanations of the main modeling and animation techniques for virtual characters - one of the most important aspects in a virtual world. A review and discussion of the main types of VR system architectures defines the different alternatives for organizing and designing a VR application. The final part covers the main principles of Virtual Reality hardware using a generic classification of interaction devices based on a human-centered approach (via the five human senses: vision, sound, touch, smell and taste). The book closes with an overview of successful VR systems and applications and gives a glimpse of what lies in the future. This book was conceived as a guided tour and provides practical explanations of each step in the process of creating a Virtual Reality application. It can be used both as textbook for a Virtual Reality course, and as a reference for courses covering computer graphics, computer animation or human-computer interaction topics. {\copyright} Springer-Verlag London Limited 2008.",0,141,45
45,A bayesian approach for the detection of code and design smells,,"The presence of code and design smells can have a severe impact on the quality of a program. Consequently, their detection and correction have drawn the attention of both researchers and practitioners who have proposed various approaches to detect code and design smells in programs. However, none of these approaches handle the inherent uncertainty of the detection process. We propose a Bayesian approach to manage this uncertainty. First, we present a systematic process to convert existing state-of-the-art detection rules into a probabilistic model. We illustrate this process by generating a model to detect occurrences of the Blob antipattern. Second, we present results of the validation of the model: we built this model on two open-source programs, GanttProject v1.10.2 and Xerces v2.7.0, and measured its accuracy. Third, we compare our model with another approach to show that it returns the same candidate classes while ordering them to minimise the quality analysts' effort. Finally, we show that when past detection results are available, our model can be calibrated using machine learning techniques to offer an improved, context-specfic detection. {\copyright} 2009 IEEE.",1,12,46
46,Automated API property inference techniques,API evolution; API property; API usage pattern; data mining; interface; pattern mining; programming rules; protocols; specifications,"Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information. {\copyright} 1976-2012 IEEE.",0,141,47
47,Innovative diagnostic tools for early detection of Alzheimer's disease,Alzheimer's disease; Diagnostic tools; Early detection; Noninvasive tests; Screening tests,"Current state-of-the-art diagnostic measures of Alzheimer's disease (AD) are invasive (cerebrospinal fluid analysis), expensive (neuroimaging) and time-consuming (neuropsychological assessment) and thus have limited accessibility as frontline screening and diagnostic tools for AD. Thus, there is an increasing need for additional noninvasive and/or cost-effective tools, allowing identification of subjects in the preclinical or early clinical stages of AD who could be suitable for further cognitive evaluation and dementia diagnostics. Implementation of such tests may facilitate early and potentially more effective therapeutic and preventative strategies for AD. Before applying them in clinical practice, these tools should be examined in ongoing large clinical trials. This review will summarize and highlight the most promising screening tools including neuropsychometric, clinical, blood, and neurophysiological tests. {\copyright} 2015 The Alzheimer's Association.",0,141,48
48,Revisiting common bug prediction findings using effort-aware models,,"Bug prediction models are often used to help allocate software quality assurance efforts (e.g. testing and code reviews). Mende and Koschke have recently proposed bug prediction models that are effort-aware. These models factor in the effort needed to review or test code when evaluating the effectiveness of prediction models, leading to more realistic performance evaluations. In this paper, we revisit two common findings in the bug prediction literature: 1) Process metrics (e.g., change history) outperform product metrics (e.g., LOC), 2) Packagelevel predictions outperform file-level predictions. Through a case study on three projects from the Eclipse Foundation, we find that the first finding holds when effort is considered, while the second finding does not hold. These findings validate the practical significance of prior findings in the bug prediction literature and encourage their adoption in practice. {\copyright} 2010 IEEE.",0,141,49
49,Subjective evaluation of software evolvability using code smells: An empirical study,Code metrics; Code smells; Evolvability; Human factors; Maintainability; Perceived evaluation; Software metrics; Subjective evaluation,"This paper presents the results of an empirical study on the subjective evaluation of code smells that identify poorly evolvable structures in software. We propose use of the term software evolvability to describe the ease of further developing a piece of software and outline the research area based on four different viewpoints. Furthermore, we describe the differences between human evaluations and automatic program analysis based on software evolvability metrics. The empirical component is based on a case study in a Finnish software product company, in which we studied two topics. First, we looked at the effect of the evaluator when subjectively evaluating the existence of smells in code modules. We found that the use of smells for code evaluation purposes can be difficult due to conflicting perceptions of different evaluators. However, the demographics of the evaluators partly explain the variation. Second, we applied selected source code metrics for identifying four smells and compared these results to the subjective evaluations. The metrics based on automatic program analysis and the human-based smell evaluations did not fully correlate. Based upon our results, we suggest that organizations should make decisions regarding software evolvability improvement based on a combination of subjective evaluations and code metrics. Due to the limitations of the study we also recognize the need for conducting more refined studies and experiments in the area of software evolvability.",0,141,50
50,Individual variation in resisting temptation: Implications for addiction,Accumbens; Binge eating; Dopamine; Goal-tracking; Human; Individual differences; Learning; Motivation; Obesity; Pavlovian; Rat; Relapse; Sign-tracking,"When exposed to the sights, sounds, smells and/or places that have been associated with rewards, such as food or drugs, some individuals have difficulty resisting the temptation to seek out and consume them. Others have less difficulty restraining themselves. Thus, Pavlovian reward cues may motivate maladaptive patterns of behavior to a greater extent in some individuals than in others. We are just beginning to understand the factors underlying individual differences in the extent to which reward cues acquire powerful motivational properties, and therefore, the ability to act as incentive stimuli. Here we review converging evidence from studies in both human and non-human animals suggesting that a subset of individuals are more ""cue reactive"", in that certain reward cues are more likely to attract these individuals to them and motivate actions to get them. We suggest that those individuals for whom Pavlovian reward cues become especially powerful incentives may be more vulnerable to impulse control disorders, such as binge eating and addiction. {\copyright} 2013 Elsevier Ltd.",0,141,51
51,Neuromarketing: Exploring the brain of the consumer,,"Over the last 10 years advances in the new field of neuromarketing have yielded a host of findings which defy common stereotypes about consumer behavior. Reason and emotions do not necessarily appear as opposing forces. Rather, they complement one another. Hence, it reveals that consumers utilize mental accounting processes different from those assumed in marketers' logical inferences when it comes to time, problems with rating and choosing, and in post-purchase evaluation. People are often guided by illusions not only when they perceive the outside world but also when planning their actions - and consumer behavior is no exception. Strengthening the control over their own desires and the ability to navigate the maze of data are crucial skills consumers can gain to benefit themselves, marketers and the public. Understanding the mind of the consumer is the hardest task faced by business researchers. This book presents the first analytical perspective on the brain - and biometric studies which open a new frontier in market research. {\copyright} Springer-Verlag Berlin Heidelberg 2010.",0,141,52
52,Comparing and experimenting machine learning techniques for code smell detection,Benchmark for code smell detection; Code smells detection; Machine learning techniques,"Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (>96 %), and only a hundred training examples are needed to reach at least 95 % accuracy. {\copyright} 2015, Springer Science+Business Media New York.",1,12,53
53,A quantitative investigation of the acceptable risk levels of object-oriented metrics in open-source systems,CK metrics; Object-oriented programming; Open-source software; Product metrics; Threshold values,"Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source systemthe-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects. {\copyright} 2006 IEEE.",0,141,54
54,Software defect prediction using relational association rule mining,Association rule; Data mining; Defect prediction; Software engineering,"This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. {\copyright} 2014 Elsevier Inc. All rights reserved.",0,141,55
55,Maintainability defects detection and correction: A multi-objective approach,By example; Effort; Maintainability defects; Multi-objective optimization; Search-based software engineering; Software maintenance,"Software defects often lead to bugs, runtime errors and software maintenance difficulties. They should be systematically prevented, found, removed or fixed all along the software lifecycle. However, detecting and fixing these defects is still, to some extent, a difficult, time-consuming and manual process. In this paper, we propose a two-step automated approach to detect and then to correct various types of maintainability defects in source code. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. Then, we correct the detected defects while minimizing the correction effort. A correction solution is defined as the combination of refactoring operations that should maximize as much as possible the number of corrected defects with minimal code modification effort. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise. For six open source projects, we succeeded in detecting the majority of known defects, and the proposed corrections fixed most of them with minimal effort. {\copyright} 2012 Springer Science+Business Media, LLC.",0,141,56
56,On the ability of complexity metrics to predict fault-prone classes in object-oriented systems,Complexity; Concordant pairs; Fault-prone; Logistic regression; Metrics; Odds ratio; Prediction,"Many studies use logistic regression models to investigate the ability of complexity metrics to predict fault-prone classes. However, it is not uncommon to see the inappropriate use of performance indictors such as odds ratio in previous studies. In particular, a recent study by Olague et al. uses the odds ratio associated with one unit increase in a metric to compare the relative magnitude of the associations between individual metrics and fault-proneness. In addition, the percents of concordant, discordant, and tied pairs are used to evaluate the predictive effectiveness of a univariate logistic regression model. Their results suggest that lesser known complexity metrics such as standard deviation method complexity (SDMC) and average method complexity (AMC) are better predictors than the two commonly used metrics: lines of code (LOC) and weighted method McCabe complexity (WMC). In this paper, however, we show that (1) the odds ratio associated with one standard deviation increase, rather than one unit increase, in a metric should be used to compare the relative magnitudes of the effects of individual metrics on fault-proneness. Otherwise, misleading results may be obtained; and that (2) the connection of the percents of concordant, discordant, and tied pairs with the predictive effectiveness of a univariate logistic regression model is false, as they indeed do not depend on the model. Furthermore, we use the data collected from three versions of Eclipse to re-examine the ability of complexity metrics to predict fault-proneness. Our experimental results reveal that: (1) many metrics exhibit moderate or almost moderate ability in discriminating between fault-prone and not fault-prone classes; (2) LOC and WMC are indeed better fault-proneness predictors than SDMC and AMC; and (3) the explanatory power of other complexity metrics in addition to LOC is limited. {\copyright} 2009 Elsevier Inc. All rights reserved.",0,141,57
57,International Consensus Statement on Allergy and Rhinology: Allergic Rhinitis,allergen extract; allergen immunotherapy; allergic rhinitis; allergy; antihistamine; asthma; atopic dermatitis; avoidance; biologic; cockroach; conjunctivitis; consensus; corticosteroid; cough; cromolyn; decongestant; environment; eosinophilic esophagitis; epicutaneous immunotherapy; epidemiology; evidence-based medicine; food allergy; genetics; house dust mite; IgE; immunoglobulin E; immunotherapy; inhalant allergy; leukotriene; microbiome; occupational rhinitis; omalizumab; pathophysiology; perennial; pet dander; pollen; probiotic; quality of life; rhinitis; rhinitis; rhinosinusitis; risk factor; saline; seasonal; sensitization; sinusitis; sleep; socioeconomic; specific IgE; subcutaneous immunotherapy; sublingual immunotherapy; systematic review; total IgE; transcutaneous immunotherapy; validated survey,"Background: Critical examination of the quality and validity of available allergic rhinitis (AR) literature is necessary to improve understanding and to appropriately translate this knowledge to clinical care of the AR patient. To evaluate the existing AR literature, international multidisciplinary experts with an interest in AR have produced the International Consensus statement on Allergy and Rhinology: Allergic Rhinitis (ICAR:AR). Methods: Using previously described methodology, specific topics were developed relating to AR. Each topic was assigned a literature review, evidence-based review (EBR), or evidence-based review with recommendations (EBRR) format as dictated by available evidence and purpose within the ICAR:AR document. Following iterative reviews of each topic, the ICAR:AR document was synthesized and reviewed by all authors for consensus. Results: The ICAR:AR document addresses over 100 individual topics related to AR, including diagnosis, pathophysiology, epidemiology, disease burden, risk factors for the development of AR, allergy testing modalities, treatment, and other conditions/comorbidities associated with AR. Conclusion: This critical review of the AR literature has identified several strengths; providers can be confident that treatment decisions are supported by rigorous studies. However, there are also substantial gaps in the AR literature. These knowledge gaps should be viewed as opportunities for improvement, as often the things that we teach and the medicine that we practice are not based on the best quality evidence. This document aims to highlight the strengths and weaknesses of the AR literature to identify areas for future AR research and improved understanding. {\copyright} 2018 ARS-AAOA, LLC",0,141,58
58,Imaging brain function with EEG: Advanced temporal and spatial analysis of electroencephalographic signals,,"The scalp and cortex lie like pages of an open book on which the cortex enciphers vast quantities of information and knowledge. They are recorded and analyzed as temporal and spatial patterns in the electroencephalogram and electrocorticogram. This book describes basic tools and concepts needed to measure and decipher the patterns extracted from the EEG and ECoG. This book emphasizes the need for single trial analysis using new methods and paradigms, as well as large, high-density spatial arrays of electrodes for pattern sampling. The deciphered patterns reveal neural mechanisms by which brains process sensory information into precepts and concepts. It describes the brain as a thermodynamic system that uses chemical energy to construct knowledge. The results are intended for use in the search for the neural correlates of intention, attention, perception and learning; in the design of human brain-computer interfaces enabling mental control of machines; and in exploring and explaining the physicochemical foundation of biological intelligence. {\copyright} Springer Science+Business Media New York 2013. All rights are reserved.",0,141,59
59,Short-term forecasting of high-speed rail demand: A hybrid approach combining ensemble empirical mode decomposition and gray support vector machine with real-world applications in China,Demand forecasting; Ensemble empirical mode decomposition (EEMD); Grey support vector machine (GSVM); High-speed rail (HSR); Hybrid model,"Short-term forecasting of high-speed rail (HSR) passenger flow provides daily ridership estimates that account for day-to-day demand variations in the near future (e.g., next week, next month). It is one of the most critical tasks in high-speed passenger rail planning, operational decision-making and dynamic operation adjustment. An accurate short-term HSR demand prediction provides a basis for effective rail revenue management. In this paper, a hybrid short-term demand forecasting approach is developed by combining the ensemble empirical mode decomposition (EEMD) and grey support vector machine (GSVM) models. There are three steps in this hybrid forecasting approach: (i) decompose short-term passenger flow data with noises into a number of intrinsic mode functions (IMFs) and a trend term; (ii) predict each IMF using GSVM calibrated by the particle swarm optimization (PSO); (iii) reconstruct the refined IMF components to produce the final predicted daily HSR passenger flow, where the PSO is also applied to achieve the optimal refactoring combination. This innovative hybrid approach is demonstrated with three typical origin-destination pairs along the Wuhan-Guangzhou HSR in China. Mean absolute percentage errors of the EEMD-GSVM predictions using testing sets are 6.7%, 5.1% and 6.5%, respectively, which are much lower than those of two existing forecasting approaches (support vector machine and autoregressive integrated moving average). Application results indicate that the proposed hybrid forecasting approach performs well in terms of prediction accuracy and is especially suitable for short-term HSR passenger flow forecasting. {\copyright} 2014 Elsevier Ltd.",0,141,60
60,Some code smells have a significant but small effect on faults,Defects; Software code smells,"We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness. {\copyright} 2014 ACM.",0,141,61
61,Self-organization in the olfactory system: One shot odor recognition in insects,Fan-in; Fan-out; Information coding; Olfaction; Pattern recognition; Synaptic convergence,"We show in a model of spiking neurons that synaptic plasticity in the mushroom bodies in combination with the general fan-in, fan-out properties of the early processing layers of the olfactory system might be sufficient to account for its efficient recognition of odors. For a large variety of initial conditions the model system consistently finds a working solution without any fine-tuning, and is, therefore, inherently robust. We demonstrate that gain control through the known feedforward inhibition of lateral horn interneurons increases the capacity of the system but is not essential for its general function. We also predict an upper limit for the number of odor classes Drosophila can discriminate based on the number and connectivity of its olfactory neurons.",0,141,62
62,An empirical study of just-in-time defect prediction using cross-project models,Empirical study; Software quality,"Prior research suggests that predicting defect-inducing changes, i.e., Just-In-Time (JIT) defect prediction is a more practical alternative to traditional defect prediction techniques, providing immediate feedback while design decisions are still fresh in the minds of developers. Unfortunately, similar to traditional defect prediction models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this flaw in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from older projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT cross-project models. Through a case study on 11 open source projects, we find that in a JIT cross-project context: (1) high performance within-project models rarely perform well; (2) models trained on projects that have similar correlations between predictor and dependent variables often perform well; and (3) ensemble learning techniques that leverage historical data from several other projects (e.g., voting experts) often perform well. Our findings empirically confirm that JIT cross-project models learned using other projects are a viable solution for projects with little historical data. However, JIT cross-project models perform best when the data used to learn them is carefully selected. Copyright 2014 ACM.",0,141,63
63,Why Red Doesn't Sound Like A Bell: Understanding the feel of consciousness,Color perception; Feeling; Hearing; Seeing; Sensorimotor approach; Sensory experiences; Touch; Visual perception; Visual sensation,"This book proposes a novel view to explain how we as humans-contrary to current robots-can have the impression of consciously feeling things: for example the red of a sunset, the smell of a rose, the sound of a symphony, or a pain. The book starts off by looking at visual perception. Our ability to see turns out to be much more mysterious than one might think. The eye contains many defects which should seriously interfere with vision. Yet we have the impression of seeing the world in glorious panavision and technicolor. Explaining how this can be the case leads to a new idea about what seeing really is. Seeing is not passively receiving information in the brain, but rather a way of interacting with the world. The role of the brain is not to create visual sensation, but to enable the necessary interactions with the world. This new approach to seeing is extended in the second part of the book to encompass the other senses: hearing, touch, taste, and smell. Taking sensory experiences to be modes of interacting with the world explains why these experiences are different in the way they are. It also explains why thoughts or automatic functions in the body, and indeed the vast majority brain functions, are not accompanied by any real feeling. The ""sensorimotor"" approach is not simply a philosophical argument: It leads to scientifically verifiable predictions and new research directions. Among these are the phenomena of change blindness, sensory substitution, ""looked but failed to see"", as well as results on color naming and color perception and the localization of touch on the body. {\copyright} 2011 by Oxford University Press, Inc. All rights reserved.",0,141,64
64,Artificial immune multi-objective SAR image segmentation with fused complementary features,Artificial immune system; Clustering validity indices; Evolutionary computation; Feature fusion; Gabor filter; Gray level co-occurrence probability; Multi-objective optimization (MO); Single-objective optimization (SO),"Artificial immune systems (AIS) are the computational systems inspired by the principles and processes of the vertebrate immune system. AIS-based algorithms typically mimic the human immune system's characteristics of learning and adaptability to solve some complicated problems. Here, an artificial immune multi-objective optimization framework is formulated and applied to synthetic aperture radar (SAR) image segmentation. The important innovations of the framework are listed as follows: (1) an efficient and robust immune, multi-objective optimization algorithm is proposed, which has the features of adaptive rank clones and diversity maintenance by K-nearest-neighbor list; (2) besides, two conflicting, fuzzy clustering validity indices are incorporated into this framework and optimized simultaneously and (3) moreover, an effective, fused feature set for texture representation and discrimination is constructed and researched, which utilizes both the Gabor filter's ability to precisely extract texture features in low- and mid-frequency components and the gray level co-occurrence probability's (GLCP) ability to measure information in high-frequency. Two experiments with synthetic texture images and SAR images are implemented to evaluate the performance of the proposed framework in comparison with other five clustering algorithms: fuzzy C-means (FCM), single-objective genetic algorithm (SOGA), self-organizing map (SOM), wavelet-domain hidden Markov models (HMTseg), and spectral clustering ensemble (SCE). Experimental results show the proposed framework has obtained the better performance in segmenting SAR images than other five algorithms and behaves insensitive to the speckle noise. {\copyright} 2011 Elsevier Inc. All rights reserved.",0,141,65
65,Static analysis of android apps: A systematic literature review,,"Context Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps. Objective We aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed. Method We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011--December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed. Results Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available. Conclusion The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers. {\copyright} 2017 Elsevier B.V.",0,141,66
66,Studying just-in-time defect prediction using cross-project models,Defect prediction; Empirical study; Just-in-time prediction,"Unlike traditional defect prediction models that identify defect-prone modules, Just-In-Time (JIT) defect prediction models identify defect-inducing changes. As such, JIT defect models can provide earlier feedback for developers, while design decisions are still fresh in their minds. Unfortunately, similar to traditional defect models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this limitation in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from other projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT models in a cross-project context. Through an empirical study on 11 open source projects, we find that while JIT models rarely perform well in a cross-project context, their performance tends to improve when using approaches that: (1) select models trained using other projects that are similar to the testing project, (2) combine the data of several other projects to produce a larger pool of training data, and (3) combine the models of several other projects to produce an ensemble model. Our findings empirically confirm that JIT models learned using other projects are a viable solution for projects with limited historical data. However, JIT models tend to perform best in a cross-project context when the data used to learn them are carefully selected. {\copyright} 2015, Springer Science+Business Media New York.",0,141,67
67,How do code refactorings affect energy usage?,code refactoring; empirical study; energy usage,"Context: Code refactoring's benefits to understandability, maintainability and extensibility are well known enough that automated support for refactoring is now common in IDEs. However, the decision to apply such transformations is currently performed without regard to the impacts of the refactorings on energy consumption. This is primarily due to a lack of information and tools to provide such relevant information to developers. Unfortunately, concerns about energy efficiency are rapidly becoming a high priority concern in many environments, including embedded systems, laptops, mobile devices, and data centers. Goal: We aim to address the lack of information about the energy efficiency impacts of code refactorings. Method: We conducted an empirical study to investigate the energy impacts of 197 applications of 6 commonly-used refactorings. Results: We found that refactorings can not only impact energy usage but can also increase and decrease the amount of energy used by an application. In addition, we also show that metrics commonly believed to correlate with energy usage are unlikely to be able to fully predict the impact of applying a refactoring. Conclusion: The results from this and similar studies could be used to augment IDEs to help software developers build more energy efficient software. {\copyright} 2014 ACM.",0,141,68
68,A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection,code-smells; distributed evolutionary algorithms; Search-based software engineering; software quality,"We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells. {\copyright} 2014 IEEE.",0,141,69
69,Real-Time Systems Design and Analysis: Tools for the Practitioner,,"The leading text in the field explains step by step how to write software that responds in real time From power plants to medicine to avionics, the world increasingly depends on computer systems that can compute and respond to various excitations in real time. The Fourth Edition of Real-Time Systems Design and Analysis gives software designers the knowledge and the tools needed to create real-time software using a holistic, systems-based approach. The text covers computer architecture and organization, operating systems, software engineering, programming languages, and compiler theory, all from the perspective of real-time systems design. The Fourth Edition of this renowned text brings it thoroughly up to date with the latest technological advances and applications. This fully updated edition includes coverage of the following concepts: Multidisciplinary design challenges Time-triggered architectures Architectural advancements Automatic code generation Peripheral interfacing Life-cycle processes The final chapter of the text offers an expert perspective on the future of real-time systems and their applications. The text is self-contained, enabling instructors and readers to focus on the material that is most important to their needs and interests. Suggestions for additional readings guide readers to more in-depth discussions on each individual topic. In addition, each chapter features exercises ranging from simple to challenging to help readers progressively build and fine-tune their ability to design their own real-time software programs. Now fully up to date with the latest technological advances and applications in the field, Real-Time Systems Design and Analysis remains the top choice for students and software engineers who want to design better and faster real-time systems at minimum cost. {\copyright} 2012 the Institute of Electrical and Electronics Engineers, Inc.",0,141,70
70,Network versus code metrics to predict defects: A replication study,Code metrics; Defect prediction; Network metrics; Open-source; Replication study,"Several defect prediction models have been proposed to identify which entities in a software system are likely to have defects before its release. This paper presents a replication of one such study conducted by Zimmermann and Nagappan [1] on Windows Server 2003 where the authors leveraged dependency relationships between software entities captured using social network metrics to predict whether they are likely to have defects. They found that network metrics perform significantly better than source code metrics at predicting defects. In order to corroborate the generality of their findings, we replicate their study on three open source Java projects, viz., JRuby, ArgoUML, and Eclipse. Our results are in agreement with the original study by Zimmermann and Nagappan when using a similar experimental setup as them (random sampling). However, when we evaluated the metrics using setups more suited for industrial use - forward-release and cross-project prediction - we found network metrics to offer no vantage over code metrics. Moreover, code metrics may be preferable to network metrics considering the data is easier to collect and we used only 8 code metrics compared to approximately 58 network metrics. {\copyright} 2011 IEEE.",0,141,71
71,A survey on the use of topic models when mining software repositories,LDA; LSI; Survey; Topic modeling,"Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task. {\copyright} 2015, Springer Science+Business Media New York.",0,141,72
72,Optical-fiber arrays for vapor sensing,Artificial nose; Fluorescence; Microarray; Optical fiber; Vapor sensing,This paper reviews the use of optical fibers as a platform to fabricate vapor sensitive arrays that have been developed in the authors' laboratory in recent years. Two types of fluorescence-based optical-fiber array systems are described: polymer-coated single core optical fibers and imaging fiber-optic bundles incorporating functionalized microspheres. Each system responds to an analyte vapor in a cross-reactive manner generating a multi-dimensional signal that can be used to train a pattern recognition program to identify subsequent exposures of the array to learned vapors. Several applications are described including explosives and nerve agent detection as well as the detection of several volatile organic compounds and ignitable liquids. {\copyright} 2009 Elsevier B.V. All rights reserved.,0,141,73
73,"Is software ""green""? Application development environments and energy efficiency in open source applications",Green IT; Software development application environment; Software energy efficiency,"Context: The energy efficiency of IT systems, also referred to as Green IT, is attracting more and more attention. While several researchers have focused on the energy efficiency of hardware and embedded systems, the role of application software in IT energy consumption still needs investigation. Objective: This paper aims to define a methodology for measuring software energy efficiency and to understand the consequences of abstraction layers and application development environments for the energy efficiency of software applications. Method: We first develop a measure of energy efficiency that is appropriate for software applications. We then examine how the use of application development environments relates to this measure of energy efficiency for a sample of 63 open source software applications. Results: Our findings indicate that a greater use of application development environments - specifically, frameworks and external libraries - is more detrimental in terms of energy efficiency for larger applications than for smaller applications. We also find that different functional application types have distinctly different levels of energy efficiency, with text and image editing and gaming applications being the most energy inefficient due to their intense use of the processor. Conclusion: We conclude that different designs can have a significant impact on the energy efficiency of software applications. We have related the use of software application development environments to software energy efficiency suggesting that there may be a trade-off between development efficiency and energy efficiency. We propose new research to further investigate this topic. {\copyright} 2011 Elsevier B.V. All rights reserved.",0,141,74
74,A procedure to detect problems of processes in software development projects using Bayesian networks,Bayesian networks; Software development project; Software process management; Software process simulation modeling,"There are several software process models and methodologies such as waterfall, spiral and agile. Even so, the rate of successful software development projects is low. Since software is the major output of software processes, increasing software process management quality should increase the project's chances of success. Organizations have invested to adapt software processes to their environments and the characteristics of projects to improve the productivity and quality of the products. In this paper, we present a procedure to detect problems of processes in software development projects using Bayesian networks. The procedure was successfully applied to Scrum-based software development projects. The research results should encourage the usage of Bayesian networks to manage software processes and increase the rate of successful software development projects. {\copyright} 2014 Elsevier Ltd. All rights reserved.",0,141,75
75,Mining software evolution to predict refactoring,,"Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, propositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects. {\copyright} 2007 IEEE.",0,141,76
76,A review-based comparative study of bad smell detection tools,Bad smells; Comparative study; Detection tools; Systematic literature review,"Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools. {\copyright} 2016 ACM.",0,141,77
77,Empirical evidence on the link between object-oriented measures and external quality attributes: A systematic literature review,Object-oriented system; Software metrics; Software quality; Source code analysis; Source code measures; Static analysis; Systematic literature review,"There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The least investigated attributes were: effectiveness and functionality. Measures from the C&K measurement suite were the most popular across studies. Votecounting results suggest that complexity, cohesion, size and coupling measures have a better link with reliability and maintainability than inheritance measures. However, inheritance measures should not be overlooked during quality assessment initiatives; their link with reliability and maintainability could be context dependent. There were too few studies traced to effectiveness and functionality attributes; thus a meaningful vote-counting analysis could not be conducted for these attributes. Thus, there is a need for diversification of quality attributes investigated in empirical studies. This would help with identifying useful measures during quality assessment initiatives, and not just for reliability and maintainability aspects. {\copyright} Springer Science+Business Media New York 2014.",0,141,78
78,Statistical learning approach for mining API usage mappings for code migration,API mappings; API usages; Code migration; Statistical learning,"The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C#. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy. {\copyright} 2014 ACM.",0,141,79
79,A review of code smell mining techniques,code quality; code smells; design flaws; detection techniques; literature review,"Over the past 15 years, researchers presented numerous techniques and tools for mining code smells. It is imperative to classify, compare, and evaluate existing techniques and tools used for the detection of code smells because of their varying features and outcomes. This paper presents an up-to-date review on the state-of-the-art techniques and tools used for mining code smells from the source code of different software applications. We classify selected code smell detection techniques and tools based on their detection methods and analyze the results of the selected techniques. We present our observations and recommendations after our critical analysis of existing code smell techniques and tools. Our recommendations may be used by existing and new tool developers working in the field of code smell detection. The scope of this review is limited to research publications in the area of code smells that focus on detection of code smells as compared with previous reviews that cover all aspects of code smells. Copyright {\copyright} 2015 John Wiley & Sons, Ltd.",0,141,80
80,Software mutational robustness,Genetic programming; Mutation testing; Mutational robustness; N-version programming; Neutral landscapes; Proactive diversity,"Neutral landscapes and mutational robustness are believed to be important enablers of evolvability in biology. We apply these concepts to software, defining mutational robustness to be the fraction of random mutations to program code that leave a program's behavior unchanged. Test cases are used to measure program behavior and mutation operators are taken from earlier work on genetic programming. Although software is often viewed as brittle, with small changes leading to catastrophic changes in behavior, our results show surprising robustness in the face of random software mutations. The paper describes empirical studies of the mutational robustness of 22 programs, including 14 production software projects, the Siemens benchmarks, and four specially constructed programs. We find that over 30 % of random mutations are neutral with respect to their test suite. The results hold across all classes of programs, for mutations at both the source code and assembly instruction levels, across various programming languages, and bear only a limited relation to test suite coverage. We conclude that mutational robustness is an inherent property of software, and that neutral variants (i.e., those that pass the test suite) often fulfill the program's original purpose or specification. Based on these results, we conjecture that neutral mutations can be leveraged as a mechanism for generating software diversity. We demonstrate this idea by generating a population of neutral program variants and showing that the variants automatically repair latent bugs. Neutral landscapes also provide a partial explanation for recent results that use evolutionary computation to automatically repair software bugs. {\copyright} 2013 Springer Science+Business Media New York.",0,141,81
81,Support vector machines for anti-pattern detection,Anti-pattern; Empirical software engineering; Program comprehension; Program maintenance,"Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and-or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a is important to ease the maintenance of software. Detecting anti-patterns could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique- support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX. Copyright 2012 ACM.",0,141,82
82,A bayesian network based approach for change coupling prediction,,"Source code coupling and change history are two important data sources for change coupling analysis. The popularity of public open source projects in recent years makes both sources available. Based on our previous research, in this paper, we inspect different dimensions of software changes including change significance or source code dependency levels, extract a set of features from the two sources and propose a bayesian network-based approach for change coupling prediction. By combining the features from the co-changed entities and their dependency relation, the approach can model the underlying uncertainty. The empirical case study on two medium-sized open source projects demonstrates the feasibility and effectiveness of our approach compared to previous work. {\copyright} 2008 IEEE.",0,141,83
83,Taste quality decoding parallels taste sensations,,"In most species, the sense of taste is key in the distinction of potentially nutritious and harmful food constituents and thereby in the acceptance (or rejection) of food. Taste quality is encoded by specialized receptors on the tongue, which detect chemicals corresponding to each of the basic tastes (sweet, salty, sour, bitter, and savory [1]), before taste quality information is transmitted via segregated neuronal fibers [2], distributed coding across neuronal fibers [3], or dynamic firing patterns [4] to the gustatory cortex in the insula. In rodents, both hardwired coding by labeled lines [2] and flexible, learning-dependent representations [5] and broadly tuned neurons [6] seem to coexist. It is currently unknown how, when, and where taste quality representations are established in the cortex and whether these representations are used for perceptual decisions. Here, we show that neuronal response patterns allow to decode which of four tastants (salty, sweet, sour, and bitter) participants tasted in a given trial by using time-resolved multivariate pattern analyses of large-scale electrophysiological brain responses. The onset of this prediction coincided with the earliest taste-evoked responses originating from the insula and opercular cortices, indicating that quality is among the first attributes of a taste represented in the central gustatory system. These response patterns correlated with perceptual decisions of taste quality: tastes that participants discriminated less accurately also evoked less discriminated brain response patterns. The results therefore provide the first evidence for a link between taste-related decision-making and the predictive value of these brain response patterns. {\copyright}2015 Elsevier Ltd All rights reserved.",0,141,84
84,How changes affect software entropy: An empirical study,Mining software repositories; Software complexity; Software entropy,"Context Software systems continuously change for various reasons, such as adding new features, fixing bugs, or refactoring. Changes may either increase the source code complexity and disorganization, or help to reducing it. Aim This paper empirically investigates the relationship of source code complexity and disorganization - measured using source code change entropy - with four factors, namely the presence of refactoring activities, the number of developers working on a source code file, the participation of classes in design patterns, and the different kinds of changes occurring on the system, classified in terms of their topics extracted from commit notes. Method We carried out an exploratory study on an interval of the life-time span of four open source systems, namely ArgoUML, Eclipse-JDT, Mozilla, and Samba, with the aim of analyzing the relationship between the source code change entropy and four factors: refactoring activities, number of contributors for a file, participation of classes in design patterns, and change topics. Results The study shows that (i) the change entropy decreases after refactoring, (ii) files changed by a higher number of developers tend to exhibit a higher change entropy than others, (iii) classes participating in certain design patterns exhibit a higher change entropy than others, and (iv) changes related to different topics exhibit different change entropy, for example bug fixings exhibit a limited change entropy while changes introducing new features exhibit a high change entropy. Conclusions Results provided in this paper indicate that the nature of changes (in particular changes related to refactorings), the software design, and the number of active developers are factors related to change entropy. Our findings contribute to understand the software aging phenomenon and are preliminary to identifying better ways to contrast it. {\copyright} 2012 Springer Science+Business Media, LLC.",0,141,85
85,Code-smell detection as a bilevel problem,,"Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the out performance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems. {\copyright} 2014 ACM.",0,141,86
86,Code anomalies flock together,,"Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies-popularly known as code smells-may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to ""flock together"" to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%. {\copyright} 2016 ACM.",0,141,87
87,Identifying and summarizing systematic code changes via rule inference,Logic-based program representation; Program differencing; Rule learning; Software evolution,"Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company. {\copyright} 2013 IEEE.",0,141,88
88,Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt,empirical study; natural language processing; source code comments; Technical debt,"The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-Admitted technical debt), and that the most common types of self-Admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-Admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-Admitted technical debt, significantly outperforming the current state-of-The-Art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-Admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-Admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset. {\copyright} 1976-2012 IEEE.",0,141,89
89,Metaphor wars: Conceptual metaphors in human life,,"The study of metaphor is now firmly established as a central topic within cognitive science and the humanities. We marvel at the creative dexterity of gifted speakers and writers for their special talents in both thinking about certain ideas in new ways, and communicating these thoughts in vivid, poetic forms. Yet metaphors may not only be special communicative devices, but a fundamental part of everyday cognition in the form of 'conceptual metaphors'. An enormous body of empirical evidence from cognitive linguistics and related disciplines has emerged detailing how conceptual metaphors underlie significant aspects of language, thought, cultural and expressive action. Despite its influence and popularity, there have been major criticisms of conceptual metaphor. This book offers an evaluation of the arguments and empirical evidence for and against conceptual metaphors, much of which scholars on both sides of the wars fail to properly acknowledge. {\copyright} Raymond W. Gibbs, Jr. 2017. All rights reserved.",0,141,90
90,Adaptive detection of design flaws,Code smell; Design flaw; Machine learning; Object-oriented design; Program analysis; Refactoring; Software quality,"Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid ""bad"" - incomprehensible, hardly expandable and changeable - program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study. {\copyright} 2005 Elsevier B.V. All rights reserved.",1,12,91
91,Community detection in networks: A multidisciplinary review,Anomaly detection; Clustering algorithms; Community detection; Modularity; Online social networks,"The modern science of networks has made significant advancement in the modeling of complex real-world systems. One of the most important features in these networks is the existence of community structure. In recent years, many community detection algorithms have been proposed to unveil the structural properties and dynamic behaviors of networks. In this study, we attempt a contemporary survey on the methods of community detection and its applications in the various domains of real life. Besides highlighting the strengths and weaknesses of each community detection approach, different aspects of algorithmic performance comparison and their testing on standard benchmarks are discussed. The challenges faced by community detection algorithms, open issues and future trends related to community detection are also postulated. The main goal of this paper is to put forth a review of prevailing community detection algorithms that range from traditional algorithms to state of the art algorithms for overlapping community detection. Algorithms based on dimensionality reduction techniques such as non-negative matrix factorization (NMF) and principal component analysis (PCA) are also focused. This study will serve as an up-to-date report on the evolution of community detection and its potential applications in various domains from real world networks. {\copyright} 2018 Elsevier Ltd",0,141,92
92,Detecting software design defects using relational association rule mining,Association rule mining; Data mining; Defect detection; Machine learning; Software design,"In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. {\copyright} 2014, Springer-Verlag London.",0,141,93
93,Bayesian networks for evidence-based decision-making in software engineering,Bayesian networks; Bayesian statistics; Evidence-based decision-making; post-release defects; software metrics; software reliability,"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making. {\copyright} 2014 IEEE.",0,141,94
94,Studying evolving software ecosystems based on ecological models,,"Research on software evolution is very active, but evolutionary principles, models and theories that properly explain why and how software systems evolve over time are still lacking. Similarly, more empirical research is needed to understand how different software projects co-exist and co-evolve, and how contributors collaborate within their encompassing software ecosystem. In this chapter, we explore the differences and analogies between natural ecosystems and biological evolution on the one hand, and software ecosystems and software evolution on the other hand. The aim is to learn from research in ecology to advance the understanding of evolving software ecosystems. Ultimately, we wish to use such knowledge to derive diagnostic tools aiming to predict survival of software projects within their ecosystem, to analyse and optimise the fitness of software projcts in their environment, and to help software project communities in managing their projects better. {\copyright} Springer-Verlag Berlin Heidelberg 2014.",0,141,95
95,A survey on software smells,Antipatterns; Code smells; Maintainability; Smell detection tools; Software quality; Software smells; Technical debt,"Context: Smells in software systems impair software quality and make them hard to maintain and evolve. The software engineering community has explored various dimensions concerning smells and produced extensive research related to smells. The plethora of information poses challenges to the community to comprehend the state-of-the-art tools and techniques. Objective: We aim to present the current knowledge related to software smells and identify challenges as well as opportunities in the current practices. Method: We explore the definitions of smells, their causes as well as effects, and their detection mechanisms presented in the current literature. We studied 445 primary studies in detail, synthesized the information, and documented our observations. Results: The study reveals five possible defining characteristics of smells --- indicator, poor solution, violates best-practices, impacts quality, and recurrence. We curate ten common factors that cause smells to occur including lack of skill or awareness and priority to features over quality. We classify existing smell detection methods into five groups --- metrics, rules/heuristics, history, machine learning, and optimization-based detection. Challenges in the smells detection include the tools' proneness to false-positives and poor coverage of smells detectable by existing tools. {\copyright} 2017",0,141,96
96,Olfactory receptor subgenomes linked with broad ecological adaptations in sauropsida,adaptation; birds; olfactory receptors; selection,"Olfactory receptors (ORs) govern a prime sensory function. Extant birds have distinct olfactory abilities, but the molecular mechanisms underlining diversification and specialization remain mostly unknown. We explored OR diversity in 48 phylogenetic and ecologically diverse birds and 2 reptiles (alligator and green sea turtle). OR subgenomes showed species- and lineage-specific variation related with ecological requirements. Overall 1,953 OR genes were identified in reptiles and 16,503 in birds. The two reptiles had larger OR gene repertoires (989 and 964 genes, respectively) than birds (182-688 genes). Overall, birds had more pseudogenes (7,855) than intact genes (1,944). The alligator had significantly more functional genes than sea turtle, likely because of distinct foraging habits. We found rapid species-specific expansion and positive selection in OR14 (detects hydrophobic compounds) in birds and in OR51 and OR52 (detect hydrophilic compounds) in sea turtle, suggestive of terrestrial and aquatic adaptations, respectively. Ecological partitioning among birds of prey, water birds, land birds, and vocal learners showed that diverse ecological factors determined olfactory ability and influenced corresponding olfactory-receptor subgenome. OR5/8/9 was expanded in predatory birds and alligator, suggesting adaptive specialization for carnivory. OR families 2/13, 51, and 52 were correlated with aquatic adaptations (water birds), OR families 6 and 10 were more pronounced in vocal-learning birds, whereas most specialized land birds had an expanded OR family 14. Olfactory bulb ratio (OBR) and OR gene repertoire were correlated. Birds that forage for prey (carnivores/piscivores) had relatively complex OBR and OR gene repertoires compared with modern birds, including passerines, perhaps due to highly developed cognitive capacities facilitating foraging innovations. {\copyright} The Author 2015. Published by Oxford University Press on behalf of the Society for Molecular Biology and Evolution.",0,141,97
97,Evolving software systems,,"During the last few years, software evolution research has explored new domains such as the study of socio-technical aspects and collaboration between different individuals contributing to a software system, theuse of search-based techniques and meta-heuristics, the mining of unstructured software repositories, the evolution of software requirements, and the dynamic adaptation of software systems at runtime. Also more and more attention is being paid to the evolution of collections of inter-related and inter-dependent software projects, be it in the form of web systems, software product families, software ecosystems or systems of systems. Withthis book, the editors present insightful contributions on these and other domains currently being intensively explored, written by renowned researchers in the respective fields of software evolution. Each chapter presents the state of the art in a particular topic, as well as the current research, available tool support and remaining challenges. The book is complemented by a glossary of important terms used in the community, a reference list of nearly 1,000 papers and books and tips on additional resources that may be useful to the reader(reference books, journals, standards and major scientific events in the domain of software evolution and datasets). This book is intended for all those interested in software engineering, and more particularly, software maintenance and evolution. Researchers and software practitioners alike will find in the contributed chapters an overview of the most recent findings, covering a broad spectrum of software evolution topics. In addition, it can also serve as the basis of graduate or postgraduate courses on e.g., software evolution, requirements engineering, model-driven software development or social informatics. {\copyright} Springer-Verlag Berlin Heidelberg 2014.",0,141,98
98,Fungal and bacterial volatile organic compounds: An overview and their role as ecological signaling agents,,"Both fungi and bacteria emit many volatile organic compounds (VOCs) as mixtures of low molecular mass alcohols, aldehydes, esters, terpenoids, thiols, and other small molecules that easily volatilize. Most determination (separation and identification) of VOCs now relies on gas chromatography-mass spectrometry (GC-MS) but developments in ``electronic nose'' technology promise to revolutionize the field. Microbial VOC profiles are both complex and dynamic: the compounds produced and their abundance vary with the producing species, the age of the colony, water availability, the substrate, the temperature, and other environmental parameters. The single most commonly reported volatile from fungi is 1-octen-3-ol which is a breakdown product of linoleic acid. It functions as a hormone within many fungal species, serves as both an attractant and deterrent for certain species of arthropods, and exhibits toxicity at relatively low concentrations in model systems. Bacterial and fungal VOCs have been studied by scientists from a broad range of subdisciplines in both theoretical and applied contexts. VOCs are exploited for their food and flavor properties, their use as indirect indicators of microbial growth, their ability to stimulate plant growth, and their ability to attract insect pests. Because these compounds can diffuse a long way from their point of origin, they are excellent chemical signaling molecules (semiochemicals) in non-aqueous habitats and facilitate the ability of microbes to engage in ``chemical conversations.'' The physiological effects of bacterial and fungal VOCs in host-pathogen relationships and in mediating interspecific associations in natural ecosystem functioning is an emerging frontier for future research. {\copyright} Springer-Verlag Berlin Heidelberg 2012.",0,141,99
99,Introductory programming: A systematic literature review,CS1; Introductory programming; ITiCSE working group; Literature review; Novice programming; Overview; Review; SLR; Systematic literature review; Systematic review,"As computing becomes a mainstream discipline embedded in the school curriculum and acts as an enabler for an increasing range of academic disciplines in higher education, the literature on introductory programming is growing. Although there have been several reviews that focus on specific aspects of introductory programming, there has been no broad overview of the literature exploring recent trends across the breadth of introductory programming. This paper is the report of an ITiCSE working group that conducted a systematic review in order to gain an overview of the introductory programming literature. Partitioning the literature into papers addressing the student, teaching, the curriculum, and assessment, we explore trends, highlight advances in knowledge over the past 15 years, and indicate possible directions for future research. {\copyright} 2018 Association for Computing Machinery.",0,141,100
100,Search-based inference of polynomial metamorphic relations,Invariant inference; Metamorphic testing; Particle swarm optimization,"Metamorphic testing (MT) is an effective methodology for testing those so-called ""non-testable"" programs (e.g., scientific programs), where it is sometimes very difficult for testers to know whether the outputs are correct. In metamorphic testing, metamorphic relations (MRs) (which specify how particular changes to the input of the program under test would change the output) play an essential role. However, testers may typically have to obtain MRs manually. In this paper, we propose a search-based approach to automatic inference of polynomial MRs for a program under test. In particular, we use a set of parameters to represent a particular class of MRs, which we refer to as polynomial MRs, and turn the problem of inferring MRs into a problem of searching for suitable values of the parameters. We then dynamically analyze multiple executions of the program, and use particle swarm optimization to solve the search problem. To improve the quality of inferred MRs, we further use MR filtering to remove some inferred MRs. We also conducted three empirical studies to evaluate our approach using four scientific libraries (including 189 scientific functions). From our empirical results, our approach is able to infer many high-quality MRs in acceptable time (i.e., from 9.87 seconds to 1231.16 seconds), which are effective in detecting faults with no false detection. {\copyright} 2014 ACM.",0,141,101
101,Effective regression test case selection: A systematic literature review,Cost effectiveness; Coverage; Fault detection ability; Slr; Software testing,"Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results. The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible. There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage. {\copyright} 2017 ACM.",0,141,102
102,"Managing agile: Strategy, implementation, organisation and people",,"This book examines agile approaches from a management perspective by focusing on matters of strategy, implementation, organization and people. It examines the turbulence of the marketplace and business environment in order to identify what role agile management has to play in coping with such change and uncertainty. Based on observations, personal experience and extensive research, it clearly identifies the fabric of the agile organization, helping managers to become agile leaders in an uncertain world. The book opens with a broad survey of agile strategies, comparing and contrasting some of the major methodologies selected on the basis of where they lie on a continuum of ceremony and formality, ranging from the minimalist technique-driven and software engineering focused XP, to the pragmatic product-project paradigm that is Scrum and its scaled counterpart SAFe{	extregistered}, to the comparatively project-centric DSDM. Subsequently, the core of the book focuses on DSDM, owing to the method's comprehensive elaboration of program and project management practices. This work will chiefly be of interest to all those with decision-making authority within their organizations (e.g., senior managers, line managers, program, project and risk managers) and for whom topics such as strategy, finance, quality, governance and risk management constitute a daily aspect of their work. It will, however, also be of interest to those readers in advanced management or business administration cou rses (e.g., MBA, MSc), who wish to engage in the management of agile organizations and thus need to adapt their skills and knowledge accordingly. {\copyright} Springer International Publishing Switzerland 2015.",0,141,103
103,Measuring architecture quality by structure plus history analysis,change history; fault prediction; measure; software architecture; structure,"This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented. {\copyright} 2013 IEEE.",0,141,104
104,Developer Micro Interaction Metrics for Software Defect Prediction,Defect prediction; developer interaction; Mylyn; software metrics; software quality,"To facilitate software quality assurance, defect prediction metrics, such as source code metrics, change churns, and the number of previous defects, have been actively studied. Despite the common understanding that developer behavioral interaction patterns can affect software quality, these widely used defect prediction metrics do not consider developer behavior. We therefore propose micro interaction metrics (MIMs), which are metrics that leverage developer interaction information. The developer interactions, such as file editing and browsing events in task sessions, are captured and stored as information by Mylyn, an Eclipse plug-in. Our experimental evaluation demonstrates that MIMs significantly improve overall defect prediction accuracy when combined with existing software measures, perform well in a cost-effective manner, and provide intuitive feedback that enables developers to recognize their own inefficient behaviors during software development. {\copyright} 2016 IEEE.",0,141,105
105,Mining co-change information to understand when build changes are necessary,Build systems; mining software repositories; software evolution,"As a software project ages, its source code is modified to add new features, restructure existing ones, and fix defects. These source code changes often induce changes in the build system, i.e., the system that specifies how source code is translated into deliverables. However, since developers are often not familiar with the complex and occasionally archaic technologies used to specify build systems, they may not be able to identify when their source code changes require accompanying build system changes. This can cause build breakages that slow development progress and impact other developers, testers, or even users. In this paper, we mine the source and test code changes that required accompanying build changes in order to better understand this co-change relationship. We build random forest classifiers using language-agnostic and language-specific code change characteristics to explain when code-accompanying build changes are necessary based on historical trends. Case studies of the Mozilla C++ system, the Lucene and Eclipse open source Java systems, and the IBM Jazz proprietary Java system indicate that our classifiers can accurately explain when build co-changes are necessary with an AUC of 0.60-0.88. Unsurprisingly, our highly accurate C++ classifiers (AUC of 0.88) derive much of their explanatory power from indicators of structural change (e.g., was a new source file added?). On the other hand, our Java classifiers are less accurate (AUC of 0.60-0.78) because roughly 75% of Java build co-changes do not coincide with changes to the structure of a system, but rather are instigated by concerns related to release engineering, quality assurance, and general build maintenance. {\copyright} 2014 IEEE.",0,141,106
106,Human-Computer Interaction,,"""Human-Computer Interaction: An Empirical Research Perspective"" is the definitive guide to empirical research in HCI. The book begins with foundational topics including historical context, the human factor, interaction elements, and the fundamentals of science and research. From there, youll progress to learning about the methods for conducting an experiment to evaluate a new computer interface or interaction technique. There are detailed discussions and how-to analyses on models of interaction, focusing on descriptive models and predictive models. Writing and publishing a research paper is explored with helpful tips for success. Throughout the book, youll find hands-on exercises, checklists, and real-world examples. This is your must-have, comprehensive guide to empirical and experimental research in HCI-an essential addition to your HCI library. Master empirical and experimental research with this comprehensive, A-to-Z guide in a concise, hands-on reference Discover the practical and theoretical ins-and-outs of user studies Find exercises, takeaway points, and case studies throughout. {\copyright} 2013 Elsevier Inc. All rights reserved.",0,141,107
107,Can Lexicon Bad Smells improve fault prediction?,Fault prediction; lexicon bad smells; program understanding; structural metrics,"In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases. {\copyright} 2012 IEEE.",0,141,108
108,Discovering bug patterns in Javascript,Bug Patterns; Data Mining; Javascript; Node.Js; Static Analysis,"JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. {\copyright} 2016 ACM.",0,141,109
109,Evolving estimators of the pointwise Holder exponent with Genetic Programming,Genetic Programming; Holder regularity; Local image description,"The regularity of a signal can be numerically expressed using Holder exponents, which characterize the singular structures a signal contains. In particular, within the domains of image processing and image understanding, regularity-based analysis can be used to describe local image shape and appearance. However, estimating the Holder exponent is not a trivial task, and current methods tend to be computationally slow and complex. This work presents an approach to automatically synthesize estimators of the pointwise Holder exponent for digital images. This task is formulated as an optimization problem and Genetic Programming (GP) is used to search for operators that can approximate a traditional estimator, the oscillations method. Experimental results show that GP can generate estimators that achieve a low error and a high correlation with the ground truth estimation. Furthermore, most of the GP estimators are faster than traditional approaches, in some cases their runtime is orders of magnitude smaller. This result allowed us to implement a real-time estimation of the Holder exponent on a live video signal, the first such implementation in current literature. Moreover, the evolved estimators are used to generate local descriptors of salient image regions, a task for which a stable and robust matching is achieved, comparable with state-of-the-art methods. In conclusion, the evolved estimators produced by GP could help expand the application domain of Holder regularity within the fields of image analysis and signal processing. {\copyright} 2012 Elsevier Inc. All rights reserved.",0,141,110
110,Smells like teen spirit: Improving bug prediction performance using the intensity of code smells,,"Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models and comparing the results of the new model against the baseline model. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also evaluate the actual gain provided by the intensity index with respect to the other metrics in the model, including the ones used to compute the code smell intensity. We observe that the intensity index is much more important as compared to other metrics used for predicting the buggyness of smelly classes. {\copyright} 2016 IEEE.",0,141,111
111,An empirical framework for defect prediction using machine learning techniques with Android software,Inter-release validation; Machine-learning; Object-oriented metrics; Software defect proneness; Statistical tests,"Context Software defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. Objective The main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. Method In this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. Results The results show that the area under the curve measure for Na{""\i}ve Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. Conclusion The results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software. {\copyright} 2016 Elsevier B.V.",0,141,112
112,Code ownership and software quality: A replication study,Code ownership; Empirical software engineering; Software quality,"In a traditional sense, ownership determines rights and duties in regard to an object, for example a property. The owner of source code usually refers to the person that invented the code. However, larger code artifacts, such as files, are usually composed by multiple engineers contributing to the entity over time through a series of changes. Frequently, the person with the highest contribution, e.g. The most number of code changes, is defined as the code owner and takes responsibility for it. Thus, code ownership relates to the knowledge engineers have about code. Lacking responsibility and knowledge about code can reduce code quality. In an earlier study, Bird et al. [1] showed that Windows binaries that lacked clear code ownership were more likely to be defect prone. However recommendations for large artifacts such as binaries are usually not actionable. E.g. Changing the concept of binaries and refactoring them to ensure strong ownership would violate system architecture principles. A recent replication study by Foucault et al. [2] on open source software replicate the original results and lead to doubts about the general concept of ownership impacting code quality. In this paper, we replicated and extended the previous two ownership studies [1, 2] and reflect on their findings. Further, we define several new ownership metrics to investigate the dependency between ownership and code quality on file and directory level for 4 major Microsoft products. The results confirm the original findings by Bird et al. [1] that code ownership correlates with code quality. Using new and refined code ownership metrics we were able to classify source files that contained at least one bug with a median precision of 0.74 and a median recall of 0.38. On directory level, we achieve a precision of 0.76 and a recall of 0.60. {\copyright} 2015 IEEE.",0,141,113
113,Bad-smell prediction from software design model using machine learning techniques,Bad-smell; Design Diagram Metrics; Machine Learners; Prediction models; Random Forest; Software Design Model,Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values. {\copyright} 2011 IEEE.,1,12,114
114,Progress on approaches to software defect prediction,,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction. {\copyright} The Institution of Engineering and Technology 2018.",0,141,115
115,The impact of tangled code changes on defect prediction models,Data noise; Defect prediction; Untangling,"When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets---in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes. {\copyright} 2015, Springer Science+Business Media New York.",0,141,116
116,Combining Software Metrics and Text Features for Vulnerable File Prediction,Machine Learning; Text Mining; Vulnerable File,"In recent years, to help developers reduce time and effort required to build highly secure software, a number of prediction models which are built on different kinds of features have been proposed to identify vulnerable source code files. In this paper, we propose a novel approach VULPREDICTOR to predict vulnerable files, it analyzes software metrics and text mining together to build a composite prediction model. VULPREDICTOR first builds 6 underlying classifiers on a training set of vulnerable and non-vulnerable files represented by their software metrics and text features, and then constructs a meta classifier to process the outputs of the 6 underlying classifiers. We evaluate our solution on datasets from three web applications including Drupal, PHPMyAdmin and Moodle which contain a total of 3,466 files and 223 vulnerabilities. The experiment results show that VULPREDICTOR can achieve F1 and EffectivenessRatio@20% scores of up to 0.683 and 75%, respectively. On average across the 3 projects, VULPREDICTOR improves the F1 and EffectivenessRatio@20% scores of the best performing state-of-the-art approaches proposed by Walden et al. by 46.53% and 14.93%, respectively. {\copyright} 2015 IEEE.",0,141,117
117,Quality: Its definition and measurement as applied to the Medically ill,,"Quality, as exemplified by Quality-of-life (QoL) assessment, is frequently discussed among health care professionals and often invoked as a goal for improvement, but somehow rarely defined, even as it is regularly assessed. It is understood that some medical patients have a better QoL than others, but should the QoL achieved be compared to an ideal state, or is it too personal and subjective to gauge? Can a better understanding of the concept help health care systems deliver services more effectively? Is QoL worth measuring at all? Integrating concepts from psychology, philosophy, neurocognition, and linguistics, this book attempts to answer these complex questions. It also breaks down the cognitive-linguistic components that comprise the judgment of quality, including description, evaluation, and valuations, and applies them to issues specific to individuals with chronic medical illness. In this context, quality/QoL assessment becomes an essential contributor to ethical practice, a critical step towards improving the nature of social interactions. The author considers linear, non-linear, and complexity-based models in analyzing key methodology and content issues in health-related QoL assessment.This book is certain to stimulate debate in the research and scientific communities. Its forward-looking perspective takes great strides toward promoting a common cognitive-linguistic model of how the judgment of quality occurs, thereby contributing important conceptual and empirical tools to its varied applications, including QoL assessment. {\copyright} Springer Science+Business Media, LLC 2012. All rights reserved.",0,141,118
118,Measuring code quality to improve specification mining,code metrics; machine learning; program understanding; software engineering; Specification mining,"Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code. {\copyright} 2006 IEEE.",0,141,119
119,Enhancing change prediction models using developer-related factors,Change prediction; Empirical study; Mining software repositories,"Continuous changes applied during software maintenance risk to deteriorate the structure of a system and are a threat to its maintainability. In this context, predicting the portions of source code where specific maintenance operations should be focused on may be crucial for developers to prevent maintainability issues. Previous work proposed change prediction models relying on product and process metrics as predictors of change-prone source code classes. However, we believe that existing approaches still miss an important piece of information, i.e., developer-related factors that are able to capture the complexity of the development process under different perspectives. In this paper, we firstly investigate three change prediction models that exploit developer-related factors (e.g., number of developers working on a class) as predictors of change-proneness of classes and then we compare them with existing models. Our findings reveal that these factors improve the capabilities of change prediction models. Moreover, we observed interesting complementarities among the prediction models. For this reason, we devised a novel change prediction model exploiting the combination of developer-related factors and product and evolution metrics. The results show that such a combined model is up to 22% more effective than the single models in the identification of change-prone classes. {\copyright} 2018 Elsevier Inc.",0,141,120
120,Mutation-aware fault prediction,Empirical study; Mutation testing; Software defect prediction; Software fault prediction; Software metrics,"We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 Different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p â¤ 0:05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments. {\copyright} 2016 ACM.",0,141,121
121,Detecting API documentation errors,API documentation error; Outdated documentation,"When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DocRef, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DocRef and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DocRef has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them. Copyright {\copyright} 2013. Copyright {\copyright} 2013 ACM.",0,141,122
122,Fault-prone module detection using large-scale text features based on spam filtering,Fault-prone module; Large-scale; Software repository; Spam filtering; Text feature; Text mining,"This paper proposes an approach using large-scale text features for faultprone module detection inspired by spam filtering. The number of every text feature in the source code of a module is counted and used as data for training detection models. In this paper, we prepared a naive Bayes classifier and a logistic regression model as detection models. To show the effectiveness of our approaches, we conducted experiments with five open source projects and compared them with a well-known metrics set, thereby achieving higher detection results. The results imply that large-scale text features are useful in constructing practical detection models, and measuring sophisticated metrics is not always necessary for detecting fault-prone modules. {\copyright} Springer Science + Business Media, LLC 2009.",0,141,123
123,Teaching software engineering principles to K-12 students: A MOOC on scratch,code smells; dropout prediction; MOOC; Programming education; Scratch,"In the last few years, many books, online puzzles, apps and games have been created to teach young children programming. However, most of these do not introduce children to broader concepts from software engineering, such as debugging and code quality issues like smells, duplication, refactoring and naming. To address this, we designed and ran an online introductory Scratch programming course in which we teach elementary programming concepts and software engineering concepts simultaneously. In total 2,220 children actively participated in our course in June and July of 2016, most of which (73%) between the ages of 7 and 11. In this paper we describe our course design and analyze the resulting data. More specifically, we investigate whether 1) students find programming concepts more difficult than software engineering concepts, 2) there are age-related differences in their performance, and 3) we can predict successful course completion. Our results show that there is no difference in students' scores between the programming concepts and the software engineering concepts, suggesting that it is indeed possible to teach these concepts to this age group. We also find that students over 12 years of age perform significantly better in questions related to operators and procedures. Finally, we identify the factors from the students' profile and their behaviour in the first week of the course that can be used to predict its successful completion. {\copyright} 2017 IEEE.",0,141,124
124,Multi-objective code-smells detection using good and bad design examples,Search-based software engineering; Software maintenance; Software metrics,"Code-smells are identified, in general, by using a set of detection rules. These rules are manually defined to identify the key symptoms that characterize a code-smell using combinations of mainly quantitative (metrics), structural, and/or lexical information. We propose in this work to consider the problem of code-smell detection as a multi-objective problem where examples of code-smells and well-designed code are used to generate detection rules. To this end, we use multi-objective genetic programming (MOGP) to find the best combination of metrics that maximizes the detection of code-smell examples and minimizes the detection of well-designed code examples. We evaluated our proposal on seven large open-source systems and found that, on average, most of the different five code-smell types were detected with an average of 87 % of precision and 92 % of recall. Statistical analysis of our experiments over 51 runs shows that MOGP performed significantly better than state-of-the-art code-smell detectors. {\copyright} 2016, Springer Science+Business Media New York.",0,141,125
125,Studying the effectiveness of Application Performance Management (APM) tools for detecting performance regressions for web applications: An experience report,,"Performance regressions, such as a higher CPU utilization than in the previous version of an application, are caused by software application updates that negatively affect the performance of an application. Although a plethora of mining software repository research has been done to detect such regressions, research tools are generally not readily available to practitioners. Application Performance Management (APM) tools are commonly used in practice for detecting performance issues in the field by mining operational data. In contrast to performance regression detection tools that assume a changing code base and a stable workload, APM tools mine operational data to detect performance anomalies caused by a changing workload in an otherwise stable code base. Although APM tools are widely used in practice, no research has been done to understand 1) whether APM tools can identify performance regressions caused by code changes and 2) how well these APM tools support diagnosing the root-cause of these regressions. In this paper, we explore if the readily accessible APM tools can help practitioners detect performance regressions. We perform a case study using three commercial (AppDynamics, New Relic and Dynatrace) and one open source (Pinpoint) APM tools. In particular, we examine the effectiveness of leveraging these APM tools in detecting and diagnosing injected performance regressions (excessive memory usage, high CPU utilization and inefficient database queries) in three open source applications. We find that APM tools can detect most of the injected performance regressions, making them good candidates to detect performance regressions in practice. However, there is a gap between mining approaches that are proposed in state-of-theart performance regression detection research and the ones used by APM tools. In addition, APM tools lack the ability to be extended, which makes it hard to enhance them when exploring novel mining approaches for detecting performance regressions. {\copyright} 2016 ACM.",0,141,126
126,Data stream mining for predicting software build outcomes using source code metrics,Concept drift detection; Data stream mining; Hoeffding tree; Jazz; Software metrics; Software repositories,"Context Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process. Objective This paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure. Method This research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool. Results The results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches. Conclusion Overall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure. {\copyright} 2013 Elsevier B.V. All rights reserved.",0,141,127
127,A fuzzy classifier approach to estimating software quality,Computational intelligence; Fuzzy logic; Pattern classification; Software engineering; Software metric,"With the increasing sophistication of today's software systems, it is often difficult to estimate the overall quality of underlying software components with respect to attributes such as complexity, utility, and extensibility. Many metrics exist in the software engineering literature that attempt to quantify, with varying levels of accuracy, a large swath of qualitative attributes. However, the overall quality of a software object may manifest itself in ways that the simple interpretation of metrics fails to identify. A better strategy is to determine the best, possibly non-linear, subset of many software metrics for accurately estimating software quality. This strategy may be couched in terms of a problem of classification, that is, determine a mapping from a set of software metrics to a set of class labels representing software quality. We implement this strategy using a fuzzy classification approach. The software metrics are automatically computed and presented as features (input) to a classifier, while the class labels (output) are assigned via an expert's (software architect) thorough assessment of the quality of individual software objects. A large collection of classifiers is presented with subsets of the software metric features. Subsets are selected stochastically using a fuzzy logic based sampling method. The classifiers then predict the quality, specifically the class label, of each software object. Fuzzy integration is applied to the results from the most accurate individual classifiers. We empirically evaluate this approach using software objects from a sophisticated algorithm development framework used to develop biomedical data analysis systems. We demonstrate that the sampling method attenuates the effects of confounding features, and the aggregated classification results using fuzzy integration are superior to the predictions from the respective best individual classifiers. {\copyright} 2013 Elsevier Inc. All rights reserved.",0,141,128
128,An expert system for determining candidate software classes for refactoring,Naive Bayes; Refactor prediction; Refactoring; Software metrics,"In the lifetime of a software product, development costs are only the tip of the iceberg. Nearly 90% of the cost is maintenance due to error correction, adaptation and mainly enhancements. As Lehman and Belady [Lehman, M. M., & Belady, L. A. (1985). Program evolution: Processes of software change. Academic Press Professional.] state that software will become increasingly unstructured as it is changed. One way to overcome this problem is refactoring. Refactoring is an approach which reduces the software complexity by incrementally improving internal software quality. Our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity. We propose a machine learning based model to predict classes to be refactored. We use Weighted Na{""\i}ve Bayes with InfoGain heuristic as the learner and we conducted experiments with metric data that we collected from the largest GSM operator in Turkey. Our results showed that we can predict 82% of the classes that need refactoring with 13% of manual inspection effort on the average. {\copyright} 2008 Elsevier Ltd. All rights reserved.",0,141,129
129,Evolutionary trends of developer coordination: a network approach,Developer coordination; Developer networks; Software evolution,"Software evolution is a fundamental process that transcends the realm of technical artifacts and permeates the entire organizational structure of a software project. By means of a longitudinal empirical study of 18 large open-source projects, we examine and discuss the evolutionary principles that govern the coordination of developers. By applying a network-analytic approach, we found that the implicit and self-organizing structure of developer coordination is ubiquitously described by non-random organizational principles that defy conventional software-engineering wisdom. In particular, we found that: (a) developers form scale-free networks, in which the majority of coordination requirements arise among an extremely small number of developers, (b) developers tend to accumulate coordination requirements with more and more developers over time, presumably limited by an upper bound, and (c) initially developers are hierarchically arranged, but over time, form a hybrid structure, in which core developers are hierarchically arranged and peripheral developers are not. Our results suggest that the organizational structure of large projects is constrained to evolve towards a state that balances the costs and benefits of developer coordination, and the mechanisms used to achieve this state depend on the project's scale. {\copyright} 2016, Springer Science+Business Media New York.",0,141,130
130,Using Visual Symptoms for Debugging Presentation Failures in Web Applications,image processing; presentation failures; probabilistic model; Root cause analysis; visual symptoms; web applications,"Presentation failures in a website can undermine its success by giving users a negative perception of the trustworthiness of the site and the quality of the services it delivers. Unfortunately, existing techniques for debugging presentation failures do not provide developers with automated and broadly applicable solutions for finding the site's faulty HTML elements and CSS properties. To address this limitation, we propose a novel automated approach for debugging web sites that is based on image processing and probabilistic techniques. Our approach first builds a model that links observable changes in the web site's appearance to faulty elements and styling properties. Then using this model, our approach predicts the elements and styling properties most likely to cause the observed failure for the page under test and reports these to the developer. In evaluation, our approach was more accurate and faster than prior techniques for identifying faulty elements in a website. {\copyright} 2016 IEEE.",0,141,131
131,Reflections on self-deception,,"Commentators raised 10 major questions with regard to self-deception: Are dual representations necessary? Does self-deception serve intrapersonal goals? What forces shape self-deception? Are there cultural differences in self-deception? What is the self? Does self-deception have costs? How well do people detect deception? Are self-deceivers lying? Do cognitive processes account for seemingly motivational ones? And how is mental illness tied up with self-deception? We address these questions and conclude that none of them compel major modifications to our theory of self-deception, although many commentators provided helpful suggestions and observations. {\copyright} 2011 Cambridge University Press.",0,141,132
132,A systematic review of requirements change management,Agile; Requirements change management; Systematic review,"Context Software requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement. Objective In this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature. Method We use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work. Results The four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified. Conclusions This paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success. {\copyright} 2017 Elsevier B.V.",0,141,133
133,Developer-Related Factors in Change Prediction: An Empirical Assessment,Change prediction; Empirical Studies; Mining Software Repositories,"Predicting the areas of the source code having a higher likelihood to change in the future is a crucial activity to allow developers to plan preventive maintenance operations such as refactoring or peer-code reviews. In the past the research community was active in devising change prediction models based on structural metrics extracted from the source code. More recently, Elish et al. showed how evolution metrics can be more efficient for predicting change-prone classes. In this paper, we aim at making a further step ahead by investigating the role of different developer-related factors, which are able to capture the complexity of the development process under different perspectives, in the context of change prediction. We also compared such models with existing change-prediction models based on evolution and code metrics. Our findings reveal the capabilities of developer-based metrics in identifying classes of a software system more likely to be changed in the future. Moreover, we observed interesting complementarities among the experimented prediction models, that may possibly lead to the definition of new combined models exploiting developer-related factors as well as product and evolution metrics. {\copyright} 2017 IEEE.",0,141,134
134,Finding and analyzing compiler warning defects,,"Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers. At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting? Our technique is very effective-we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers. {\copyright} 2016 ACM.",0,141,135
135,Using (bio)metrics to predict code quality online,,"Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the diffculty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository. In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classi fiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs. {\copyright} 2016 ACM.",0,141,136
136,Brain asymmetry and neural systems: Foundations in clinical neuroscience and neuropsychology,,"The proposed book investigates brain asymmetry from the perspective of functional neural systems theory, a foundational approach for the topic. There is currently no such book available on the market and there is a need for a neuroscience book, with a focus on the functional asymmetry of these two integrated and dynamic brains using historical and modern clinical and experimental research findings with the field. The book provides evidence from multiple methodologies, including clinical lesion studies, brain stimulation, and modern imaging techniques. The author has successfully used the book in doctoral and advances undergraduate courses on neuroscience and neuropsychology. It has also been used to teach a course on the biological basis of behavior and could be used in a variety of contexts and courses. {\copyright} Springer International Publishing Switzerland 2015.",0,141,137
137,Family-based performance measurement,family-based analysis; featurehouse; performance prediction,"Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing. {\copyright} 2013 ACM.",0,141,138
138,Mining Sequences of Developer Interactions in Visual Studio for Usage Smells,data mining; IDE usage data; pattern mining; usability analysis,"In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional. {\copyright} 2017 IEEE.",0,141,139
139,Confounds and consequences in geotagged twitter data,,"Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location field in each profile. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and textbased geolocation. GPS-tagging and self-reported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40. {\copyright} 2015 Association for Computational Linguistics.",0,141,140
140,Heuristic Search,,"Search has been vital to artificial intelligence from the very beginning as a core technique in problem solving. The authors present a thorough overview of heuristic search with a balance of discussion between theoretical analysis and efficient implementation and application to real-world problems. Current developments in search such as pattern databases and search with efficient use of external memory and parallel processing units on main boards and graphics cards are detailed. Heuristic search as a problem solving tool is demonstrated in applications for puzzle solving, game playing, constraint satisfaction and machine learning. While no previous familiarity with heuristic search is necessary the reader should have a basic knowledge of algorithms, data structures, and calculus. Real-world case studies and chapter ending exercises help to create a full and realized picture of how search fits into the world of artificial intelligence and the one around us. The content is organized into five parts as follows: Search Primer: State-Space Search, Basic Search Algorithms, Dictionary Data Structures, and Automatically Created Heuristics Search under Memory Constraints: Linear-Space Search, Memory-Restricted Search, Symbolic Search, External Search Search Under Time Constraints: Distributed Search, State-Space Pruning, and Real-Time Search Search Variants: Adversary Search, Constraint Satisfaction Search, and Local Search Search Applications: Robotics, Automated System Verification, Action Planning, Vehicle Navigation, and Computational Biology. {\copyright} 2012 Elsevier Inc. All rights reserved.",0,141,141
141,"Object oriented design expertise reuse: An approach based on heuristics, design patterns and anti-patterns",,"Object Oriented (OO) languages do not guarantee that a system is flexible enough to absorb future requirements, nor that its components can be reused in other contexts. This paper presents an approach to OO design expertise reuse, which is able to detect certain constructions that compromise future expansion or modification of OO systems, and suggest their replacement by more adequate ones. Both reengineering legacy systems, and systems that are still under development are considered by the approach. A tool (OOPDTool) was developed to support the approach, comprising a knowledge base of good design constructions, that correspond to heuristics and design patterns, as well as problematic constructions (i.e., anti-patterns). {\copyright} Springer-Verlag Berlin Heidelberg 2000.",0,141,142
142,A comparison of code similarity analysers,Clone detection; Code similarity measurement; Empirical study; Parameter optimisation; Plagiarism detection,"Copying and pasting of source code is a common activity in software engineering. Often, the code is not copied as it is and it may be modified for various purposes; e.g. refactoring, bug fixing, or even software plagiarism. These code modifications could affect the performance of code similarity analysers including code clone and plagiarism detectors to some certain degree. We are interested in two types of code modification in this study: pervasive modifications, i.e. transformations that may have a global effect, and local modifications, i.e. code changes that are contained in a single method or code block. We evaluate 30 code similarity detection techniques and tools using five experimental scenarios for Java source code. These are (1) pervasively modified code, created with tools for source code and bytecode obfuscation, and boiler-plate code, (2) source code normalisation through compilation and decompilation using different decompilers, (3) reuse of optimal configurations over different data sets, (4) tool evaluation using ranked-based measures, and (5) local + global code modifications. Our experimental results show that in the presence of pervasive modifications, some of the general textual similarity measures can offer similar performance to specialised code similarity tools, whilst in the presence of boiler-plate code, highly specialised source code similarity detection techniques and tools outperform textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for three of the tools. Moreover, we demonstrate that optimal configurations are very sensitive to a specific data set. After directly applying optimal configurations derived from one data set to another, the tools perform poorly on the new data set. The code similarity analysers are thoroughly evaluated not only based on several well-known pair-based and query-based error measures but also on each specific type of pervasive code modification. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code. {\copyright} 2017, The Author(s).",0,141,143
143,On the use of design defect examples to detect model refactoring opportunities,Design defects; Detection by example; Genetic algorithm; Search-based software engineering,"Design defects are symptoms of design decay, which can lead to several maintenance problems. To detect these defects, most of existing research is based on the definition of rules that represent a combination of software metrics. These rules are sometimes not enough to detect design defects since it is difficult to find the best threshold values; the rules do not take into consideration the programming context, and it is challenging to find the best combination of metrics. As an alternative, we propose in this paper to identify design defects using a genetic algorithm based on the similarity/distance between the system under study and a set of defect examples without the need to define detection rules. We tested our approach on four open-source systems to identify three potential design defects. The results of our experiments confirm the effectiveness of the proposed approach. {\copyright} 2015, Springer Science+Business Media New York.",0,141,144
144,A combined ant colony optimization and simulated annealing algorithm to assess stability and fault-proneness of classes based on internal software quality attributes,Ant colony optimization; C4.5; Metric; Prediction; Rule sets; Search-based software engineering; Simulated annealing; Software quality,"Several machine learning algorithms have been used to assess external quality attributes of software systems. Given a set of metrics that describe internal software attributes (cohesion, complexity, size, etc.), the purpose is to construct a model that can be used to assess external quality attributes (stability, reliability, maintainability, etc.) based on the internal ones. Most of these algorithms result in assessment models that are hard to generalize. As a result, they show a degradation in their assessment performance when used to estimate quality of new software modules. This paper presents a hybrid heuristic to construct software quality estimation models that can be used to predict software quality attributes of new unseen systems prior to re-using them or purchasing them. The technique relies on two heuristics: simulated annealing and ant colony optimization. It learns from the data available in a particular domain guidelines and rules to achieve a particular external software quality. These guidelines are presented as rule-based logical models. We validate our technique on two software quality attributes namely stability and fault-proneness - a subattribute of maintainability. We compare our technique to two state-of-the-art algorithms: Neural Networks (NN) and C4.5 as well as to a previously published Ant Colony Optimization algorithm. Results show that our hybrid technique out-performs both C4.5 and ACO in most of the cases. Compared to NN, our algorithm preserves the white-box nature of the predictive models hence, giving not only the classification of a particular module but also guidelines for software engineers to follow in order to reach a particular external quality attribute. Our algorithm gives promising results and is generic enough to apply to any software quality attribute. {\copyright} 2016 [International Journal of Artificial Intelligence].",0,141,145
145,Automatic clustering of code changes,Clustering; Code changes; Software repositories,"Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. {\copyright} 2016 ACM.",0,141,146
146,Experience report: Evaluating the effectiveness of decision trees for detecting code smells,Code Smells; Decision Tree; Genetic Algorithm; Software Quality,"Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use. {\copyright} 2015 IEEE.",1,12,147
147,Code Bad Smell Detection through Evolutionary Data Mining,bad smell detection; data mining; software evolutionary history,"The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure. {\copyright} 2015 IEEE.",0,141,148
148,The ancient origins of consciousness: How the brain created experience,,"How is consciousness created? When did it first appear on Earth, and how did it evolve? What constitutes consciousness, and which animals can be said to be sentient? In this book, Todd Feinberg and Jon Mallatt draw on recent scientific findings to answer these questions -- and to tackle the most fundamental question about the nature of consciousness: how does the material brain create subjective experience? After assembling a list of the biological and neurobiological features that seem responsible for consciousness, and considering the fossil record of evolution, Feinberg and Mallatt argue that consciousness appeared much earlier in evolutionary history than is commonly assumed. About 520 to 560 million years ago, they explain, the great ""Cambrian explosion"" of animal diversity produced the first complex brains, which were accompanied by the first appearance of consciousness; simple reflexive behaviors evolved into a unified inner world of subjective experiences. From this they deduce that all vertebrates are and have always been conscious -- not just humans and other mammals, but also every fish, reptile, amphibian, and bird. Considering invertebrates, they find that arthropods (including insects and probably crustaceans) and cephalopods (including the octopus) meet many of the criteria for consciousness. The obvious and conventional wisdom--shattering implication is that consciousness evolved simultaneously but independently in the first vertebrates and possibly arthropods more than half a billion years ago. Combining evolutionary, neurobiological, and philosophical approaches allows Feinberg and Mallatt to offer an original solution to the ""hard problem"" of consciousness. {\copyright} 2016 Massachusetts Institute of Technology. All rights reserved.",0,141,149
149,Protein- and Peptide-Based Biosensors in Artificial Olfaction,artificial olfaction; biosensor; odorant-binding protein; olfactory receptor; peptide; Volatile organic compounds,"Animals' olfactory systems rely on proteins, olfactory receptors (ORs) and odorant-binding proteins (OBPs), as their native sensing units to detect odours. Recent advances demonstrate that these proteins can also be employed as molecular recognition units in gas-phase biosensors. In addition, the interactions between odorant molecules and ORs or OBPs are a source of inspiration for designing peptides with tunable odorant selectivity. We review recent progress in gas biosensors employing biological units (ORs, OBPs, and peptides) in light of future developments in artificial olfaction, emphasizing examples where biological components have been employed to detect gas-phase analytes. {\copyright} 2018 The Authors",0,141,150
150,An empirical study on the removal of Self-Admitted Technical Debt,Mining software repositories; Self-Admitted Technical Debt; Source code quality,"Technical debt refers to the phenomena of taking shortcuts to achieve short term gain at the cost of higher maintenance efforts in the future. Recently, approaches were developed to detect technical debt through code comments, referred to as Self-Admitted Technical Debt (SATD). Due to its importance, several studies have focused on the detection of SATD and examined its impact on software quality. However, preliminary findings showed that in some cases SATD may live in a project for a long time, i.e., more than 10 years. These findings clearly show that not all SATD may be regarded as 'bad' and some SATD needs to be removed, while other SATD may be fine to take on. Therefore, in this paper, we study the removal of SATD. In an empirical study on five open source projects, we examine how much SATD is removed and who removes SATD? We also investigate for how long SATD lives in a project and what activities lead to the removal of SATD? Our findings indicate that the majority of SATD is removed and that the majority is self-removed (i.e., removed by the same person that introduced it). Moreover, we find that SATD can last between approx. 18-172 days, on median. Finally, through a developer survey, we find that developers mostly use SATD to track future bugs and areas of the code that need improvements. Also, developers mostly remove SATD when they are fixing bugs or adding new features. Our findings contribute to the body of empirical evidence on SATD, in particular evidence pertaining to its removal. {\copyright} 2017 IEEE.",0,141,151
151,Predicting query quality for applications of text retrieval to software engineering tasks,Artifact traceability; Concept location; Text retrieval,"Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself. Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery. Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average. Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR. {\copyright} 2017 ACM.",0,141,152
152,Predicting delays in software projects using networked classification,Machine Learning; Networked classification; Risk management; Software analytics,"Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) - as done in previous work - but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46% - 97% precision (49% improved), 46% - 97% recall (28% improved), 56% - 75% F-measure (39% improved), and 78% - 95% Area Under the ROC Curve (16% improved). {\copyright} 2015 IEEE.",0,141,153
